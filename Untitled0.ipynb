{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN3OKuwZh9uCWTLU9kQ2x+b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JulianZablonski37/uczenie_glebokie/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ee48bDmecHX",
        "outputId": "720da212-5be6-4904-ae55-4464d35c01cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'uczenie_glebokie'...\n",
            "remote: Enumerating objects: 139, done.\u001b[K\n",
            "remote: Counting objects: 100% (139/139), done.\u001b[K\n",
            "remote: Compressing objects: 100% (115/115), done.\u001b[K\n",
            "remote: Total 139 (delta 73), reused 37 (delta 16), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (139/139), 6.48 MiB | 10.77 MiB/s, done.\n",
            "Resolving deltas: 100% (73/73), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/JulianZablonski37/uczenie_glebokie.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/uczenie_glebokie\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szdyvOGJemtX",
        "outputId": "51337b23-4778-44c8-de8b-7dea6bf06f01"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/uczenie_glebokie\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install 'transformers==4.23.1' torch sentencepiece datasets evaluate sacrebleu scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WY1JoOLogOew",
        "outputId": "66ab72d4-1059-4f13-932e-b48fb037c167"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers==4.23.1\n",
            "  Downloading transformers-4.23.1-py3-none-any.whl (5.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (1.13.1+cu116)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets\n",
            "  Downloading datasets-2.9.0-py3-none-any.whl (462 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.8/462.8 KB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting evaluate\n",
            "  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 KB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sacrebleu\n",
            "  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 KB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (1.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers==4.23.1) (2.25.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers==4.23.1) (23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers==4.23.1) (3.9.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.23.1) (2022.6.2)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers==4.23.1) (4.64.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers==4.23.1) (6.0)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.12.0-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.23.1) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (4.4.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (2023.1.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets) (1.3.5)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.0/213.0 KB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (9.0.0)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 KB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets) (3.8.3)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.8/dist-packages (from sacrebleu) (0.8.10)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.8/dist-packages (from sacrebleu) (4.9.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.7.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.8.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (22.2.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (2.1.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.23.1) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.23.1) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.23.1) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.23.1) (1.24.3)\n",
            "Collecting urllib3<1.27,>=1.21.1\n",
            "  Downloading urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: tokenizers, sentencepiece, xxhash, urllib3, portalocker, multiprocess, colorama, sacrebleu, responses, huggingface-hub, transformers, datasets, evaluate\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed colorama-0.4.6 datasets-2.9.0 evaluate-0.4.0 huggingface-hub-0.12.0 multiprocess-0.70.14 portalocker-2.7.0 responses-0.18.0 sacrebleu-2.3.1 sentencepiece-0.1.97 tokenizers-0.13.2 transformers-4.23.1 urllib3-1.26.14 xxhash-3.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p data\n",
        "!python prepare_beer_reviews.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "faiXKVPfg15N",
        "outputId": "ec23d4bd-962e-4484-f106-f51c362c2bf5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading builder script: 100% 8.12k/8.12k [00:00<00:00, 7.77MB/s]\n",
            "Downloading readme: 100% 3.31k/3.31k [00:00<00:00, 4.50MB/s]\n",
            "No config specified, defaulting to: beer_reviews_label_drift_neg/default\n",
            "Downloading and preparing dataset beer_reviews_label_drift_neg/default to /root/.cache/huggingface/datasets/arize-ai___beer_reviews_label_drift_neg/default/1.0.0/19f47acda7a949b5528784d68d9f6aa4eaa144ba8ecaf7a234e4243e2ad1c4a0...\n",
            "Downloading data files:   0% 0/3 [00:00<?, ?it/s]\n",
            "Downloading data:   0% 0.00/6.98M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading data: 100% 6.98M/6.98M [00:00<00:00, 57.4MB/s]\n",
            "Downloading data files:  33% 1/3 [00:00<00:01,  1.86it/s]\n",
            "Downloading data: 100% 980k/980k [00:00<00:00, 16.3MB/s]\n",
            "Downloading data files:  67% 2/3 [00:00<00:00,  2.14it/s]\n",
            "Downloading data:   0% 0.00/21.5M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading data:  22% 4.80M/21.5M [00:00<00:00, 48.0MB/s]\u001b[A\n",
            "Downloading data: 100% 21.5M/21.5M [00:00<00:00, 76.7MB/s]\n",
            "Downloading data files: 100% 3/3 [00:01<00:00,  1.87it/s]\n",
            "Extracting data files: 100% 3/3 [00:00<00:00, 2298.67it/s]\n",
            "Dataset beer_reviews_label_drift_neg downloaded and prepared to /root/.cache/huggingface/datasets/arize-ai___beer_reviews_label_drift_neg/default/1.0.0/19f47acda7a949b5528784d68d9f6aa4eaa144ba8ecaf7a234e4243e2ad1c4a0. Subsequent calls will reuse this data.\n",
            "100% 3/3 [00:00<00:00, 153.27it/s]\n",
            "Saving into: data/training.json\n",
            "Saving into: data/s2s-training.json\n",
            "Saving into: data/validation.json\n",
            "Saving into: data/s2s-validation.json\n",
            "Saving into: data/production.json\n",
            "Saving into: data/s2s-production.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional, Union, Tuple\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import MSELoss, CrossEntropyLoss, BCEWithLogitsLoss\n",
        "from transformers import RobertaForSequenceClassification, RobertaModel\n",
        "from transformers.modeling_outputs import SequenceClassifierOutput\n"
      ],
      "metadata": {
        "id": "Qn_w1WxnjXm3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RoBERTa**"
      ],
      "metadata": {
        "id": "waeoyRtzkU5I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class RobertaClassificationHeadCustom(nn.Module):\n",
        "    \"\"\"Head for sentence-level classification tasks.\"\"\"\n",
        "\n",
        "    def __init__(self, config,use_hidden_states : bool=False):\n",
        "        super().__init__()\n",
        "        self.use_hidden_states = use_hidden_states\n",
        "        hidden_size = config.hidden_size\n",
        "        if self.use_hidden_states:\n",
        "            hidden_size *= 2\n",
        "\n",
        "        self.dense_1 = nn.Linear(hidden_size, 2 * hidden_size)\n",
        "        self.dense_2 = nn.Linear(2 * hidden_size, hidden_size)\n",
        "        self.dense_3 = nn.Linear(hidden_size, 2 * hidden_size)\n",
        "        self.dense_4 = nn.Linear(2 * hidden_size, hidden_size)\n",
        "        self.dense_5 = nn.Linear(hidden_size, 2 * hidden_size)\n",
        "        self.dense_6 = nn.Linear(2 * hidden_size, hidden_size)\n",
        "        classifier_dropout = (\n",
        "            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n",
        "        )\n",
        "        self.dropout = nn.Dropout(classifier_dropout)\n",
        "        self.out_proj = nn.Linear(hidden_size, config.num_labels)\n",
        "\n",
        "    def forward(self, features, **kwargs):\n",
        "        if 'hidden_states' in kwargs and kwargs['hidden_states'] is not None:\n",
        "            if self.use_hidden_states:\n",
        "                x = torch.cat(\n",
        "                    (\n",
        "                        features[:, 0, :],\n",
        "                        # take <s> token (equiv. to [CLS]) from hidden states from last layer\n",
        "                        kwargs['hidden_states'][-2][:, 0, :]\n",
        "                    ),\n",
        "                    dim=1\n",
        "                )\n",
        "            else:\n",
        "                x = features[:, 0, :] + kwargs['hidden_states'][-2][:, 0, :]\n",
        "        else:\n",
        "            x = features[:, 0, :]  # take <s> token (equiv. to [CLS])\n",
        "            if self.use_hidden_states:\n",
        "                x = torch.cat(\n",
        "                    (\n",
        "                        features[:, 0, :],\n",
        "                        torch.zeros(x.size(), dtype=x.dtype, device=x.device)\n",
        "                    ),\n",
        "                    dim=1\n",
        "                )\n",
        "\n",
        "        x = self.dense_1(x)\n",
        "        x = torch.rrelu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.dense_2(x)\n",
        "        x = torch.rrelu(x)\n",
        "        x = self.dropout(x)\n",
        "        \n",
        "        x = self.dense_3(x)\n",
        "        x = torch.rrelu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.dense_4(x)\n",
        "        x = torch.rrelu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.dense_5(x)\n",
        "        x = torch.rrelu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.dense_6(x)\n",
        "        x = torch.rrelu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.out_proj(x)\n",
        "        return x\n",
        "\n",
        "class RobertaForSequenceClassificationCustom(RobertaForSequenceClassification):\n",
        "    def __init__(self, config,):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "        self.config = config\n",
        "\n",
        "        self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
        "        self.classifier = RobertaClassificationHeadCustom(config, use_hidden_states=True)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            input_ids=None,\n",
        "            attention_mask=None,\n",
        "            token_type_ids=None,\n",
        "            position_ids=None,\n",
        "            head_mask=None,\n",
        "            inputs_embeds=None,\n",
        "            labels=None,\n",
        "            output_attentions=None,\n",
        "            output_hidden_states=None,\n",
        "            return_dict=None,\n",
        "    ):\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        outputs = self.roberta(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "        sequence_output = outputs[0]\n",
        "        if return_dict:\n",
        "            logits = self.classifier(sequence_output, hidden_states=outputs.hidden_states)\n",
        "        else:\n",
        "            raise NotImplemented('Not implemented for using non-dictionary object')\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            if self.config.problem_type is None:\n",
        "                if self.num_labels == 1:\n",
        "                    self.config.problem_type = \"regression\"\n",
        "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
        "                    self.config.problem_type = \"single_label_classification\"\n",
        "                else:\n",
        "                    self.config.problem_type = \"multi_label_classification\"\n",
        "\n",
        "            if self.config.problem_type == \"regression\":\n",
        "                loss_fct = MSELoss()\n",
        "                if self.num_labels == 1:\n",
        "                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
        "                else:\n",
        "                    loss = loss_fct(logits, labels)\n",
        "            elif self.config.problem_type == \"single_label_classification\":\n",
        "                loss_fct = CrossEntropyLoss()\n",
        "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "            elif self.config.problem_type == \"multi_label_classification\":\n",
        "                loss_fct = BCEWithLogitsLoss()\n",
        "                loss = loss_fct(logits, labels)\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (logits,) + outputs[2:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return SequenceClassifierOutput(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )"
      ],
      "metadata": {
        "id": "-KlrGIKIjTVq"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GPT-2**"
      ],
      "metadata": {
        "id": "cPlP7ZDZkcIN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import MSELoss, CrossEntropyLoss, BCEWithLogitsLoss\n",
        "from transformers import GPT2Model, GPT2ForSequenceClassification\n",
        "from transformers.modeling_outputs import SequenceClassifierOutputWithPast\n",
        "\n",
        "\n",
        "class GPT2ClassificationHeadCustom(nn.Module):\n",
        "    def __init__(self, config,):\n",
        "        super().__init__()\n",
        "        hidden_size = config.n_embd\n",
        "        self.dense_1_input = nn.Linear(hidden_size, 2 * hidden_size)\n",
        "        self.dense_1_hidden = nn.Linear(hidden_size, 2 * hidden_size)\n",
        "        self.dense_2 = nn.Linear(4 * hidden_size, hidden_size)\n",
        "        self.dropout = nn.Dropout(config.resid_pdrop)\n",
        "        self.out_proj = nn.Linear(hidden_size, config.num_labels, bias=False)\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        if 'hidden_states' in kwargs and kwargs['hidden_states'] is not None:\n",
        "            # Get hidden states from last layer\n",
        "            hidden = kwargs['hidden_states'][-1]\n",
        "        else:\n",
        "            hidden = torch.zeros(x.size(), dtype=x.dtype, device=x.device)\n",
        "\n",
        "        x = self.dense_1_input(x)\n",
        "        x = torch.rrelu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        hidden = self.dense_1_hidden(hidden)\n",
        "        hidden = torch.rrelu(hidden)\n",
        "        hidden = self.dropout(hidden)\n",
        "\n",
        "        x = torch.cat((x, hidden), dim=2)\n",
        "        x = self.dense_2(x)\n",
        "        x = torch.rrelu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.out_proj(x)\n",
        "        return x\n",
        "\n",
        "class GPT2ForSequenceClassificationCustom(GPT2ForSequenceClassification):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "        self.transformer = GPT2Model(config)\n",
        "        self.score = GPT2ClassificationHeadCustom(config)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "        # Model parallel\n",
        "        self.model_parallel = False\n",
        "        self.device_map = None\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        past_key_values=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "        use_cache=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
        "            Labels for computing the sequence classification/regression loss. Indices should be in :obj:`[0, ...,\n",
        "            config.num_labels - 1]`. If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n",
        "            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        transformer_outputs = self.transformer(\n",
        "            input_ids,\n",
        "            past_key_values=past_key_values,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "        hidden_states = transformer_outputs[0]\n",
        "        if return_dict:\n",
        "            logits = self.score(hidden_states, hidden_states=transformer_outputs.hidden_states)\n",
        "        else:\n",
        "            raise NotImplemented('Not implemented for using non-dictionary object')\n",
        "\n",
        "        if input_ids is not None:\n",
        "            batch_size, sequence_length = input_ids.shape[:2]\n",
        "        else:\n",
        "            batch_size, sequence_length = inputs_embeds.shape[:2]\n",
        "\n",
        "        assert (\n",
        "            self.config.pad_token_id is not None or batch_size == 1\n",
        "        ), \"Cannot handle batch sizes > 1 if no padding token is defined.\"\n",
        "        if self.config.pad_token_id is None:\n",
        "            sequence_lengths = -1\n",
        "        else:\n",
        "            if input_ids is not None:\n",
        "                sequence_lengths = torch.ne(input_ids, self.config.pad_token_id).sum(-1) - 1\n",
        "            else:\n",
        "                sequence_lengths = -1\n",
        "\n",
        "        pooled_logits = logits[range(batch_size), sequence_lengths]\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            if self.num_labels == 1:\n",
        "                #  We are doing regression\n",
        "                loss_fct = MSELoss()\n",
        "                loss = loss_fct(pooled_logits.view(-1), labels.to(self.dtype).view(-1))\n",
        "            else:\n",
        "                loss_fct = CrossEntropyLoss()\n",
        "                loss = loss_fct(pooled_logits.view(-1, self.num_labels), labels.view(-1))\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (pooled_logits,) + transformer_outputs[1:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return SequenceClassifierOutputWithPast(\n",
        "            loss=loss,\n",
        "            logits=pooled_logits,\n",
        "            past_key_values=transformer_outputs.past_key_values,\n",
        "            hidden_states=transformer_outputs.hidden_states,\n",
        "            attentions=transformer_outputs.attentions,\n",
        "        )"
      ],
      "metadata": {
        "id": "PdXbua6Fju1g"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget 'https://raw.githubusercontent.com/huggingface/transformers/v4.23.1/examples/pytorch/text-classification/run_glue.py' -O 'original_run_glue.py'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lfxDChLhY9I",
        "outputId": "ce21774d-9257-4a54-cd73-01631bee70e0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-15 18:17:09--  https://raw.githubusercontent.com/huggingface/transformers/v4.23.1/examples/pytorch/text-classification/run_glue.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 27259 (27K) [text/plain]\n",
            "Saving to: ‘original_run_glue.py’\n",
            "\n",
            "original_run_glue.p 100%[===================>]  26.62K  --.-KB/s    in 0.003s  \n",
            "\n",
            "2023-02-15 18:17:09 (8.21 MB/s) - ‘original_run_glue.py’ saved [27259/27259]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git diff --no-index original_run_glue.py run_glue.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPIrJcQQhc2g",
        "outputId": "28fe89b0-b195-4041-dcea-a7b2006a6fd9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mdiff --git a/original_run_glue.py b/run_glue.py\u001b[m\n",
            "\u001b[1mindex 3eb423f..346ea99 100644\u001b[m\n",
            "\u001b[1m--- a/original_run_glue.py\u001b[m\n",
            "\u001b[1m+++ b/run_glue.py\u001b[m\n",
            "\u001b[36m@@ -20,6 +20,7 @@\u001b[m \u001b[mimport logging\u001b[m\n",
            " import os\u001b[m\n",
            " import random\u001b[m\n",
            " import sys\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32mfrom collections import defaultdict\u001b[m\n",
            " from dataclasses import dataclass, field\u001b[m\n",
            " from typing import Optional\u001b[m\n",
            " \u001b[m\n",
            "\u001b[36m@@ -47,6 +48,14 @@\u001b[m \u001b[mfrom transformers.utils import check_min_version, send_example_telemetry\u001b[m\n",
            " from transformers.utils.versions import require_version\u001b[m\n",
            " \u001b[m\n",
            " \u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32mfrom roberta import  RobertaForSequenceClassificationCustom\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32mfrom gpt2 import GPT2ForSequenceClassificationCustom\u001b[m\n",
            "\u001b[32m+\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32mMODEL_NAME_TO_CLASS = {\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m    'roberta_hidden': RobertaForSequenceClassificationCustom,\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m    'gpt2_hidden': GPT2ForSequenceClassificationCustom,\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m}\u001b[m\n",
            "\u001b[32m+\u001b[m\n",
            " # Will error if the minimal version of Transformers is not installed. Remove at your own risks.\u001b[m\n",
            " check_min_version(\"4.23.0\")\u001b[m\n",
            " \u001b[m\n",
            "\u001b[36m@@ -201,6 +210,13 @@\u001b[m \u001b[mclass ModelArguments:\u001b[m\n",
            "         default=False,\u001b[m\n",
            "         metadata={\"help\": \"Will enable to load a pretrained model whose head dimensions are different.\"},\u001b[m\n",
            "     )\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m    custom_model: str = field(\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m        default=None,\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m        metadata={\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m            \"help\": \"Use custom implementation from available list\",\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m            \"choices\": list(MODEL_NAME_TO_CLASS.keys()),\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m        },\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m    )\u001b[m\n",
            " \u001b[m\n",
            " \u001b[m\n",
            " def main():\u001b[m\n",
            "\u001b[36m@@ -358,6 +374,7 @@\u001b[m \u001b[mdef main():\u001b[m\n",
            "         cache_dir=model_args.cache_dir,\u001b[m\n",
            "         revision=model_args.model_revision,\u001b[m\n",
            "         use_auth_token=True if model_args.use_auth_token else None,\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m        use_hidden_states = True\u001b[m\n",
            "     )\u001b[m\n",
            "     tokenizer = AutoTokenizer.from_pretrained(\u001b[m\n",
            "         model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\u001b[m\n",
            "\u001b[36m@@ -366,7 +383,25 @@\u001b[m \u001b[mdef main():\u001b[m\n",
            "         revision=model_args.model_revision,\u001b[m\n",
            "         use_auth_token=True if model_args.use_auth_token else None,\u001b[m\n",
            "     )\u001b[m\n",
            "\u001b[31m-    model = AutoModelForSequenceClassification.from_pretrained(\u001b[m\n",
            "\u001b[32m+\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m    custom_model = model_args.custom_model\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m    if custom_model is not None:\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m        # Check model and implementation is the same\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m        if 'roberta' in custom_model and 'roberta' not in model_args.model_name_or_path:\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m            raise RuntimeError('Model and custom implementation should be the same type: RoBERTa')\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m        elif 'gpt2' in custom_model and 'gpt2' not in model_args.model_name_or_path:\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m            raise RuntimeError('Model and custom implementation should be the same type: GPT-2')\u001b[m\n",
            "\u001b[32m+\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m        # Set custom configuration in model configuration\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m        config.use_hidden_states = 'hidden' in custom_model\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m        logger.info(f'Using hidden states in model: {config.use_hidden_states}')\u001b[m\n",
            "\u001b[32m+\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m        # Get class to initialize model\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m        model_cls = MODEL_NAME_TO_CLASS[custom_model]\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m    else:\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m        model_cls = AutoModelForSequenceClassification\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m    logger.info(f'Using implementation from class: {model_cls.__name__}')\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m    model = model_cls.from_pretrained(\u001b[m\n",
            "         model_args.model_name_or_path,\u001b[m\n",
            "         from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\u001b[m\n",
            "         config=config,\u001b[m\n",
            "\u001b[36m@@ -376,6 +411,21 @@\u001b[m \u001b[mdef main():\u001b[m\n",
            "         ignore_mismatched_sizes=model_args.ignore_mismatched_sizes,\u001b[m\n",
            "     )\u001b[m\n",
            " \u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m    if model_args.model_name_or_path.startswith('roberta') or model_args.model_name_or_path.startswith('gpt2'):\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m        for name, param in model.named_parameters():\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m            if 'block' in name:\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m                layer_number = name.split('.')[2]\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m                if int(layer_number)%5 ==0:\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m                    param.requires_grad = False\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m        frozen_layer = [(name,param.requires_grad)for(name,param) in model.named_parameters()]\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m        print(\"\\n\\n\\n frozen Layer\")\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m        print(frozen_layer)\u001b[m\n",
            "\u001b[32m+\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m    if 'gpt2' in tokenizer.name_or_path and tokenizer.pad_token is None:\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m        logger.info(f'Set PAD token to EOS: {tokenizer.eos_token}')\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m        tokenizer._pad_token = tokenizer.eos_token\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m        model.config.pad_token_id = model.config.eos_token_id\u001b[m\n",
            "\u001b[32m+\u001b[m\n",
            "     # Preprocessing the raw_datasets\u001b[m\n",
            "     if data_args.task_name is not None:\u001b[m\n",
            "         sentence1_key, sentence2_key = task_to_keys[data_args.task_name]\u001b[m\n",
            "\u001b[36m@@ -464,7 +514,16 @@\u001b[m \u001b[mdef main():\u001b[m\n",
            "         eval_dataset = raw_datasets[\"validation_matched\" if data_args.task_name == \"mnli\" else \"validation\"]\u001b[m\n",
            "         if data_args.max_eval_samples is not None:\u001b[m\n",
            "             max_eval_samples = min(len(eval_dataset), data_args.max_eval_samples)\u001b[m\n",
            "\u001b[31m-            eval_dataset = eval_dataset.select(range(max_eval_samples))\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m            label_to_indexes = defaultdict(list)\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m            for index, eval_sample in enumerate(eval_dataset):\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m                label_to_indexes[eval_sample['label']].append(index)\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m            max_samples_per_label = int(max_eval_samples / len(label_to_indexes))\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m            eval_sample_indexes = []\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m            for label, indexes in label_to_indexes.items():\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m                eval_sample_indexes.extend(indexes[:max_samples_per_label])\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m                logger.info(f\"Set {max_samples_per_label} samples for {label}-class\")\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m            eval_sample_indexes.sort()\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m            eval_dataset = eval_dataset.select(eval_sample_indexes)\u001b[m\n",
            " \u001b[m\n",
            "     if training_args.do_predict or data_args.task_name is not None or data_args.test_file is not None:\u001b[m\n",
            "         if \"test\" not in raw_datasets and \"test_matched\" not in raw_datasets:\u001b[m\n",
            "\u001b[36m@@ -521,13 +580,14 @@\u001b[m \u001b[mdef main():\u001b[m\n",
            "     )\u001b[m\n",
            " \u001b[m\n",
            "     # Training\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m    ignore_keys_for_eval = ['hidden_states', 'attentions', 'past_key_values']\u001b[m\n",
            "     if training_args.do_train:\u001b[m\n",
            "         checkpoint = None\u001b[m\n",
            "         if training_args.resume_from_checkpoint is not None:\u001b[m\n",
            "             checkpoint = training_args.resume_from_checkpoint\u001b[m\n",
            "         elif last_checkpoint is not None:\u001b[m\n",
            "             checkpoint = last_checkpoint\u001b[m\n",
            "\u001b[31m-        train_result = trainer.train(resume_from_checkpoint=checkpoint)\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m        train_result = trainer.train(resume_from_checkpoint=checkpoint, ignore_keys_for_eval=ignore_keys_for_eval)\u001b[m\n",
            "         metrics = train_result.metrics\u001b[m\n",
            "         max_train_samples = (\u001b[m\n",
            "             data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)\u001b[m\n",
            "\u001b[36m@@ -557,7 +617,7 @@\u001b[m \u001b[mdef main():\u001b[m\n",
            "             combined = {}\u001b[m\n",
            " \u001b[m\n",
            "         for eval_dataset, task in zip(eval_datasets, tasks):\u001b[m\n",
            "\u001b[31m-            metrics = trainer.evaluate(eval_dataset=eval_dataset)\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m            metrics = trainer.evaluate(eval_dataset=eval_dataset, ignore_keys=ignore_keys_for_eval)\u001b[m\n",
            " \u001b[m\n",
            "             max_eval_samples = (\u001b[m\n",
            "                 data_args.max_eval_samples if data_args.max_eval_samples is not None else len(eval_dataset)\u001b[m\n",
            "\u001b[36m@@ -585,7 +645,7 @@\u001b[m \u001b[mdef main():\u001b[m\n",
            "         for predict_dataset, task in zip(predict_datasets, tasks):\u001b[m\n",
            "             # Removing the `label` columns because it contains -1 and Trainer won't like that.\u001b[m\n",
            "             predict_dataset = predict_dataset.remove_columns(\"label\")\u001b[m\n",
            "\u001b[31m-            predictions = trainer.predict(predict_dataset, metric_key_prefix=\"predict\").predictions\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m            predictions = trainer.predict(predict_dataset, metric_key_prefix=\"predict\", ignore_keys=ignore_keys_for_eval).predictions\u001b[m\n",
            "             predictions = np.squeeze(predictions) if is_regression else np.argmax(predictions, axis=1)\u001b[m\n",
            " \u001b[m\n",
            "             output_predict_file = os.path.join(training_args.output_dir, f\"predict_results_{task}.txt\")\u001b[m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget 'https://raw.githubusercontent.com/huggingface/transformers/v4.23.1/examples/pytorch/translation/run_translation.py' -O 'original_run_translation.py'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MhHACpbi5uJ",
        "outputId": "87b27775-487d-4946-9b57-009e92858dc4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-15 18:17:10--  https://raw.githubusercontent.com/huggingface/transformers/v4.23.1/examples/pytorch/translation/run_translation.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 28240 (28K) [text/plain]\n",
            "Saving to: ‘original_run_translation.py’\n",
            "\n",
            "original_run_transl 100%[===================>]  27.58K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2023-02-15 18:17:10 (14.0 MB/s) - ‘original_run_translation.py’ saved [28240/28240]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git diff --no-index original_run_translation.py run_translation.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TL71DZC0i9DF",
        "outputId": "46f7c754-400d-478d-b510-7d4415f89817"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mdiff --git a/original_run_translation.py b/run_translation.py\u001b[m\n",
            "\u001b[1mindex ce8a04f..606ce00 100644\u001b[m\n",
            "\u001b[1m--- a/original_run_translation.py\u001b[m\n",
            "\u001b[1m+++ b/run_translation.py\u001b[m\n",
            "\u001b[36m@@ -21,11 +21,13 @@\u001b[m \u001b[mFine-tuning the library models for sequence to sequence.\u001b[m\n",
            " import logging\u001b[m\n",
            " import os\u001b[m\n",
            " import sys\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32mfrom collections import defaultdict\u001b[m\n",
            " from dataclasses import dataclass, field\u001b[m\n",
            " from typing import Optional\u001b[m\n",
            " \u001b[m\n",
            " import datasets\u001b[m\n",
            " import numpy as np\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32mimport torch\u001b[m\n",
            " from datasets import load_dataset\u001b[m\n",
            " \u001b[m\n",
            " import evaluate\u001b[m\n",
            "\u001b[36m@@ -62,6 +64,9 @@\u001b[m \u001b[mlogger = logging.getLogger(__name__)\u001b[m\n",
            " MULTILINGUAL_TOKENIZERS = [MBartTokenizer, MBartTokenizerFast, MBart50Tokenizer, MBart50TokenizerFast, M2M100Tokenizer]\u001b[m\n",
            " \u001b[m\n",
            " \u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32mMAP_CLASSIFICATION_LABEL = {'positive': 2,'neutral':1, 'negative': 0}\u001b[m\n",
            "\u001b[32m+\u001b[m\n",
            "\u001b[32m+\u001b[m\n",
            " @dataclass\u001b[m\n",
            " class ModelArguments:\u001b[m\n",
            "     \"\"\"\u001b[m\n",
            "\u001b[36m@@ -98,6 +103,10 @@\u001b[m \u001b[mclass ModelArguments:\u001b[m\n",
            "             )\u001b[m\n",
            "         },\u001b[m\n",
            "     )\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m    freeze_weights: bool = field(\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m        default=False,\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m        metadata={\"help\": \"Freeze encoder weights\"},\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m    )\u001b[m\n",
            " \u001b[m\n",
            " \u001b[m\n",
            " @dataclass\u001b[m\n",
            "\u001b[36m@@ -248,6 +257,11 @@\u001b[m \u001b[mclass DataTrainingArguments:\u001b[m\n",
            "             self.val_max_target_length = self.max_target_length\u001b[m\n",
            " \u001b[m\n",
            " \u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32mdef freeze_model_weights(model: torch.nn.Module) -> None:\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m    for param in model.parameters():\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m        param.requires_grad = False\u001b[m\n",
            "\u001b[32m+\u001b[m\n",
            "\u001b[32m+\u001b[m\n",
            " def main():\u001b[m\n",
            "     # See all possible arguments in src/transformers/training_args.py\u001b[m\n",
            "     # or by passing the --help flag to this script.\u001b[m\n",
            "\u001b[36m@@ -380,6 +394,20 @@\u001b[m \u001b[mdef main():\u001b[m\n",
            "         use_auth_token=True if model_args.use_auth_token else None,\u001b[m\n",
            "     )\u001b[m\n",
            " \u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m    if model_args.model_name_or_path.startswith('google/t5'):\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m        for name, param in model.named_parameters():\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m            if 'block' in name:\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m                layer_number = name.split('.')[2]\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m                if int(layer_number)%5 ==0:\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m                    param.requires_grad = False\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m        frozen_layer = [(name,param.requires_grad)for(name,param) in model.named_parameters()]\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m        print(\"\\n\\n\\n frozen Layer\")\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m        print(frozen_layer)\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[41m        \u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m    if model_args.freeze_weights:\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m        logger.info(\"Freezing encoder weights\")\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m        freeze_model_weights(model.encoder)\u001b[m\n",
            "\u001b[32m+\u001b[m\n",
            "     model.resize_token_embeddings(len(tokenizer))\u001b[m\n",
            " \u001b[m\n",
            "     # Set decoder_start_token_id\u001b[m\n",
            "\u001b[36m@@ -393,6 +421,13 @@\u001b[m \u001b[mdef main():\u001b[m\n",
            "         raise ValueError(\"Make sure that `config.decoder_start_token_id` is correctly defined\")\u001b[m\n",
            " \u001b[m\n",
            "     prefix = data_args.source_prefix if data_args.source_prefix is not None else \"\"\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m    if 'classification' not in prefix:\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m        raise RuntimeError('Not found \"classification\" prefix!')\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m    prefix = prefix.strip()\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m    if not prefix.endswith(':'):\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m        prefix += ':'\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m    prefix += ' '\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m    logger.info(f'Using translation prefix: \"{prefix}\"')\u001b[m\n",
            " \u001b[m\n",
            "     # Preprocessing the datasets.\u001b[m\n",
            "     # We need to tokenize inputs and targets.\u001b[m\n",
            "\u001b[36m@@ -414,8 +449,8 @@\u001b[m \u001b[mdef main():\u001b[m\n",
            "             \"--target_lang arguments.\"\u001b[m\n",
            "         )\u001b[m\n",
            " \u001b[m\n",
            "\u001b[31m-        tokenizer.src_lang = data_args.source_lang\u001b[m\n",
            "\u001b[31m-        tokenizer.tgt_lang = data_args.target_lang\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m        tokenizer.src_lang = 'text'\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m        tokenizer.tgt_lang = 'label'\u001b[m\n",
            " \u001b[m\n",
            "         # For multilingual translation models like mBART-50 and M2M100 we need to force the target language token\u001b[m\n",
            "         # as the first generated token. We ask the user to explicitly provide this as --forced_bos_token argument.\u001b[m\n",
            "\u001b[36m@@ -425,8 +460,8 @@\u001b[m \u001b[mdef main():\u001b[m\n",
            "         model.config.forced_bos_token_id = forced_bos_token_id\u001b[m\n",
            " \u001b[m\n",
            "     # Get the language codes for input/target.\u001b[m\n",
            "\u001b[31m-    source_lang = data_args.source_lang.split(\"_\")[0]\u001b[m\n",
            "\u001b[31m-    target_lang = data_args.target_lang.split(\"_\")[0]\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m    source_lang = 'text'\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m    target_lang = 'label'\u001b[m\n",
            " \u001b[m\n",
            "     # Temporarily set max_target_length for training.\u001b[m\n",
            "     max_target_length = data_args.max_target_length\u001b[m\n",
            "\u001b[36m@@ -439,8 +474,8 @@\u001b[m \u001b[mdef main():\u001b[m\n",
            "         )\u001b[m\n",
            " \u001b[m\n",
            "     def preprocess_function(examples):\u001b[m\n",
            "\u001b[31m-        inputs = [ex[source_lang] for ex in examples[\"translation\"]]\u001b[m\n",
            "\u001b[31m-        targets = [ex[target_lang] for ex in examples[\"translation\"]]\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m        inputs = [ex for ex in examples[source_lang]]\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m        targets = [ex for ex in examples[target_lang]]\u001b[m\n",
            "         inputs = [prefix + inp for inp in inputs]\u001b[m\n",
            "         model_inputs = tokenizer(inputs, max_length=data_args.max_source_length, padding=padding, truncation=True)\u001b[m\n",
            " \u001b[m\n",
            "\u001b[36m@@ -481,7 +516,16 @@\u001b[m \u001b[mdef main():\u001b[m\n",
            "         eval_dataset = raw_datasets[\"validation\"]\u001b[m\n",
            "         if data_args.max_eval_samples is not None:\u001b[m\n",
            "             max_eval_samples = min(len(eval_dataset), data_args.max_eval_samples)\u001b[m\n",
            "\u001b[31m-            eval_dataset = eval_dataset.select(range(max_eval_samples))\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m            label_to_indexes = defaultdict(list)\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m            for index, eval_sample in enumerate(eval_dataset):\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m                label_to_indexes[eval_sample['label']].append(index)\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m            max_samples_per_label = int(max_eval_samples / len(label_to_indexes))\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m            eval_sample_indexes = []\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m            for label, indexes in label_to_indexes.items():\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m                eval_sample_indexes.extend(indexes[:max_samples_per_label])\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m                logger.info(f\"Set {max_samples_per_label} samples for {label}-class\")\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m            eval_sample_indexes.sort()\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m            eval_dataset = eval_dataset.select(eval_sample_indexes)\u001b[m\n",
            "         with training_args.main_process_first(desc=\"validation dataset map pre-processing\"):\u001b[m\n",
            "             eval_dataset = eval_dataset.map(\u001b[m\n",
            "                 preprocess_function,\u001b[m\n",
            "\u001b[36m@@ -524,6 +568,7 @@\u001b[m \u001b[mdef main():\u001b[m\n",
            " \u001b[m\n",
            "     # Metric\u001b[m\n",
            "     metric = evaluate.load(\"sacrebleu\")\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m    metric_accuracy = evaluate.load(\"accuracy\")\u001b[m\n",
            " \u001b[m\n",
            "     def postprocess_text(preds, labels):\u001b[m\n",
            "         preds = [pred.strip() for pred in preds]\u001b[m\n",
            "\u001b[36m@@ -543,9 +588,12 @@\u001b[m \u001b[mdef main():\u001b[m\n",
            " \u001b[m\n",
            "         # Some simple post-processing\u001b[m\n",
            "         decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m        decoded_preds_accuracy = [MAP_CLASSIFICATION_LABEL.get(decoded_pred, -1) for decoded_pred in decoded_preds]\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m        decoded_labels_accuracy = [MAP_CLASSIFICATION_LABEL.get(decoded_label[0], -1) for decoded_label in decoded_labels]\u001b[m\n",
            " \u001b[m\n",
            "         result = metric.compute(predictions=decoded_preds, references=decoded_labels)\u001b[m\n",
            "\u001b[31m-        result = {\"bleu\": result[\"score\"]}\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m        result_accuracy = metric_accuracy.compute(predictions=decoded_preds_accuracy, references=decoded_labels_accuracy)\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m        result = {\"bleu\": result[\"score\"], \"accuracy\": result_accuracy[\"accuracy\"]}\u001b[m\n",
            " \u001b[m\n",
            "         prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\u001b[m\n",
            "         result[\"gen_len\"] = np.mean(prediction_lens)\u001b[m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Custom classifier head with custom forward-RoBERTa**"
      ],
      "metadata": {
        "id": "3-A82YIrk9o1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! python run_glue.py \\\n",
        "  --cache_dir .cache_training \\\n",
        "  --model_name_or_path roberta-base \\\n",
        "  --custom_model roberta_hidden \\\n",
        "  --train_file data/training.json  \\\n",
        "  --validation_file data/validation.json \\\n",
        "  --test_file data/production.json \\\n",
        "  --per_device_train_batch_size 8 \\\n",
        "  --per_device_eval_batch_size 8\\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --do_predict \\\n",
        "  --max_seq_length 128 \\\n",
        "  --learning_rate 2e-5 \\\n",
        "  --max_eval_samples 2000 \\\n",
        "  --max_steps 2500 \\\n",
        "  --num_train_epochs 1 \\\n",
        "  --save_strategy steps \\\n",
        "  --save_steps 250 \\\n",
        "  --save_total_limit 5 \\\n",
        "  --logging_strategy steps \\\n",
        "  --logging_steps 100 \\\n",
        "  --eval_steps 250 \\\n",
        "  --evaluation_strategy steps \\\n",
        "  --metric_for_best_model accuracy \\\n",
        "  --greater_is_better True \\\n",
        "  --load_best_model_at_end True \\\n",
        "  --output_dir out/beer_reviews/roberta_hidden_v1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lu3PnYGxgaur",
        "outputId": "764c9592-3bcf-4dfb-c33f-3ef4d883201f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-15 18:17:17.325129: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-02-15 18:17:18.549113: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-15 18:17:18.549236: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-15 18:17:18.549254: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=250,\n",
            "evaluation_strategy=steps,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=passive,\n",
            "log_on_each_node=True,\n",
            "logging_dir=out/beer_reviews/roberta_hidden_v1/runs/Feb15_18-17-20_ed170e621d72,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=100,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=2500,\n",
            "metric_for_best_model=accuracy,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=1.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=out/beer_reviews/roberta_hidden_v1,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=out/beer_reviews/roberta_hidden_v1,\n",
            "save_on_each_node=False,\n",
            "save_steps=250,\n",
            "save_strategy=steps,\n",
            "save_total_limit=5,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "INFO:__main__:load a local file for train: data/training.json\n",
            "INFO:__main__:load a local file for validation: data/validation.json\n",
            "INFO:__main__:load a local file for test: data/production.json\n",
            "WARNING:datasets.builder:Using custom data configuration default-618d4135584dd38b\n",
            "INFO:datasets.info:Loading Dataset Infos from /usr/local/lib/python3.8/dist-packages/datasets/packaged_modules/json\n",
            "INFO:datasets.builder:Generating dataset json (/content/uczenie_glebokie/.cache_training/json/default-618d4135584dd38b/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n",
            "Downloading and preparing dataset json/default to /content/uczenie_glebokie/.cache_training/json/default-618d4135584dd38b/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51...\n",
            "Downloading data files: 100% 3/3 [00:00<00:00, 13457.66it/s]\n",
            "INFO:datasets.download.download_manager:Downloading took 0.0 min\n",
            "INFO:datasets.download.download_manager:Checksum Computation took 0.0 min\n",
            "Extracting data files: 100% 3/3 [00:00<00:00, 2308.79it/s]\n",
            "INFO:datasets.utils.info_utils:Unable to verify checksums.\n",
            "INFO:datasets.builder:Generating train split\n",
            "INFO:datasets.builder:Generating validation split\n",
            "INFO:datasets.builder:Generating test split\n",
            "INFO:datasets.utils.info_utils:Unable to verify splits sizes.\n",
            "Dataset json downloaded and prepared to /content/uczenie_glebokie/.cache_training/json/default-618d4135584dd38b/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51. Subsequent calls will reuse this data.\n",
            "100% 3/3 [00:00<00:00, 1005.67it/s]\n",
            "Downloading (…)lve/main/config.json: 100% 481/481 [00:00<00:00, 69.5kB/s]\n",
            "[INFO|configuration_utils.py:653] 2023-02-15 18:17:22,227 >> loading configuration file config.json from cache at .cache_training/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/config.json\n",
            "[INFO|configuration_utils.py:705] 2023-02-15 18:17:22,228 >> Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.23.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:418] 2023-02-15 18:17:22,365 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:653] 2023-02-15 18:17:22,503 >> loading configuration file config.json from cache at .cache_training/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/config.json\n",
            "[INFO|configuration_utils.py:705] 2023-02-15 18:17:22,504 >> Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.23.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "Downloading (…)olve/main/vocab.json: 100% 899k/899k [00:00<00:00, 6.00MB/s]\n",
            "Downloading (…)olve/main/merges.txt: 100% 456k/456k [00:00<00:00, 3.15MB/s]\n",
            "Downloading (…)/main/tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 7.48MB/s]\n",
            "[INFO|tokenization_utils_base.py:1773] 2023-02-15 18:17:24,295 >> loading file vocab.json from cache at .cache_training/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/vocab.json\n",
            "[INFO|tokenization_utils_base.py:1773] 2023-02-15 18:17:24,295 >> loading file merges.txt from cache at .cache_training/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/merges.txt\n",
            "[INFO|tokenization_utils_base.py:1773] 2023-02-15 18:17:24,295 >> loading file tokenizer.json from cache at .cache_training/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1773] 2023-02-15 18:17:24,295 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1773] 2023-02-15 18:17:24,295 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1773] 2023-02-15 18:17:24,295 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:653] 2023-02-15 18:17:24,295 >> loading configuration file config.json from cache at .cache_training/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/config.json\n",
            "[INFO|configuration_utils.py:705] 2023-02-15 18:17:24,296 >> Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.23.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "INFO:__main__:Using hidden states in model: True\n",
            "INFO:__main__:Using implementation from class: RobertaForSequenceClassificationCustom\n",
            "Downloading (…)\"pytorch_model.bin\";: 100% 501M/501M [00:04<00:00, 104MB/s]\n",
            "[INFO|modeling_utils.py:2156] 2023-02-15 18:17:29,402 >> loading weights file pytorch_model.bin from cache at .cache_training/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/pytorch_model.bin\n",
            "[WARNING|modeling_utils.py:2596] 2023-02-15 18:17:32,824 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassificationCustom: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassificationCustom from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassificationCustom from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:2608] 2023-02-15 18:17:32,824 >> Some weights of RobertaForSequenceClassificationCustom were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense_3.weight', 'classifier.dense_2.weight', 'classifier.dense_1.weight', 'classifier.dense_5.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense_4.bias', 'classifier.dense_6.bias', 'classifier.dense_2.bias', 'classifier.dense_5.weight', 'classifier.dense_4.weight', 'classifier.dense_3.bias', 'classifier.dense_1.bias', 'classifier.dense_6.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "\n",
            "\n",
            " frozen Layer\n",
            "[('roberta.embeddings.word_embeddings.weight', True), ('roberta.embeddings.position_embeddings.weight', True), ('roberta.embeddings.token_type_embeddings.weight', True), ('roberta.embeddings.LayerNorm.weight', True), ('roberta.embeddings.LayerNorm.bias', True), ('roberta.encoder.layer.0.attention.self.query.weight', True), ('roberta.encoder.layer.0.attention.self.query.bias', True), ('roberta.encoder.layer.0.attention.self.key.weight', True), ('roberta.encoder.layer.0.attention.self.key.bias', True), ('roberta.encoder.layer.0.attention.self.value.weight', True), ('roberta.encoder.layer.0.attention.self.value.bias', True), ('roberta.encoder.layer.0.attention.output.dense.weight', True), ('roberta.encoder.layer.0.attention.output.dense.bias', True), ('roberta.encoder.layer.0.attention.output.LayerNorm.weight', True), ('roberta.encoder.layer.0.attention.output.LayerNorm.bias', True), ('roberta.encoder.layer.0.intermediate.dense.weight', True), ('roberta.encoder.layer.0.intermediate.dense.bias', True), ('roberta.encoder.layer.0.output.dense.weight', True), ('roberta.encoder.layer.0.output.dense.bias', True), ('roberta.encoder.layer.0.output.LayerNorm.weight', True), ('roberta.encoder.layer.0.output.LayerNorm.bias', True), ('roberta.encoder.layer.1.attention.self.query.weight', True), ('roberta.encoder.layer.1.attention.self.query.bias', True), ('roberta.encoder.layer.1.attention.self.key.weight', True), ('roberta.encoder.layer.1.attention.self.key.bias', True), ('roberta.encoder.layer.1.attention.self.value.weight', True), ('roberta.encoder.layer.1.attention.self.value.bias', True), ('roberta.encoder.layer.1.attention.output.dense.weight', True), ('roberta.encoder.layer.1.attention.output.dense.bias', True), ('roberta.encoder.layer.1.attention.output.LayerNorm.weight', True), ('roberta.encoder.layer.1.attention.output.LayerNorm.bias', True), ('roberta.encoder.layer.1.intermediate.dense.weight', True), ('roberta.encoder.layer.1.intermediate.dense.bias', True), ('roberta.encoder.layer.1.output.dense.weight', True), ('roberta.encoder.layer.1.output.dense.bias', True), ('roberta.encoder.layer.1.output.LayerNorm.weight', True), ('roberta.encoder.layer.1.output.LayerNorm.bias', True), ('roberta.encoder.layer.2.attention.self.query.weight', True), ('roberta.encoder.layer.2.attention.self.query.bias', True), ('roberta.encoder.layer.2.attention.self.key.weight', True), ('roberta.encoder.layer.2.attention.self.key.bias', True), ('roberta.encoder.layer.2.attention.self.value.weight', True), ('roberta.encoder.layer.2.attention.self.value.bias', True), ('roberta.encoder.layer.2.attention.output.dense.weight', True), ('roberta.encoder.layer.2.attention.output.dense.bias', True), ('roberta.encoder.layer.2.attention.output.LayerNorm.weight', True), ('roberta.encoder.layer.2.attention.output.LayerNorm.bias', True), ('roberta.encoder.layer.2.intermediate.dense.weight', True), ('roberta.encoder.layer.2.intermediate.dense.bias', True), ('roberta.encoder.layer.2.output.dense.weight', True), ('roberta.encoder.layer.2.output.dense.bias', True), ('roberta.encoder.layer.2.output.LayerNorm.weight', True), ('roberta.encoder.layer.2.output.LayerNorm.bias', True), ('roberta.encoder.layer.3.attention.self.query.weight', True), ('roberta.encoder.layer.3.attention.self.query.bias', True), ('roberta.encoder.layer.3.attention.self.key.weight', True), ('roberta.encoder.layer.3.attention.self.key.bias', True), ('roberta.encoder.layer.3.attention.self.value.weight', True), ('roberta.encoder.layer.3.attention.self.value.bias', True), ('roberta.encoder.layer.3.attention.output.dense.weight', True), ('roberta.encoder.layer.3.attention.output.dense.bias', True), ('roberta.encoder.layer.3.attention.output.LayerNorm.weight', True), ('roberta.encoder.layer.3.attention.output.LayerNorm.bias', True), ('roberta.encoder.layer.3.intermediate.dense.weight', True), ('roberta.encoder.layer.3.intermediate.dense.bias', True), ('roberta.encoder.layer.3.output.dense.weight', True), ('roberta.encoder.layer.3.output.dense.bias', True), ('roberta.encoder.layer.3.output.LayerNorm.weight', True), ('roberta.encoder.layer.3.output.LayerNorm.bias', True), ('roberta.encoder.layer.4.attention.self.query.weight', True), ('roberta.encoder.layer.4.attention.self.query.bias', True), ('roberta.encoder.layer.4.attention.self.key.weight', True), ('roberta.encoder.layer.4.attention.self.key.bias', True), ('roberta.encoder.layer.4.attention.self.value.weight', True), ('roberta.encoder.layer.4.attention.self.value.bias', True), ('roberta.encoder.layer.4.attention.output.dense.weight', True), ('roberta.encoder.layer.4.attention.output.dense.bias', True), ('roberta.encoder.layer.4.attention.output.LayerNorm.weight', True), ('roberta.encoder.layer.4.attention.output.LayerNorm.bias', True), ('roberta.encoder.layer.4.intermediate.dense.weight', True), ('roberta.encoder.layer.4.intermediate.dense.bias', True), ('roberta.encoder.layer.4.output.dense.weight', True), ('roberta.encoder.layer.4.output.dense.bias', True), ('roberta.encoder.layer.4.output.LayerNorm.weight', True), ('roberta.encoder.layer.4.output.LayerNorm.bias', True), ('roberta.encoder.layer.5.attention.self.query.weight', True), ('roberta.encoder.layer.5.attention.self.query.bias', True), ('roberta.encoder.layer.5.attention.self.key.weight', True), ('roberta.encoder.layer.5.attention.self.key.bias', True), ('roberta.encoder.layer.5.attention.self.value.weight', True), ('roberta.encoder.layer.5.attention.self.value.bias', True), ('roberta.encoder.layer.5.attention.output.dense.weight', True), ('roberta.encoder.layer.5.attention.output.dense.bias', True), ('roberta.encoder.layer.5.attention.output.LayerNorm.weight', True), ('roberta.encoder.layer.5.attention.output.LayerNorm.bias', True), ('roberta.encoder.layer.5.intermediate.dense.weight', True), ('roberta.encoder.layer.5.intermediate.dense.bias', True), ('roberta.encoder.layer.5.output.dense.weight', True), ('roberta.encoder.layer.5.output.dense.bias', True), ('roberta.encoder.layer.5.output.LayerNorm.weight', True), ('roberta.encoder.layer.5.output.LayerNorm.bias', True), ('roberta.encoder.layer.6.attention.self.query.weight', True), ('roberta.encoder.layer.6.attention.self.query.bias', True), ('roberta.encoder.layer.6.attention.self.key.weight', True), ('roberta.encoder.layer.6.attention.self.key.bias', True), ('roberta.encoder.layer.6.attention.self.value.weight', True), ('roberta.encoder.layer.6.attention.self.value.bias', True), ('roberta.encoder.layer.6.attention.output.dense.weight', True), ('roberta.encoder.layer.6.attention.output.dense.bias', True), ('roberta.encoder.layer.6.attention.output.LayerNorm.weight', True), ('roberta.encoder.layer.6.attention.output.LayerNorm.bias', True), ('roberta.encoder.layer.6.intermediate.dense.weight', True), ('roberta.encoder.layer.6.intermediate.dense.bias', True), ('roberta.encoder.layer.6.output.dense.weight', True), ('roberta.encoder.layer.6.output.dense.bias', True), ('roberta.encoder.layer.6.output.LayerNorm.weight', True), ('roberta.encoder.layer.6.output.LayerNorm.bias', True), ('roberta.encoder.layer.7.attention.self.query.weight', True), ('roberta.encoder.layer.7.attention.self.query.bias', True), ('roberta.encoder.layer.7.attention.self.key.weight', True), ('roberta.encoder.layer.7.attention.self.key.bias', True), ('roberta.encoder.layer.7.attention.self.value.weight', True), ('roberta.encoder.layer.7.attention.self.value.bias', True), ('roberta.encoder.layer.7.attention.output.dense.weight', True), ('roberta.encoder.layer.7.attention.output.dense.bias', True), ('roberta.encoder.layer.7.attention.output.LayerNorm.weight', True), ('roberta.encoder.layer.7.attention.output.LayerNorm.bias', True), ('roberta.encoder.layer.7.intermediate.dense.weight', True), ('roberta.encoder.layer.7.intermediate.dense.bias', True), ('roberta.encoder.layer.7.output.dense.weight', True), ('roberta.encoder.layer.7.output.dense.bias', True), ('roberta.encoder.layer.7.output.LayerNorm.weight', True), ('roberta.encoder.layer.7.output.LayerNorm.bias', True), ('roberta.encoder.layer.8.attention.self.query.weight', True), ('roberta.encoder.layer.8.attention.self.query.bias', True), ('roberta.encoder.layer.8.attention.self.key.weight', True), ('roberta.encoder.layer.8.attention.self.key.bias', True), ('roberta.encoder.layer.8.attention.self.value.weight', True), ('roberta.encoder.layer.8.attention.self.value.bias', True), ('roberta.encoder.layer.8.attention.output.dense.weight', True), ('roberta.encoder.layer.8.attention.output.dense.bias', True), ('roberta.encoder.layer.8.attention.output.LayerNorm.weight', True), ('roberta.encoder.layer.8.attention.output.LayerNorm.bias', True), ('roberta.encoder.layer.8.intermediate.dense.weight', True), ('roberta.encoder.layer.8.intermediate.dense.bias', True), ('roberta.encoder.layer.8.output.dense.weight', True), ('roberta.encoder.layer.8.output.dense.bias', True), ('roberta.encoder.layer.8.output.LayerNorm.weight', True), ('roberta.encoder.layer.8.output.LayerNorm.bias', True), ('roberta.encoder.layer.9.attention.self.query.weight', True), ('roberta.encoder.layer.9.attention.self.query.bias', True), ('roberta.encoder.layer.9.attention.self.key.weight', True), ('roberta.encoder.layer.9.attention.self.key.bias', True), ('roberta.encoder.layer.9.attention.self.value.weight', True), ('roberta.encoder.layer.9.attention.self.value.bias', True), ('roberta.encoder.layer.9.attention.output.dense.weight', True), ('roberta.encoder.layer.9.attention.output.dense.bias', True), ('roberta.encoder.layer.9.attention.output.LayerNorm.weight', True), ('roberta.encoder.layer.9.attention.output.LayerNorm.bias', True), ('roberta.encoder.layer.9.intermediate.dense.weight', True), ('roberta.encoder.layer.9.intermediate.dense.bias', True), ('roberta.encoder.layer.9.output.dense.weight', True), ('roberta.encoder.layer.9.output.dense.bias', True), ('roberta.encoder.layer.9.output.LayerNorm.weight', True), ('roberta.encoder.layer.9.output.LayerNorm.bias', True), ('roberta.encoder.layer.10.attention.self.query.weight', True), ('roberta.encoder.layer.10.attention.self.query.bias', True), ('roberta.encoder.layer.10.attention.self.key.weight', True), ('roberta.encoder.layer.10.attention.self.key.bias', True), ('roberta.encoder.layer.10.attention.self.value.weight', True), ('roberta.encoder.layer.10.attention.self.value.bias', True), ('roberta.encoder.layer.10.attention.output.dense.weight', True), ('roberta.encoder.layer.10.attention.output.dense.bias', True), ('roberta.encoder.layer.10.attention.output.LayerNorm.weight', True), ('roberta.encoder.layer.10.attention.output.LayerNorm.bias', True), ('roberta.encoder.layer.10.intermediate.dense.weight', True), ('roberta.encoder.layer.10.intermediate.dense.bias', True), ('roberta.encoder.layer.10.output.dense.weight', True), ('roberta.encoder.layer.10.output.dense.bias', True), ('roberta.encoder.layer.10.output.LayerNorm.weight', True), ('roberta.encoder.layer.10.output.LayerNorm.bias', True), ('roberta.encoder.layer.11.attention.self.query.weight', True), ('roberta.encoder.layer.11.attention.self.query.bias', True), ('roberta.encoder.layer.11.attention.self.key.weight', True), ('roberta.encoder.layer.11.attention.self.key.bias', True), ('roberta.encoder.layer.11.attention.self.value.weight', True), ('roberta.encoder.layer.11.attention.self.value.bias', True), ('roberta.encoder.layer.11.attention.output.dense.weight', True), ('roberta.encoder.layer.11.attention.output.dense.bias', True), ('roberta.encoder.layer.11.attention.output.LayerNorm.weight', True), ('roberta.encoder.layer.11.attention.output.LayerNorm.bias', True), ('roberta.encoder.layer.11.intermediate.dense.weight', True), ('roberta.encoder.layer.11.intermediate.dense.bias', True), ('roberta.encoder.layer.11.output.dense.weight', True), ('roberta.encoder.layer.11.output.dense.bias', True), ('roberta.encoder.layer.11.output.LayerNorm.weight', True), ('roberta.encoder.layer.11.output.LayerNorm.bias', True), ('classifier.dense_1.weight', True), ('classifier.dense_1.bias', True), ('classifier.dense_2.weight', True), ('classifier.dense_2.bias', True), ('classifier.dense_3.weight', True), ('classifier.dense_3.bias', True), ('classifier.dense_4.weight', True), ('classifier.dense_4.bias', True), ('classifier.dense_5.weight', True), ('classifier.dense_5.bias', True), ('classifier.dense_6.weight', True), ('classifier.dense_6.bias', True), ('classifier.out_proj.weight', True), ('classifier.out_proj.bias', True)]\n",
            "Running tokenizer on dataset:   0% 0/9 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /content/uczenie_glebokie/.cache_training/json/default-618d4135584dd38b/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-af678433973c1fe8.arrow\n",
            "Running tokenizer on dataset: 100% 9/9 [00:03<00:00,  2.56ba/s]\n",
            "Running tokenizer on dataset:   0% 0/1 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /content/uczenie_glebokie/.cache_training/json/default-618d4135584dd38b/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-fa27bc29afef5609.arrow\n",
            "Running tokenizer on dataset: 100% 1/1 [00:00<00:00,  6.60ba/s]\n",
            "Running tokenizer on dataset:   0% 0/1 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /content/uczenie_glebokie/.cache_training/json/default-618d4135584dd38b/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-0982ae7b667d6ede.arrow\n",
            "Running tokenizer on dataset: 100% 1/1 [00:00<00:00,  3.00ba/s]\n",
            "INFO:__main__:Set 139 samples for 0-class\n",
            "INFO:__main__:Set 139 samples for 1-class\n",
            "INFO:__main__:Set 139 samples for 2-class\n",
            "INFO:__main__:Sample 1824 of the training set: {'label': 1, 'text': \"Poured from a bottle into my pewter Jagerstein at refrigetor chilled temperature. I peaked a look at the color from my friends glass and found it to be somewhere between amber and stout/porter black. The kind of color that would decieve the average drinker into thinking that this was going to be a beer as thick as motor oil. But that's not how they roll in VT. The smell betrayed a sweeter character, almost over emphasizing the malt properties. The taste was a bit on the hoppy side. That is to say that, the aftertaste left you remembering more of the hops than the also very strong, caramel malt taste. The mouthfeel was perfect for a cool summer night. This would probably be a great beer from late April to early June and late September to early November. I could drink a few of these at a time, but would probably want to move on to something lighter after that.\", 'input_ids': [0, 510, 8855, 31, 10, 7304, 88, 127, 181, 2753, 1334, 344, 6988, 7864, 23, 4885, 7638, 594, 368, 32338, 5181, 4, 38, 21174, 10, 356, 23, 5, 3195, 31, 127, 964, 4049, 8, 303, 24, 7, 28, 6152, 227, 37156, 8, 34636, 73, 35802, 909, 4, 20, 761, 9, 3195, 14, 74, 5044, 16637, 5, 674, 4076, 254, 88, 2053, 14, 42, 21, 164, 7, 28, 10, 4437, 25, 7992, 25, 4243, 681, 4, 125, 14, 18, 45, 141, 51, 3825, 11, 37599, 4, 20, 11362, 26913, 10, 24043, 5906, 2048, 6, 818, 81, 27232, 5, 34021, 3611, 4, 20, 5840, 21, 10, 828, 15, 5, 9379, 12949, 526, 4, 280, 16, 7, 224, 14, 6, 5, 71, 90, 14631, 314, 47, 21556, 55, 9, 5, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.\n",
            "INFO:__main__:Sample 409 of the training set: {'label': 1, 'text': 'This beer isn\\'t bad, but there really isn\\'t anything I can say to strongly recommend it; there aren\\'t any really distinctive or unique flavors and I thought I detected a faint flavor defect or two. Organic Revolution doesn\\'t do anything to change my impression that in general the quality (or at least degree of \"interesting\") organic beers remains distinctly below that of non-organic beers. Pours a thick gold lager-color with a very tacky white head that is loosely packed into the top of my glass. Appearance of the beer\\'s body is a bit disconcerting - it has an oily-swirlyness to it. Aroma is only slightly hoppy, with some fruity vinous-ness. Flavor is balanced, but a bit dull with a very faint hoppiness. Some vinous-ness and diacetyl are evident to me.', 'input_ids': [0, 713, 4437, 965, 75, 1099, 6, 53, 89, 269, 965, 75, 932, 38, 64, 224, 7, 5025, 5940, 24, 131, 89, 2025, 75, 143, 269, 16141, 50, 2216, 14250, 8, 38, 802, 38, 12333, 10, 27922, 12117, 17584, 50, 80, 4, 24349, 13340, 630, 75, 109, 932, 7, 464, 127, 8450, 14, 11, 937, 5, 1318, 36, 368, 23, 513, 3093, 9, 22, 42746, 8070, 6523, 16328, 1189, 33029, 874, 14, 9, 786, 12, 34098, 16328, 4, 221, 5634, 10, 7992, 1637, 784, 6988, 12, 21685, 19, 10, 182, 24420, 219, 1104, 471, 14, 16, 29213, 6515, 88, 5, 299, 9, 127, 4049, 4, 45054, 9, 5, 4437, 18, 809, 16, 10, 828, 2982, 34420, 154, 111, 24, 34, 41, 39262, 12, 4184, 853, 352, 1825, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.\n",
            "INFO:__main__:Sample 4506 of the training set: {'label': 2, 'text': \"On tap at the brewery on 6/28/09. Pours solid black with mocha head, great lacing. Aroma is certainly dominated by chocolate, but gives way to smoke and coffee. This is a straight up sweet, smooth, balanced stout with a great chocolate finish, and for the style, it's incredible. Will be looking for this on the east coast.\", 'input_ids': [0, 4148, 5797, 23, 5, 19628, 15, 231, 73, 2517, 73, 3546, 4, 221, 5634, 2705, 909, 19, 475, 4306, 102, 471, 6, 372, 784, 7575, 4, 83, 43831, 16, 1819, 6235, 30, 7548, 6, 53, 2029, 169, 7, 4603, 8, 3895, 4, 152, 16, 10, 1359, 62, 4045, 6, 6921, 6, 9320, 34636, 19, 10, 372, 7548, 2073, 6, 8, 13, 5, 2496, 6, 24, 18, 3997, 4, 2290, 28, 546, 13, 42, 15, 5, 3017, 3673, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "Downloading builder script: 100% 4.20k/4.20k [00:00<00:00, 4.49MB/s]\n",
            "[INFO|trainer.py:503] 2023-02-15 18:17:43,412 >> max_steps is given, it will override any value given in num_train_epochs\n",
            "[INFO|trainer.py:725] 2023-02-15 18:17:43,412 >> The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassificationCustom.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassificationCustom.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "[INFO|trainer.py:1607] 2023-02-15 18:17:43,423 >> ***** Running training *****\n",
            "[INFO|trainer.py:1608] 2023-02-15 18:17:43,423 >>   Num examples = 9000\n",
            "[INFO|trainer.py:1609] 2023-02-15 18:17:43,423 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1610] 2023-02-15 18:17:43,423 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1611] 2023-02-15 18:17:43,423 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:1612] 2023-02-15 18:17:43,423 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1613] 2023-02-15 18:17:43,423 >>   Total optimization steps = 2500\n",
            "{'loss': 0.8883, 'learning_rate': 1.9200000000000003e-05, 'epoch': 0.09}\n",
            "{'loss': 0.9353, 'learning_rate': 1.8400000000000003e-05, 'epoch': 0.18}\n",
            " 10% 250/2500 [01:03<08:55,  4.20it/s][INFO|trainer.py:725] 2023-02-15 18:18:46,849 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassificationCustom.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassificationCustom.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2907] 2023-02-15 18:18:46,850 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2023-02-15 18:18:46,850 >>   Num examples = 306\n",
            "[INFO|trainer.py:2912] 2023-02-15 18:18:46,850 >>   Batch size = 8\n",
            "\n",
            "  0% 0/39 [00:00<?, ?it/s]\u001b[A\n",
            "  8% 3/39 [00:00<00:01, 23.63it/s]\u001b[A\n",
            " 15% 6/39 [00:00<00:01, 18.60it/s]\u001b[A\n",
            " 21% 8/39 [00:00<00:01, 17.25it/s]\u001b[A\n",
            " 26% 10/39 [00:00<00:01, 16.50it/s]\u001b[A\n",
            " 31% 12/39 [00:00<00:01, 16.17it/s]\u001b[A\n",
            " 36% 14/39 [00:00<00:01, 15.70it/s]\u001b[A\n",
            " 41% 16/39 [00:00<00:01, 15.69it/s]\u001b[A\n",
            " 46% 18/39 [00:01<00:01, 15.52it/s]\u001b[A\n",
            " 51% 20/39 [00:01<00:01, 15.61it/s]\u001b[A\n",
            " 56% 22/39 [00:01<00:01, 15.73it/s]\u001b[A\n",
            " 62% 24/39 [00:01<00:00, 15.67it/s]\u001b[A\n",
            " 67% 26/39 [00:01<00:00, 15.53it/s]\u001b[A\n",
            " 72% 28/39 [00:01<00:00, 15.74it/s]\u001b[A\n",
            " 77% 30/39 [00:01<00:00, 15.54it/s]\u001b[A\n",
            " 82% 32/39 [00:01<00:00, 15.61it/s]\u001b[A\n",
            " 87% 34/39 [00:02<00:00, 15.73it/s]\u001b[A\n",
            " 92% 36/39 [00:02<00:00, 15.65it/s]\u001b[A\n",
            " 97% 38/39 [00:02<00:00, 15.74it/s]\u001b[A\n",
            "{'eval_loss': 0.9424422383308411, 'eval_accuracy': 0.4542483687400818, 'eval_runtime': 2.4559, 'eval_samples_per_second': 124.597, 'eval_steps_per_second': 15.88, 'epoch': 0.22}\n",
            "\n",
            " 10% 250/2500 [01:05<08:55,  4.20it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2656] 2023-02-15 18:18:49,307 >> Saving model checkpoint to out/beer_reviews/roberta_hidden_v1/checkpoint-250\n",
            "[INFO|configuration_utils.py:447] 2023-02-15 18:18:49,309 >> Configuration saved in out/beer_reviews/roberta_hidden_v1/checkpoint-250/config.json\n",
            "[INFO|modeling_utils.py:1624] 2023-02-15 18:18:51,476 >> Model weights saved in out/beer_reviews/roberta_hidden_v1/checkpoint-250/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2023-02-15 18:18:51,477 >> tokenizer config file saved in out/beer_reviews/roberta_hidden_v1/checkpoint-250/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2023-02-15 18:18:51,477 >> Special tokens file saved in out/beer_reviews/roberta_hidden_v1/checkpoint-250/special_tokens_map.json\n",
            "{'loss': 0.8744, 'learning_rate': 1.76e-05, 'epoch': 0.27}\n",
            "{'loss': 0.8273, 'learning_rate': 1.6800000000000002e-05, 'epoch': 0.36}\n",
            "{'loss': 0.7454, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.44}\n",
            " 20% 500/2500 [02:13<08:11,  4.07it/s][INFO|trainer.py:725] 2023-02-15 18:19:56,473 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassificationCustom.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassificationCustom.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2907] 2023-02-15 18:19:56,476 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2023-02-15 18:19:56,476 >>   Num examples = 306\n",
            "[INFO|trainer.py:2912] 2023-02-15 18:19:56,476 >>   Batch size = 8\n",
            "\n",
            "  0% 0/39 [00:00<?, ?it/s]\u001b[A\n",
            "  8% 3/39 [00:00<00:01, 21.82it/s]\u001b[A\n",
            " 15% 6/39 [00:00<00:01, 18.02it/s]\u001b[A\n",
            " 21% 8/39 [00:00<00:01, 17.07it/s]\u001b[A\n",
            " 26% 10/39 [00:00<00:01, 16.28it/s]\u001b[A\n",
            " 31% 12/39 [00:00<00:01, 15.90it/s]\u001b[A\n",
            " 36% 14/39 [00:00<00:01, 15.55it/s]\u001b[A\n",
            " 41% 16/39 [00:00<00:01, 15.51it/s]\u001b[A\n",
            " 46% 18/39 [00:01<00:01, 15.48it/s]\u001b[A\n",
            " 51% 20/39 [00:01<00:01, 15.48it/s]\u001b[A\n",
            " 56% 22/39 [00:01<00:01, 15.42it/s]\u001b[A\n",
            " 62% 24/39 [00:01<00:00, 15.58it/s]\u001b[A\n",
            " 67% 26/39 [00:01<00:00, 15.53it/s]\u001b[A\n",
            " 72% 28/39 [00:01<00:00, 15.41it/s]\u001b[A\n",
            " 77% 30/39 [00:01<00:00, 15.34it/s]\u001b[A\n",
            " 82% 32/39 [00:02<00:00, 15.28it/s]\u001b[A\n",
            " 87% 34/39 [00:02<00:00, 15.29it/s]\u001b[A\n",
            " 92% 36/39 [00:02<00:00, 15.23it/s]\u001b[A\n",
            " 97% 38/39 [00:02<00:00, 15.47it/s]\u001b[A\n",
            "{'eval_loss': 0.8497004508972168, 'eval_accuracy': 0.6437908411026001, 'eval_runtime': 2.4847, 'eval_samples_per_second': 123.156, 'eval_steps_per_second': 15.696, 'epoch': 0.44}\n",
            "\n",
            " 20% 500/2500 [02:15<08:11,  4.07it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2656] 2023-02-15 18:19:58,962 >> Saving model checkpoint to out/beer_reviews/roberta_hidden_v1/checkpoint-500\n",
            "[INFO|configuration_utils.py:447] 2023-02-15 18:19:58,962 >> Configuration saved in out/beer_reviews/roberta_hidden_v1/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:1624] 2023-02-15 18:20:01,007 >> Model weights saved in out/beer_reviews/roberta_hidden_v1/checkpoint-500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2023-02-15 18:20:01,008 >> tokenizer config file saved in out/beer_reviews/roberta_hidden_v1/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2023-02-15 18:20:01,008 >> Special tokens file saved in out/beer_reviews/roberta_hidden_v1/checkpoint-500/special_tokens_map.json\n",
            "{'loss': 0.7429, 'learning_rate': 1.5200000000000002e-05, 'epoch': 0.53}\n",
            "{'loss': 0.7209, 'learning_rate': 1.4400000000000001e-05, 'epoch': 0.62}\n",
            " 30% 750/2500 [03:22<07:01,  4.15it/s][INFO|trainer.py:725] 2023-02-15 18:21:06,375 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassificationCustom.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassificationCustom.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2907] 2023-02-15 18:21:06,377 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2023-02-15 18:21:06,377 >>   Num examples = 306\n",
            "[INFO|trainer.py:2912] 2023-02-15 18:21:06,377 >>   Batch size = 8\n",
            "\n",
            "  0% 0/39 [00:00<?, ?it/s]\u001b[A\n",
            "  8% 3/39 [00:00<00:01, 22.61it/s]\u001b[A\n",
            " 15% 6/39 [00:00<00:01, 18.21it/s]\u001b[A\n",
            " 21% 8/39 [00:00<00:01, 17.09it/s]\u001b[A\n",
            " 26% 10/39 [00:00<00:01, 16.34it/s]\u001b[A\n",
            " 31% 12/39 [00:00<00:01, 16.06it/s]\u001b[A\n",
            " 36% 14/39 [00:00<00:01, 15.69it/s]\u001b[A\n",
            " 41% 16/39 [00:00<00:01, 15.80it/s]\u001b[A\n",
            " 46% 18/39 [00:01<00:01, 15.66it/s]\u001b[A\n",
            " 51% 20/39 [00:01<00:01, 15.62it/s]\u001b[A\n",
            " 56% 22/39 [00:01<00:01, 15.68it/s]\u001b[A\n",
            " 62% 24/39 [00:01<00:00, 15.66it/s]\u001b[A\n",
            " 67% 26/39 [00:01<00:00, 15.53it/s]\u001b[A\n",
            " 72% 28/39 [00:01<00:00, 15.68it/s]\u001b[A\n",
            " 77% 30/39 [00:01<00:00, 15.47it/s]\u001b[A\n",
            " 82% 32/39 [00:01<00:00, 15.62it/s]\u001b[A\n",
            " 87% 34/39 [00:02<00:00, 15.68it/s]\u001b[A\n",
            " 92% 36/39 [00:02<00:00, 15.58it/s]\u001b[A\n",
            " 97% 38/39 [00:02<00:00, 15.62it/s]\u001b[A\n",
            "{'eval_loss': 0.7218883037567139, 'eval_accuracy': 0.6797385811805725, 'eval_runtime': 2.4565, 'eval_samples_per_second': 124.567, 'eval_steps_per_second': 15.876, 'epoch': 0.67}\n",
            "\n",
            " 30% 750/2500 [03:25<07:01,  4.15it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2656] 2023-02-15 18:21:08,834 >> Saving model checkpoint to out/beer_reviews/roberta_hidden_v1/checkpoint-750\n",
            "[INFO|configuration_utils.py:447] 2023-02-15 18:21:08,835 >> Configuration saved in out/beer_reviews/roberta_hidden_v1/checkpoint-750/config.json\n",
            "[INFO|modeling_utils.py:1624] 2023-02-15 18:21:10,874 >> Model weights saved in out/beer_reviews/roberta_hidden_v1/checkpoint-750/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2023-02-15 18:21:10,875 >> tokenizer config file saved in out/beer_reviews/roberta_hidden_v1/checkpoint-750/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2023-02-15 18:21:10,875 >> Special tokens file saved in out/beer_reviews/roberta_hidden_v1/checkpoint-750/special_tokens_map.json\n",
            "{'loss': 0.7218, 'learning_rate': 1.3600000000000002e-05, 'epoch': 0.71}\n",
            "{'loss': 0.7506, 'learning_rate': 1.2800000000000001e-05, 'epoch': 0.8}\n",
            "{'loss': 0.7173, 'learning_rate': 1.2e-05, 'epoch': 0.89}\n",
            " 40% 1000/2500 [04:32<05:57,  4.19it/s][INFO|trainer.py:725] 2023-02-15 18:22:16,295 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassificationCustom.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassificationCustom.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2907] 2023-02-15 18:22:16,296 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2023-02-15 18:22:16,296 >>   Num examples = 306\n",
            "[INFO|trainer.py:2912] 2023-02-15 18:22:16,296 >>   Batch size = 8\n",
            "\n",
            "  0% 0/39 [00:00<?, ?it/s]\u001b[A\n",
            "  8% 3/39 [00:00<00:01, 22.54it/s]\u001b[A\n",
            " 15% 6/39 [00:00<00:01, 18.25it/s]\u001b[A\n",
            " 21% 8/39 [00:00<00:01, 16.96it/s]\u001b[A\n",
            " 26% 10/39 [00:00<00:01, 16.52it/s]\u001b[A\n",
            " 31% 12/39 [00:00<00:01, 16.28it/s]\u001b[A\n",
            " 36% 14/39 [00:00<00:01, 16.21it/s]\u001b[A\n",
            " 41% 16/39 [00:00<00:01, 15.87it/s]\u001b[A\n",
            " 46% 18/39 [00:01<00:01, 15.84it/s]\u001b[A\n",
            " 51% 20/39 [00:01<00:01, 15.93it/s]\u001b[A\n",
            " 56% 22/39 [00:01<00:01, 15.77it/s]\u001b[A\n",
            " 62% 24/39 [00:01<00:00, 15.71it/s]\u001b[A\n",
            " 67% 26/39 [00:01<00:00, 15.71it/s]\u001b[A\n",
            " 72% 28/39 [00:01<00:00, 15.60it/s]\u001b[A\n",
            " 77% 30/39 [00:01<00:00, 15.53it/s]\u001b[A\n",
            " 82% 32/39 [00:01<00:00, 15.71it/s]\u001b[A\n",
            " 87% 34/39 [00:02<00:00, 15.63it/s]\u001b[A\n",
            " 92% 36/39 [00:02<00:00, 15.67it/s]\u001b[A\n",
            " 97% 38/39 [00:02<00:00, 15.65it/s]\u001b[A\n",
            "{'eval_loss': 0.6676020622253418, 'eval_accuracy': 0.6666666865348816, 'eval_runtime': 2.4401, 'eval_samples_per_second': 125.403, 'eval_steps_per_second': 15.983, 'epoch': 0.89}\n",
            "\n",
            " 40% 1000/2500 [04:35<05:57,  4.19it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2656] 2023-02-15 18:22:18,738 >> Saving model checkpoint to out/beer_reviews/roberta_hidden_v1/checkpoint-1000\n",
            "[INFO|configuration_utils.py:447] 2023-02-15 18:22:18,739 >> Configuration saved in out/beer_reviews/roberta_hidden_v1/checkpoint-1000/config.json\n",
            "[INFO|modeling_utils.py:1624] 2023-02-15 18:22:20,755 >> Model weights saved in out/beer_reviews/roberta_hidden_v1/checkpoint-1000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2023-02-15 18:22:20,755 >> tokenizer config file saved in out/beer_reviews/roberta_hidden_v1/checkpoint-1000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2023-02-15 18:22:20,755 >> Special tokens file saved in out/beer_reviews/roberta_hidden_v1/checkpoint-1000/special_tokens_map.json\n",
            "{'loss': 0.6358, 'learning_rate': 1.1200000000000001e-05, 'epoch': 0.98}\n",
            "{'loss': 0.6554, 'learning_rate': 1.04e-05, 'epoch': 1.07}\n",
            " 50% 1250/2500 [05:42<04:58,  4.19it/s][INFO|trainer.py:725] 2023-02-15 18:23:26,109 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassificationCustom.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassificationCustom.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2907] 2023-02-15 18:23:26,110 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2023-02-15 18:23:26,110 >>   Num examples = 306\n",
            "[INFO|trainer.py:2912] 2023-02-15 18:23:26,110 >>   Batch size = 8\n",
            "\n",
            "  0% 0/39 [00:00<?, ?it/s]\u001b[A\n",
            "  8% 3/39 [00:00<00:01, 22.29it/s]\u001b[A\n",
            " 15% 6/39 [00:00<00:01, 18.38it/s]\u001b[A\n",
            " 21% 8/39 [00:00<00:01, 17.19it/s]\u001b[A\n",
            " 26% 10/39 [00:00<00:01, 16.54it/s]\u001b[A\n",
            " 31% 12/39 [00:00<00:01, 16.05it/s]\u001b[A\n",
            " 36% 14/39 [00:00<00:01, 15.65it/s]\u001b[A\n",
            " 41% 16/39 [00:00<00:01, 15.67it/s]\u001b[A\n",
            " 46% 18/39 [00:01<00:01, 15.46it/s]\u001b[A\n",
            " 51% 20/39 [00:01<00:01, 15.69it/s]\u001b[A\n",
            " 56% 22/39 [00:01<00:01, 15.73it/s]\u001b[A\n",
            " 62% 24/39 [00:01<00:00, 15.64it/s]\u001b[A\n",
            " 67% 26/39 [00:01<00:00, 15.58it/s]\u001b[A\n",
            " 72% 28/39 [00:01<00:00, 15.58it/s]\u001b[A\n",
            " 77% 30/39 [00:01<00:00, 15.43it/s]\u001b[A\n",
            " 82% 32/39 [00:01<00:00, 15.68it/s]\u001b[A\n",
            " 87% 34/39 [00:02<00:00, 15.62it/s]\u001b[A\n",
            " 92% 36/39 [00:02<00:00, 15.63it/s]\u001b[A\n",
            " 97% 38/39 [00:02<00:00, 15.77it/s]\u001b[A\n",
            "{'eval_loss': 0.8242064118385315, 'eval_accuracy': 0.6797385811805725, 'eval_runtime': 2.4524, 'eval_samples_per_second': 124.776, 'eval_steps_per_second': 15.903, 'epoch': 1.11}\n",
            "\n",
            " 50% 1250/2500 [05:45<04:58,  4.19it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2656] 2023-02-15 18:23:28,564 >> Saving model checkpoint to out/beer_reviews/roberta_hidden_v1/checkpoint-1250\n",
            "[INFO|configuration_utils.py:447] 2023-02-15 18:23:28,565 >> Configuration saved in out/beer_reviews/roberta_hidden_v1/checkpoint-1250/config.json\n",
            "[INFO|modeling_utils.py:1624] 2023-02-15 18:23:30,564 >> Model weights saved in out/beer_reviews/roberta_hidden_v1/checkpoint-1250/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2023-02-15 18:23:30,567 >> tokenizer config file saved in out/beer_reviews/roberta_hidden_v1/checkpoint-1250/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2023-02-15 18:23:30,568 >> Special tokens file saved in out/beer_reviews/roberta_hidden_v1/checkpoint-1250/special_tokens_map.json\n",
            "{'loss': 0.6577, 'learning_rate': 9.600000000000001e-06, 'epoch': 1.16}\n",
            "{'loss': 0.6303, 'learning_rate': 8.8e-06, 'epoch': 1.24}\n",
            "{'loss': 0.5905, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.33}\n",
            " 60% 1500/2500 [06:52<03:59,  4.18it/s][INFO|trainer.py:725] 2023-02-15 18:24:35,837 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassificationCustom.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassificationCustom.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2907] 2023-02-15 18:24:35,838 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2023-02-15 18:24:35,838 >>   Num examples = 306\n",
            "[INFO|trainer.py:2912] 2023-02-15 18:24:35,839 >>   Batch size = 8\n",
            "\n",
            "  0% 0/39 [00:00<?, ?it/s]\u001b[A\n",
            "  8% 3/39 [00:00<00:01, 23.41it/s]\u001b[A\n",
            " 15% 6/39 [00:00<00:01, 18.26it/s]\u001b[A\n",
            " 21% 8/39 [00:00<00:01, 17.12it/s]\u001b[A\n",
            " 26% 10/39 [00:00<00:01, 16.48it/s]\u001b[A\n",
            " 31% 12/39 [00:00<00:01, 16.20it/s]\u001b[A\n",
            " 36% 14/39 [00:00<00:01, 16.07it/s]\u001b[A\n",
            " 41% 16/39 [00:00<00:01, 15.71it/s]\u001b[A\n",
            " 46% 18/39 [00:01<00:01, 15.71it/s]\u001b[A\n",
            " 51% 20/39 [00:01<00:01, 15.85it/s]\u001b[A\n",
            " 56% 22/39 [00:01<00:01, 15.68it/s]\u001b[A\n",
            " 62% 24/39 [00:01<00:00, 15.71it/s]\u001b[A\n",
            " 67% 26/39 [00:01<00:00, 15.63it/s]\u001b[A\n",
            " 72% 28/39 [00:01<00:00, 15.50it/s]\u001b[A\n",
            " 77% 30/39 [00:01<00:00, 15.47it/s]\u001b[A\n",
            " 82% 32/39 [00:01<00:00, 15.37it/s]\u001b[A\n",
            " 87% 34/39 [00:02<00:00, 15.52it/s]\u001b[A\n",
            " 92% 36/39 [00:02<00:00, 15.69it/s]\u001b[A\n",
            " 97% 38/39 [00:02<00:00, 15.84it/s]\u001b[A\n",
            "{'eval_loss': 0.7504055500030518, 'eval_accuracy': 0.6764705777168274, 'eval_runtime': 2.4445, 'eval_samples_per_second': 125.181, 'eval_steps_per_second': 15.954, 'epoch': 1.33}\n",
            "\n",
            " 60% 1500/2500 [06:54<03:59,  4.18it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2656] 2023-02-15 18:24:38,284 >> Saving model checkpoint to out/beer_reviews/roberta_hidden_v1/checkpoint-1500\n",
            "[INFO|configuration_utils.py:447] 2023-02-15 18:24:38,286 >> Configuration saved in out/beer_reviews/roberta_hidden_v1/checkpoint-1500/config.json\n",
            "[INFO|modeling_utils.py:1624] 2023-02-15 18:24:40,405 >> Model weights saved in out/beer_reviews/roberta_hidden_v1/checkpoint-1500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2023-02-15 18:24:40,405 >> tokenizer config file saved in out/beer_reviews/roberta_hidden_v1/checkpoint-1500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2023-02-15 18:24:40,406 >> Special tokens file saved in out/beer_reviews/roberta_hidden_v1/checkpoint-1500/special_tokens_map.json\n",
            "[INFO|trainer.py:2734] 2023-02-15 18:24:45,281 >> Deleting older checkpoint [out/beer_reviews/roberta_hidden_v1/checkpoint-250] due to args.save_total_limit\n",
            "{'loss': 0.6016, 'learning_rate': 7.2000000000000005e-06, 'epoch': 1.42}\n",
            "{'loss': 0.6317, 'learning_rate': 6.4000000000000006e-06, 'epoch': 1.51}\n",
            " 70% 1750/2500 [08:02<03:00,  4.15it/s][INFO|trainer.py:725] 2023-02-15 18:25:45,757 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassificationCustom.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassificationCustom.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2907] 2023-02-15 18:25:45,759 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2023-02-15 18:25:45,759 >>   Num examples = 306\n",
            "[INFO|trainer.py:2912] 2023-02-15 18:25:45,760 >>   Batch size = 8\n",
            "\n",
            "  0% 0/39 [00:00<?, ?it/s]\u001b[A\n",
            "  8% 3/39 [00:00<00:01, 22.34it/s]\u001b[A\n",
            " 15% 6/39 [00:00<00:01, 18.26it/s]\u001b[A\n",
            " 21% 8/39 [00:00<00:01, 17.17it/s]\u001b[A\n",
            " 26% 10/39 [00:00<00:01, 16.57it/s]\u001b[A\n",
            " 31% 12/39 [00:00<00:01, 16.25it/s]\u001b[A\n",
            " 36% 14/39 [00:00<00:01, 16.09it/s]\u001b[A\n",
            " 41% 16/39 [00:00<00:01, 15.72it/s]\u001b[A\n",
            " 46% 18/39 [00:01<00:01, 15.82it/s]\u001b[A\n",
            " 51% 20/39 [00:01<00:01, 15.84it/s]\u001b[A\n",
            " 56% 22/39 [00:01<00:01, 15.70it/s]\u001b[A\n",
            " 62% 24/39 [00:01<00:00, 15.81it/s]\u001b[A\n",
            " 67% 26/39 [00:01<00:00, 15.73it/s]\u001b[A\n",
            " 72% 28/39 [00:01<00:00, 15.65it/s]\u001b[A\n",
            " 77% 30/39 [00:01<00:00, 15.44it/s]\u001b[A\n",
            " 82% 32/39 [00:01<00:00, 15.46it/s]\u001b[A\n",
            " 87% 34/39 [00:02<00:00, 15.46it/s]\u001b[A\n",
            " 92% 36/39 [00:02<00:00, 15.72it/s]\u001b[A\n",
            " 97% 38/39 [00:02<00:00, 15.90it/s]\u001b[A\n",
            "{'eval_loss': 0.681303858757019, 'eval_accuracy': 0.6960784196853638, 'eval_runtime': 2.4393, 'eval_samples_per_second': 125.448, 'eval_steps_per_second': 15.988, 'epoch': 1.56}\n",
            "\n",
            " 70% 1750/2500 [08:04<03:00,  4.15it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2656] 2023-02-15 18:25:48,200 >> Saving model checkpoint to out/beer_reviews/roberta_hidden_v1/checkpoint-1750\n",
            "[INFO|configuration_utils.py:447] 2023-02-15 18:25:48,202 >> Configuration saved in out/beer_reviews/roberta_hidden_v1/checkpoint-1750/config.json\n",
            "[INFO|modeling_utils.py:1624] 2023-02-15 18:25:50,302 >> Model weights saved in out/beer_reviews/roberta_hidden_v1/checkpoint-1750/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2023-02-15 18:25:50,302 >> tokenizer config file saved in out/beer_reviews/roberta_hidden_v1/checkpoint-1750/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2023-02-15 18:25:50,303 >> Special tokens file saved in out/beer_reviews/roberta_hidden_v1/checkpoint-1750/special_tokens_map.json\n",
            "[INFO|trainer.py:2734] 2023-02-15 18:25:55,188 >> Deleting older checkpoint [out/beer_reviews/roberta_hidden_v1/checkpoint-500] due to args.save_total_limit\n",
            "{'loss': 0.5725, 'learning_rate': 5.600000000000001e-06, 'epoch': 1.6}\n",
            "{'loss': 0.5962, 'learning_rate': 4.800000000000001e-06, 'epoch': 1.69}\n",
            "{'loss': 0.6119, 'learning_rate': 4.000000000000001e-06, 'epoch': 1.78}\n",
            " 80% 2000/2500 [09:12<02:03,  4.06it/s][INFO|trainer.py:725] 2023-02-15 18:26:55,722 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassificationCustom.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassificationCustom.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2907] 2023-02-15 18:26:55,724 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2023-02-15 18:26:55,724 >>   Num examples = 306\n",
            "[INFO|trainer.py:2912] 2023-02-15 18:26:55,724 >>   Batch size = 8\n",
            "\n",
            "  0% 0/39 [00:00<?, ?it/s]\u001b[A\n",
            "  8% 3/39 [00:00<00:01, 22.35it/s]\u001b[A\n",
            " 15% 6/39 [00:00<00:01, 17.50it/s]\u001b[A\n",
            " 21% 8/39 [00:00<00:01, 16.83it/s]\u001b[A\n",
            " 26% 10/39 [00:00<00:01, 15.89it/s]\u001b[A\n",
            " 31% 12/39 [00:00<00:01, 15.67it/s]\u001b[A\n",
            " 36% 14/39 [00:00<00:01, 15.40it/s]\u001b[A\n",
            " 41% 16/39 [00:00<00:01, 15.44it/s]\u001b[A\n",
            " 46% 18/39 [00:01<00:01, 15.59it/s]\u001b[A\n",
            " 51% 20/39 [00:01<00:01, 15.63it/s]\u001b[A\n",
            " 56% 22/39 [00:01<00:01, 15.58it/s]\u001b[A\n",
            " 62% 24/39 [00:01<00:00, 15.59it/s]\u001b[A\n",
            " 67% 26/39 [00:01<00:00, 15.57it/s]\u001b[A\n",
            " 72% 28/39 [00:01<00:00, 15.38it/s]\u001b[A\n",
            " 77% 30/39 [00:01<00:00, 15.43it/s]\u001b[A\n",
            " 82% 32/39 [00:02<00:00, 15.50it/s]\u001b[A\n",
            " 87% 34/39 [00:02<00:00, 15.51it/s]\u001b[A\n",
            " 92% 36/39 [00:02<00:00, 15.56it/s]\u001b[A\n",
            " 97% 38/39 [00:02<00:00, 15.47it/s]\u001b[A\n",
            "{'eval_loss': 0.6289618015289307, 'eval_accuracy': 0.7254902124404907, 'eval_runtime': 2.482, 'eval_samples_per_second': 123.288, 'eval_steps_per_second': 15.713, 'epoch': 1.78}\n",
            "\n",
            " 80% 2000/2500 [09:14<02:03,  4.06it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2656] 2023-02-15 18:26:58,207 >> Saving model checkpoint to out/beer_reviews/roberta_hidden_v1/checkpoint-2000\n",
            "[INFO|configuration_utils.py:447] 2023-02-15 18:26:58,208 >> Configuration saved in out/beer_reviews/roberta_hidden_v1/checkpoint-2000/config.json\n",
            "[INFO|modeling_utils.py:1624] 2023-02-15 18:27:00,172 >> Model weights saved in out/beer_reviews/roberta_hidden_v1/checkpoint-2000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2023-02-15 18:27:00,173 >> tokenizer config file saved in out/beer_reviews/roberta_hidden_v1/checkpoint-2000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2023-02-15 18:27:00,173 >> Special tokens file saved in out/beer_reviews/roberta_hidden_v1/checkpoint-2000/special_tokens_map.json\n",
            "[INFO|trainer.py:2734] 2023-02-15 18:27:05,027 >> Deleting older checkpoint [out/beer_reviews/roberta_hidden_v1/checkpoint-750] due to args.save_total_limit\n",
            "{'loss': 0.6022, 'learning_rate': 3.2000000000000003e-06, 'epoch': 1.87}\n",
            "{'loss': 0.608, 'learning_rate': 2.4000000000000003e-06, 'epoch': 1.96}\n",
            " 90% 2250/2500 [10:21<00:59,  4.19it/s][INFO|trainer.py:725] 2023-02-15 18:28:05,410 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassificationCustom.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassificationCustom.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2907] 2023-02-15 18:28:05,412 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2023-02-15 18:28:05,412 >>   Num examples = 306\n",
            "[INFO|trainer.py:2912] 2023-02-15 18:28:05,412 >>   Batch size = 8\n",
            "\n",
            "  0% 0/39 [00:00<?, ?it/s]\u001b[A\n",
            "  8% 3/39 [00:00<00:01, 23.16it/s]\u001b[A\n",
            " 15% 6/39 [00:00<00:01, 18.41it/s]\u001b[A\n",
            " 21% 8/39 [00:00<00:01, 17.01it/s]\u001b[A\n",
            " 26% 10/39 [00:00<00:01, 16.41it/s]\u001b[A\n",
            " 31% 12/39 [00:00<00:01, 16.11it/s]\u001b[A\n",
            " 36% 14/39 [00:00<00:01, 15.74it/s]\u001b[A\n",
            " 41% 16/39 [00:00<00:01, 15.83it/s]\u001b[A\n",
            " 46% 18/39 [00:01<00:01, 15.88it/s]\u001b[A\n",
            " 51% 20/39 [00:01<00:01, 15.77it/s]\u001b[A\n",
            " 56% 22/39 [00:01<00:01, 15.61it/s]\u001b[A\n",
            " 62% 24/39 [00:01<00:00, 15.80it/s]\u001b[A\n",
            " 67% 26/39 [00:01<00:00, 15.68it/s]\u001b[A\n",
            " 72% 28/39 [00:01<00:00, 15.61it/s]\u001b[A\n",
            " 77% 30/39 [00:01<00:00, 15.43it/s]\u001b[A\n",
            " 82% 32/39 [00:01<00:00, 15.39it/s]\u001b[A\n",
            " 87% 34/39 [00:02<00:00, 15.58it/s]\u001b[A\n",
            " 92% 36/39 [00:02<00:00, 15.66it/s]\u001b[A\n",
            " 97% 38/39 [00:02<00:00, 15.54it/s]\u001b[A\n",
            "{'eval_loss': 0.6586545705795288, 'eval_accuracy': 0.6960784196853638, 'eval_runtime': 2.4552, 'eval_samples_per_second': 124.635, 'eval_steps_per_second': 15.885, 'epoch': 2.0}\n",
            "\n",
            " 90% 2250/2500 [10:24<00:59,  4.19it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2656] 2023-02-15 18:28:07,868 >> Saving model checkpoint to out/beer_reviews/roberta_hidden_v1/checkpoint-2250\n",
            "[INFO|configuration_utils.py:447] 2023-02-15 18:28:07,869 >> Configuration saved in out/beer_reviews/roberta_hidden_v1/checkpoint-2250/config.json\n",
            "[INFO|modeling_utils.py:1624] 2023-02-15 18:28:09,840 >> Model weights saved in out/beer_reviews/roberta_hidden_v1/checkpoint-2250/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2023-02-15 18:28:09,841 >> tokenizer config file saved in out/beer_reviews/roberta_hidden_v1/checkpoint-2250/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2023-02-15 18:28:09,841 >> Special tokens file saved in out/beer_reviews/roberta_hidden_v1/checkpoint-2250/special_tokens_map.json\n",
            "[INFO|trainer.py:2734] 2023-02-15 18:28:14,741 >> Deleting older checkpoint [out/beer_reviews/roberta_hidden_v1/checkpoint-1000] due to args.save_total_limit\n",
            "{'loss': 0.5323, 'learning_rate': 1.6000000000000001e-06, 'epoch': 2.04}\n",
            "{'loss': 0.5534, 'learning_rate': 8.000000000000001e-07, 'epoch': 2.13}\n",
            "{'loss': 0.4988, 'learning_rate': 0.0, 'epoch': 2.22}\n",
            "100% 2500/2500 [11:31<00:00,  4.19it/s][INFO|trainer.py:725] 2023-02-15 18:29:15,276 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassificationCustom.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassificationCustom.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2907] 2023-02-15 18:29:15,278 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2023-02-15 18:29:15,278 >>   Num examples = 306\n",
            "[INFO|trainer.py:2912] 2023-02-15 18:29:15,278 >>   Batch size = 8\n",
            "\n",
            "  0% 0/39 [00:00<?, ?it/s]\u001b[A\n",
            "  8% 3/39 [00:00<00:01, 23.68it/s]\u001b[A\n",
            " 15% 6/39 [00:00<00:01, 18.29it/s]\u001b[A\n",
            " 21% 8/39 [00:00<00:01, 16.97it/s]\u001b[A\n",
            " 26% 10/39 [00:00<00:01, 16.44it/s]\u001b[A\n",
            " 31% 12/39 [00:00<00:01, 16.14it/s]\u001b[A\n",
            " 36% 14/39 [00:00<00:01, 15.94it/s]\u001b[A\n",
            " 41% 16/39 [00:00<00:01, 15.67it/s]\u001b[A\n",
            " 46% 18/39 [00:01<00:01, 15.81it/s]\u001b[A\n",
            " 51% 20/39 [00:01<00:01, 15.97it/s]\u001b[A\n",
            " 56% 22/39 [00:01<00:01, 15.92it/s]\u001b[A\n",
            " 62% 24/39 [00:01<00:00, 15.85it/s]\u001b[A\n",
            " 67% 26/39 [00:01<00:00, 15.63it/s]\u001b[A\n",
            " 72% 28/39 [00:01<00:00, 15.60it/s]\u001b[A\n",
            " 77% 30/39 [00:01<00:00, 15.40it/s]\u001b[A\n",
            " 82% 32/39 [00:01<00:00, 15.39it/s]\u001b[A\n",
            " 87% 34/39 [00:02<00:00, 15.35it/s]\u001b[A\n",
            " 92% 36/39 [00:02<00:00, 15.54it/s]\u001b[A\n",
            " 97% 38/39 [00:02<00:00, 15.55it/s]\u001b[A\n",
            "{'eval_loss': 0.6655606627464294, 'eval_accuracy': 0.7026143670082092, 'eval_runtime': 2.4524, 'eval_samples_per_second': 124.775, 'eval_steps_per_second': 15.903, 'epoch': 2.22}\n",
            "\n",
            "100% 2500/2500 [11:34<00:00,  4.19it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2656] 2023-02-15 18:29:17,731 >> Saving model checkpoint to out/beer_reviews/roberta_hidden_v1/checkpoint-2500\n",
            "[INFO|configuration_utils.py:447] 2023-02-15 18:29:17,732 >> Configuration saved in out/beer_reviews/roberta_hidden_v1/checkpoint-2500/config.json\n",
            "[INFO|modeling_utils.py:1624] 2023-02-15 18:29:19,690 >> Model weights saved in out/beer_reviews/roberta_hidden_v1/checkpoint-2500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2023-02-15 18:29:19,691 >> tokenizer config file saved in out/beer_reviews/roberta_hidden_v1/checkpoint-2500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2023-02-15 18:29:19,691 >> Special tokens file saved in out/beer_reviews/roberta_hidden_v1/checkpoint-2500/special_tokens_map.json\n",
            "[INFO|trainer.py:2734] 2023-02-15 18:29:24,562 >> Deleting older checkpoint [out/beer_reviews/roberta_hidden_v1/checkpoint-1250] due to args.save_total_limit\n",
            "[INFO|trainer.py:1852] 2023-02-15 18:29:24,595 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[INFO|trainer.py:1946] 2023-02-15 18:29:24,595 >> Loading best model from out/beer_reviews/roberta_hidden_v1/checkpoint-2000 (score: 0.7254902124404907).\n",
            "{'train_runtime': 704.1836, 'train_samples_per_second': 28.402, 'train_steps_per_second': 3.55, 'train_loss': 0.6761073715209961, 'epoch': 2.22}\n",
            "100% 2500/2500 [11:44<00:00,  3.55it/s]\n",
            "[INFO|trainer.py:2656] 2023-02-15 18:29:27,608 >> Saving model checkpoint to out/beer_reviews/roberta_hidden_v1\n",
            "[INFO|configuration_utils.py:447] 2023-02-15 18:29:27,609 >> Configuration saved in out/beer_reviews/roberta_hidden_v1/config.json\n",
            "[INFO|modeling_utils.py:1624] 2023-02-15 18:29:29,635 >> Model weights saved in out/beer_reviews/roberta_hidden_v1/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2023-02-15 18:29:29,636 >> tokenizer config file saved in out/beer_reviews/roberta_hidden_v1/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2023-02-15 18:29:29,637 >> Special tokens file saved in out/beer_reviews/roberta_hidden_v1/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       2.22\n",
            "  train_loss               =     0.6761\n",
            "  train_runtime            = 0:11:44.18\n",
            "  train_samples            =       9000\n",
            "  train_samples_per_second =     28.402\n",
            "  train_steps_per_second   =       3.55\n",
            "INFO:__main__:*** Evaluate ***\n",
            "[INFO|trainer.py:725] 2023-02-15 18:29:29,760 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassificationCustom.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassificationCustom.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2907] 2023-02-15 18:29:29,762 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2023-02-15 18:29:29,762 >>   Num examples = 306\n",
            "[INFO|trainer.py:2912] 2023-02-15 18:29:29,762 >>   Batch size = 8\n",
            "100% 39/39 [00:02<00:00, 16.73it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       2.22\n",
            "  eval_accuracy           =     0.7255\n",
            "  eval_loss               =      0.629\n",
            "  eval_runtime            = 0:00:02.41\n",
            "  eval_samples            =        306\n",
            "  eval_samples_per_second =    126.741\n",
            "  eval_steps_per_second   =     16.153\n",
            "INFO:__main__:*** Predict ***\n",
            "[INFO|trainer.py:725] 2023-02-15 18:29:32,180 >> The following columns in the test set don't have a corresponding argument in `RobertaForSequenceClassificationCustom.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassificationCustom.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2907] 2023-02-15 18:29:32,181 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:2909] 2023-02-15 18:29:32,181 >>   Num examples = 841\n",
            "[INFO|trainer.py:2912] 2023-02-15 18:29:32,181 >>   Batch size = 8\n",
            "100% 106/106 [00:06<00:00, 16.38it/s]\n",
            "INFO:__main__:***** Predict results None *****\n",
            "[INFO|modelcard.py:444] 2023-02-15 18:29:38,859 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.7254902124404907}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Score : 0.725**\n"
      ],
      "metadata": {
        "id": "6IE1HOErLAAu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python run_glue.py \\\n",
        "  --cache_dir .cache_training \\\n",
        "  --model_name_or_path 'out/beer_reviews/roberta_hidden_v1' \\\n",
        "  --train_file data/production.json  \\\n",
        "  --validation_file data/production.json \\\n",
        "  --per_device_eval_batch_size 24 \\\n",
        "  --do_eval \\\n",
        "  --max_seq_length 128 \\\n",
        "  --ignore_mismatched_sizes=True\\\n",
        "  --output_dir 'out/beer_reviews/roberta-evaluation'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFSY6tz0qpSJ",
        "outputId": "37397418-64e5-46dc-9592-c98461cff7ae"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-15 18:29:44.472508: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-02-15 18:29:45.947304: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-15 18:29:45.948064: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-15 18:29:45.948089: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=passive,\n",
            "log_on_each_node=True,\n",
            "logging_dir=out/beer_reviews/roberta-evaluation/runs/Feb15_18-29-48_ed170e621d72,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=out/beer_reviews/roberta-evaluation,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=24,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=out/beer_reviews/roberta-evaluation,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "INFO:__main__:load a local file for train: data/production.json\n",
            "INFO:__main__:load a local file for validation: data/production.json\n",
            "WARNING:datasets.builder:Using custom data configuration default-d5635c3fd9639ba6\n",
            "INFO:datasets.info:Loading Dataset Infos from /usr/local/lib/python3.8/dist-packages/datasets/packaged_modules/json\n",
            "INFO:datasets.builder:Generating dataset json (/content/uczenie_glebokie/.cache_training/json/default-d5635c3fd9639ba6/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n",
            "Downloading and preparing dataset json/default to /content/uczenie_glebokie/.cache_training/json/default-d5635c3fd9639ba6/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51...\n",
            "Downloading data files: 100% 2/2 [00:00<00:00, 9331.04it/s]\n",
            "INFO:datasets.download.download_manager:Downloading took 0.0 min\n",
            "INFO:datasets.download.download_manager:Checksum Computation took 0.0 min\n",
            "Extracting data files: 100% 2/2 [00:00<00:00, 1318.34it/s]\n",
            "INFO:datasets.utils.info_utils:Unable to verify checksums.\n",
            "INFO:datasets.builder:Generating train split\n",
            "INFO:datasets.builder:Generating validation split\n",
            "INFO:datasets.utils.info_utils:Unable to verify splits sizes.\n",
            "Dataset json downloaded and prepared to /content/uczenie_glebokie/.cache_training/json/default-d5635c3fd9639ba6/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51. Subsequent calls will reuse this data.\n",
            "100% 2/2 [00:00<00:00, 731.35it/s]\n",
            "[INFO|configuration_utils.py:651] 2023-02-15 18:29:49,810 >> loading configuration file out/beer_reviews/roberta_hidden_v1/config.json\n",
            "[INFO|configuration_utils.py:705] 2023-02-15 18:29:49,811 >> Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"out/beer_reviews/roberta_hidden_v1\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForSequenceClassificationCustom\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": 0,\n",
            "    \"1\": 1,\n",
            "    \"2\": 2\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"0\": 0,\n",
            "    \"1\": 1,\n",
            "    \"2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"problem_type\": \"single_label_classification\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.23.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"use_hidden_states\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1771] 2023-02-15 18:29:49,837 >> loading file vocab.json\n",
            "[INFO|tokenization_utils_base.py:1771] 2023-02-15 18:29:49,837 >> loading file merges.txt\n",
            "[INFO|tokenization_utils_base.py:1771] 2023-02-15 18:29:49,837 >> loading file tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1771] 2023-02-15 18:29:49,837 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:1771] 2023-02-15 18:29:49,837 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:1771] 2023-02-15 18:29:49,837 >> loading file tokenizer_config.json\n",
            "INFO:__main__:Using implementation from class: AutoModelForSequenceClassification\n",
            "[INFO|modeling_utils.py:2153] 2023-02-15 18:29:50,009 >> loading weights file out/beer_reviews/roberta_hidden_v1/pytorch_model.bin\n",
            "[WARNING|modeling_utils.py:2596] 2023-02-15 18:29:51,778 >> Some weights of the model checkpoint at out/beer_reviews/roberta_hidden_v1 were not used when initializing RobertaForSequenceClassification: ['classifier.dense_1.bias', 'classifier.dense_6.bias', 'classifier.dense_5.weight', 'classifier.dense_3.weight', 'classifier.dense_5.bias', 'classifier.dense_1.weight', 'classifier.dense_4.weight', 'classifier.dense_3.bias', 'classifier.dense_2.bias', 'classifier.dense_2.weight', 'classifier.dense_4.bias', 'classifier.dense_6.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:2608] 2023-02-15 18:29:51,778 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at out/beer_reviews/roberta_hidden_v1 and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[WARNING|modeling_utils.py:2627] 2023-02-15 18:29:51,778 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at out/beer_reviews/roberta_hidden_v1 and are newly initialized because the shapes did not match:\n",
            "- classifier.out_proj.weight: found shape torch.Size([3, 1536]) in the checkpoint and torch.Size([3, 768]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset:   0% 0/1 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /content/uczenie_glebokie/.cache_training/json/default-d5635c3fd9639ba6/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-cd4d4b4bb91a8822.arrow\n",
            "Running tokenizer on dataset: 100% 1/1 [00:00<00:00,  3.11ba/s]\n",
            "Running tokenizer on dataset:   0% 0/1 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /content/uczenie_glebokie/.cache_training/json/default-d5635c3fd9639ba6/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-f8354ec4e8bba037.arrow\n",
            "Running tokenizer on dataset: 100% 1/1 [00:00<00:00,  3.11ba/s]\n",
            "INFO:__main__:*** Evaluate ***\n",
            "[INFO|trainer.py:725] 2023-02-15 18:29:58,435 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2907] 2023-02-15 18:29:58,439 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2023-02-15 18:29:58,439 >>   Num examples = 841\n",
            "[INFO|trainer.py:2912] 2023-02-15 18:29:58,439 >>   Batch size = 24\n",
            "100% 36/36 [00:06<00:00,  5.34it/s]\n",
            "***** eval metrics *****\n",
            "  eval_accuracy           =      0.478\n",
            "  eval_loss               =     1.0703\n",
            "  eval_runtime            = 0:00:07.48\n",
            "  eval_samples            =        841\n",
            "  eval_samples_per_second =    112.289\n",
            "  eval_steps_per_second   =      4.807\n",
            "[INFO|modelcard.py:444] 2023-02-15 18:30:05,932 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Text Classification', 'type': 'text-classification'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Custom classifier head - GPT-2**"
      ],
      "metadata": {
        "id": "NV-op1YilRJp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! python run_glue.py \\\n",
        "  --cache_dir .cache_training \\\n",
        "  --model_name_or_path gpt2 \\\n",
        "  --custom_model gpt2_hidden\\\n",
        "  --train_file data/training.json  \\\n",
        "  --validation_file data/validation.json \\\n",
        "  --test_file data/production.json \\\n",
        "  --per_device_train_batch_size 8 \\\n",
        "  --per_device_eval_batch_size 8 \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --do_predict \\\n",
        "  --max_seq_length 128 \\\n",
        "  --learning_rate 2e-5 \\\n",
        "  --max_eval_samples 2000 \\\n",
        "  --max_steps 2500 \\\n",
        "  --num_train_epochs 1 \\\n",
        "  --save_strategy steps \\\n",
        "  --save_steps 250 \\\n",
        "  --save_total_limit 5 \\\n",
        "  --logging_strategy steps \\\n",
        "  --logging_steps 100 \\\n",
        "  --eval_steps 250 \\\n",
        "  --evaluation_strategy steps \\\n",
        "  --metric_for_best_model accuracy \\\n",
        "  --greater_is_better True \\\n",
        "  --load_best_model_at_end True \\\n",
        "  --output_dir out/beer_reviews/gpt2_hidden_v1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g98bA6Eig6pr",
        "outputId": "c9e1af83-14b4-45ae-8984-037e00aedc9a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-15 18:30:08.831557: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-02-15 18:30:09.733911: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-15 18:30:09.734053: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-15 18:30:09.734073: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=250,\n",
            "evaluation_strategy=steps,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=passive,\n",
            "log_on_each_node=True,\n",
            "logging_dir=out/beer_reviews/gpt2_hidden_v1/runs/Feb15_18-30-11_ed170e621d72,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=100,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=2500,\n",
            "metric_for_best_model=accuracy,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=1.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=out/beer_reviews/gpt2_hidden_v1,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=out/beer_reviews/gpt2_hidden_v1,\n",
            "save_on_each_node=False,\n",
            "save_steps=250,\n",
            "save_strategy=steps,\n",
            "save_total_limit=5,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "INFO:__main__:load a local file for train: data/training.json\n",
            "INFO:__main__:load a local file for validation: data/validation.json\n",
            "INFO:__main__:load a local file for test: data/production.json\n",
            "WARNING:datasets.builder:Using custom data configuration default-618d4135584dd38b\n",
            "INFO:datasets.info:Loading Dataset Infos from /usr/local/lib/python3.8/dist-packages/datasets/packaged_modules/json\n",
            "INFO:datasets.builder:Overwrite dataset info from restored data version.\n",
            "INFO:datasets.info:Loading Dataset info from .cache_training/json/default-618d4135584dd38b/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51\n",
            "WARNING:datasets.builder:Found cached dataset json (/content/uczenie_glebokie/.cache_training/json/default-618d4135584dd38b/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n",
            "INFO:datasets.info:Loading Dataset info from /content/uczenie_glebokie/.cache_training/json/default-618d4135584dd38b/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51\n",
            "100% 3/3 [00:00<00:00, 79.20it/s]\n",
            "Downloading (…)lve/main/config.json: 100% 665/665 [00:00<00:00, 105kB/s]\n",
            "[INFO|configuration_utils.py:653] 2023-02-15 18:30:12,699 >> loading configuration file config.json from cache at .cache_training/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/config.json\n",
            "[INFO|configuration_utils.py:705] 2023-02-15 18:30:12,700 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.23.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:418] 2023-02-15 18:30:12,826 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:653] 2023-02-15 18:30:12,962 >> loading configuration file config.json from cache at .cache_training/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/config.json\n",
            "[INFO|configuration_utils.py:705] 2023-02-15 18:30:12,963 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.23.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "Downloading (…)olve/main/vocab.json: 100% 1.04M/1.04M [00:00<00:00, 6.32MB/s]\n",
            "Downloading (…)olve/main/merges.txt: 100% 456k/456k [00:00<00:00, 3.37MB/s]\n",
            "Downloading (…)/main/tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 8.22MB/s]\n",
            "[INFO|tokenization_utils_base.py:1773] 2023-02-15 18:30:14,698 >> loading file vocab.json from cache at .cache_training/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/vocab.json\n",
            "[INFO|tokenization_utils_base.py:1773] 2023-02-15 18:30:14,699 >> loading file merges.txt from cache at .cache_training/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/merges.txt\n",
            "[INFO|tokenization_utils_base.py:1773] 2023-02-15 18:30:14,699 >> loading file tokenizer.json from cache at .cache_training/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1773] 2023-02-15 18:30:14,699 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1773] 2023-02-15 18:30:14,699 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1773] 2023-02-15 18:30:14,699 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:653] 2023-02-15 18:30:14,699 >> loading configuration file config.json from cache at .cache_training/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/config.json\n",
            "[INFO|configuration_utils.py:705] 2023-02-15 18:30:14,700 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.23.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "INFO:__main__:Using hidden states in model: True\n",
            "INFO:__main__:Using implementation from class: GPT2ForSequenceClassificationCustom\n",
            "Downloading (…)\"pytorch_model.bin\";: 100% 548M/548M [00:03<00:00, 165MB/s]\n",
            "[INFO|modeling_utils.py:2156] 2023-02-15 18:30:18,349 >> loading weights file pytorch_model.bin from cache at .cache_training/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:2606] 2023-02-15 18:30:20,829 >> All model checkpoint weights were used when initializing GPT2ForSequenceClassificationCustom.\n",
            "\n",
            "[WARNING|modeling_utils.py:2608] 2023-02-15 18:30:20,829 >> Some weights of GPT2ForSequenceClassificationCustom were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.dense_2.weight', 'score.dense_2.bias', 'score.dense_1_hidden.weight', 'score.out_proj.weight', 'score.dense_1_hidden.bias', 'score.dense_1_input.weight', 'score.dense_1_input.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "\n",
            "\n",
            " frozen Layer\n",
            "[('transformer.wte.weight', True), ('transformer.wpe.weight', True), ('transformer.h.0.ln_1.weight', True), ('transformer.h.0.ln_1.bias', True), ('transformer.h.0.attn.c_attn.weight', True), ('transformer.h.0.attn.c_attn.bias', True), ('transformer.h.0.attn.c_proj.weight', True), ('transformer.h.0.attn.c_proj.bias', True), ('transformer.h.0.ln_2.weight', True), ('transformer.h.0.ln_2.bias', True), ('transformer.h.0.mlp.c_fc.weight', True), ('transformer.h.0.mlp.c_fc.bias', True), ('transformer.h.0.mlp.c_proj.weight', True), ('transformer.h.0.mlp.c_proj.bias', True), ('transformer.h.1.ln_1.weight', True), ('transformer.h.1.ln_1.bias', True), ('transformer.h.1.attn.c_attn.weight', True), ('transformer.h.1.attn.c_attn.bias', True), ('transformer.h.1.attn.c_proj.weight', True), ('transformer.h.1.attn.c_proj.bias', True), ('transformer.h.1.ln_2.weight', True), ('transformer.h.1.ln_2.bias', True), ('transformer.h.1.mlp.c_fc.weight', True), ('transformer.h.1.mlp.c_fc.bias', True), ('transformer.h.1.mlp.c_proj.weight', True), ('transformer.h.1.mlp.c_proj.bias', True), ('transformer.h.2.ln_1.weight', True), ('transformer.h.2.ln_1.bias', True), ('transformer.h.2.attn.c_attn.weight', True), ('transformer.h.2.attn.c_attn.bias', True), ('transformer.h.2.attn.c_proj.weight', True), ('transformer.h.2.attn.c_proj.bias', True), ('transformer.h.2.ln_2.weight', True), ('transformer.h.2.ln_2.bias', True), ('transformer.h.2.mlp.c_fc.weight', True), ('transformer.h.2.mlp.c_fc.bias', True), ('transformer.h.2.mlp.c_proj.weight', True), ('transformer.h.2.mlp.c_proj.bias', True), ('transformer.h.3.ln_1.weight', True), ('transformer.h.3.ln_1.bias', True), ('transformer.h.3.attn.c_attn.weight', True), ('transformer.h.3.attn.c_attn.bias', True), ('transformer.h.3.attn.c_proj.weight', True), ('transformer.h.3.attn.c_proj.bias', True), ('transformer.h.3.ln_2.weight', True), ('transformer.h.3.ln_2.bias', True), ('transformer.h.3.mlp.c_fc.weight', True), ('transformer.h.3.mlp.c_fc.bias', True), ('transformer.h.3.mlp.c_proj.weight', True), ('transformer.h.3.mlp.c_proj.bias', True), ('transformer.h.4.ln_1.weight', True), ('transformer.h.4.ln_1.bias', True), ('transformer.h.4.attn.c_attn.weight', True), ('transformer.h.4.attn.c_attn.bias', True), ('transformer.h.4.attn.c_proj.weight', True), ('transformer.h.4.attn.c_proj.bias', True), ('transformer.h.4.ln_2.weight', True), ('transformer.h.4.ln_2.bias', True), ('transformer.h.4.mlp.c_fc.weight', True), ('transformer.h.4.mlp.c_fc.bias', True), ('transformer.h.4.mlp.c_proj.weight', True), ('transformer.h.4.mlp.c_proj.bias', True), ('transformer.h.5.ln_1.weight', True), ('transformer.h.5.ln_1.bias', True), ('transformer.h.5.attn.c_attn.weight', True), ('transformer.h.5.attn.c_attn.bias', True), ('transformer.h.5.attn.c_proj.weight', True), ('transformer.h.5.attn.c_proj.bias', True), ('transformer.h.5.ln_2.weight', True), ('transformer.h.5.ln_2.bias', True), ('transformer.h.5.mlp.c_fc.weight', True), ('transformer.h.5.mlp.c_fc.bias', True), ('transformer.h.5.mlp.c_proj.weight', True), ('transformer.h.5.mlp.c_proj.bias', True), ('transformer.h.6.ln_1.weight', True), ('transformer.h.6.ln_1.bias', True), ('transformer.h.6.attn.c_attn.weight', True), ('transformer.h.6.attn.c_attn.bias', True), ('transformer.h.6.attn.c_proj.weight', True), ('transformer.h.6.attn.c_proj.bias', True), ('transformer.h.6.ln_2.weight', True), ('transformer.h.6.ln_2.bias', True), ('transformer.h.6.mlp.c_fc.weight', True), ('transformer.h.6.mlp.c_fc.bias', True), ('transformer.h.6.mlp.c_proj.weight', True), ('transformer.h.6.mlp.c_proj.bias', True), ('transformer.h.7.ln_1.weight', True), ('transformer.h.7.ln_1.bias', True), ('transformer.h.7.attn.c_attn.weight', True), ('transformer.h.7.attn.c_attn.bias', True), ('transformer.h.7.attn.c_proj.weight', True), ('transformer.h.7.attn.c_proj.bias', True), ('transformer.h.7.ln_2.weight', True), ('transformer.h.7.ln_2.bias', True), ('transformer.h.7.mlp.c_fc.weight', True), ('transformer.h.7.mlp.c_fc.bias', True), ('transformer.h.7.mlp.c_proj.weight', True), ('transformer.h.7.mlp.c_proj.bias', True), ('transformer.h.8.ln_1.weight', True), ('transformer.h.8.ln_1.bias', True), ('transformer.h.8.attn.c_attn.weight', True), ('transformer.h.8.attn.c_attn.bias', True), ('transformer.h.8.attn.c_proj.weight', True), ('transformer.h.8.attn.c_proj.bias', True), ('transformer.h.8.ln_2.weight', True), ('transformer.h.8.ln_2.bias', True), ('transformer.h.8.mlp.c_fc.weight', True), ('transformer.h.8.mlp.c_fc.bias', True), ('transformer.h.8.mlp.c_proj.weight', True), ('transformer.h.8.mlp.c_proj.bias', True), ('transformer.h.9.ln_1.weight', True), ('transformer.h.9.ln_1.bias', True), ('transformer.h.9.attn.c_attn.weight', True), ('transformer.h.9.attn.c_attn.bias', True), ('transformer.h.9.attn.c_proj.weight', True), ('transformer.h.9.attn.c_proj.bias', True), ('transformer.h.9.ln_2.weight', True), ('transformer.h.9.ln_2.bias', True), ('transformer.h.9.mlp.c_fc.weight', True), ('transformer.h.9.mlp.c_fc.bias', True), ('transformer.h.9.mlp.c_proj.weight', True), ('transformer.h.9.mlp.c_proj.bias', True), ('transformer.h.10.ln_1.weight', True), ('transformer.h.10.ln_1.bias', True), ('transformer.h.10.attn.c_attn.weight', True), ('transformer.h.10.attn.c_attn.bias', True), ('transformer.h.10.attn.c_proj.weight', True), ('transformer.h.10.attn.c_proj.bias', True), ('transformer.h.10.ln_2.weight', True), ('transformer.h.10.ln_2.bias', True), ('transformer.h.10.mlp.c_fc.weight', True), ('transformer.h.10.mlp.c_fc.bias', True), ('transformer.h.10.mlp.c_proj.weight', True), ('transformer.h.10.mlp.c_proj.bias', True), ('transformer.h.11.ln_1.weight', True), ('transformer.h.11.ln_1.bias', True), ('transformer.h.11.attn.c_attn.weight', True), ('transformer.h.11.attn.c_attn.bias', True), ('transformer.h.11.attn.c_proj.weight', True), ('transformer.h.11.attn.c_proj.bias', True), ('transformer.h.11.ln_2.weight', True), ('transformer.h.11.ln_2.bias', True), ('transformer.h.11.mlp.c_fc.weight', True), ('transformer.h.11.mlp.c_fc.bias', True), ('transformer.h.11.mlp.c_proj.weight', True), ('transformer.h.11.mlp.c_proj.bias', True), ('transformer.ln_f.weight', True), ('transformer.ln_f.bias', True), ('score.dense_1_input.weight', True), ('score.dense_1_input.bias', True), ('score.dense_1_hidden.weight', True), ('score.dense_1_hidden.bias', True), ('score.dense_2.weight', True), ('score.dense_2.bias', True), ('score.out_proj.weight', True)]\n",
            "[ERROR|tokenization_utils_base.py:1019] 2023-02-15 18:30:20,836 >> Using pad_token, but it is not set yet.\n",
            "INFO:__main__:Set PAD token to EOS: <|endoftext|>\n",
            "Running tokenizer on dataset:   0% 0/9 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /content/uczenie_glebokie/.cache_training/json/default-618d4135584dd38b/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-4deeae38f0aa5cd7.arrow\n",
            "Running tokenizer on dataset: 100% 9/9 [00:03<00:00,  2.73ba/s]\n",
            "Running tokenizer on dataset:   0% 0/1 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /content/uczenie_glebokie/.cache_training/json/default-618d4135584dd38b/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-d86563e3e7a661e9.arrow\n",
            "Running tokenizer on dataset: 100% 1/1 [00:00<00:00,  6.82ba/s]\n",
            "Running tokenizer on dataset:   0% 0/1 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /content/uczenie_glebokie/.cache_training/json/default-618d4135584dd38b/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-a911877f01c9e2b3.arrow\n",
            "Running tokenizer on dataset: 100% 1/1 [00:00<00:00,  3.40ba/s]\n",
            "INFO:__main__:Set 139 samples for 0-class\n",
            "INFO:__main__:Set 139 samples for 1-class\n",
            "INFO:__main__:Set 139 samples for 2-class\n",
            "INFO:__main__:Sample 1824 of the training set: {'label': 1, 'text': \"Poured from a bottle into my pewter Jagerstein at refrigetor chilled temperature. I peaked a look at the color from my friends glass and found it to be somewhere between amber and stout/porter black. The kind of color that would decieve the average drinker into thinking that this was going to be a beer as thick as motor oil. But that's not how they roll in VT. The smell betrayed a sweeter character, almost over emphasizing the malt properties. The taste was a bit on the hoppy side. That is to say that, the aftertaste left you remembering more of the hops than the also very strong, caramel malt taste. The mouthfeel was perfect for a cool summer night. This would probably be a great beer from late April to early June and late September to early November. I could drink a few of these at a time, but would probably want to move on to something lighter after that.\", 'input_ids': [47, 8167, 422, 257, 9294, 656, 616, 279, 413, 353, 449, 3536, 5714, 379, 1006, 4359, 316, 273, 45550, 5951, 13, 314, 33948, 257, 804, 379, 262, 3124, 422, 616, 2460, 5405, 290, 1043, 340, 284, 307, 7382, 1022, 36505, 290, 39171, 14, 26634, 2042, 13, 383, 1611, 286, 3124, 326, 561, 875, 12311, 262, 2811, 4144, 263, 656, 3612, 326, 428, 373, 1016, 284, 307, 257, 6099, 355, 6546, 355, 5584, 3056, 13, 887, 326, 338, 407, 703, 484, 4836, 287, 32751, 13, 383, 8508, 26281, 257, 3490, 2357, 2095, 11, 2048, 625, 36360, 262, 26868, 6608, 13, 383, 6938, 373, 257, 1643, 319, 262, 8169, 14097, 1735, 13, 1320, 318, 284, 910, 326, 11, 262, 706, 83, 4594, 1364, 345, 24865, 517, 286, 262, 29438, 621], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.\n",
            "INFO:__main__:Sample 409 of the training set: {'label': 1, 'text': 'This beer isn\\'t bad, but there really isn\\'t anything I can say to strongly recommend it; there aren\\'t any really distinctive or unique flavors and I thought I detected a faint flavor defect or two. Organic Revolution doesn\\'t do anything to change my impression that in general the quality (or at least degree of \"interesting\") organic beers remains distinctly below that of non-organic beers. Pours a thick gold lager-color with a very tacky white head that is loosely packed into the top of my glass. Appearance of the beer\\'s body is a bit disconcerting - it has an oily-swirlyness to it. Aroma is only slightly hoppy, with some fruity vinous-ness. Flavor is balanced, but a bit dull with a very faint hoppiness. Some vinous-ness and diacetyl are evident to me.', 'input_ids': [1212, 6099, 2125, 470, 2089, 11, 475, 612, 1107, 2125, 470, 1997, 314, 460, 910, 284, 7634, 4313, 340, 26, 612, 3588, 470, 597, 1107, 18778, 393, 3748, 17361, 290, 314, 1807, 314, 12326, 257, 18107, 9565, 11855, 393, 734, 13, 31734, 9303, 1595, 470, 466, 1997, 284, 1487, 616, 10647, 326, 287, 2276, 262, 3081, 357, 273, 379, 1551, 4922, 286, 366, 47914, 4943, 10469, 16800, 3793, 30911, 2174, 326, 286, 1729, 12, 36617, 16800, 13, 350, 4662, 257, 6546, 3869, 300, 3536, 12, 8043, 351, 257, 845, 6331, 88, 2330, 1182, 326, 318, 28845, 11856, 656, 262, 1353, 286, 616, 5405, 13, 43436, 286, 262, 6099, 338, 1767, 318, 257, 1643, 595, 48415, 278, 532, 340, 468, 281, 44560, 12, 2032, 343, 306, 1108, 284, 340], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.\n",
            "INFO:__main__:Sample 4506 of the training set: {'label': 2, 'text': \"On tap at the brewery on 6/28/09. Pours solid black with mocha head, great lacing. Aroma is certainly dominated by chocolate, but gives way to smoke and coffee. This is a straight up sweet, smooth, balanced stout with a great chocolate finish, and for the style, it's incredible. Will be looking for this on the east coast.\", 'input_ids': [2202, 9814, 379, 262, 22120, 319, 718, 14, 2078, 14, 2931, 13, 350, 4662, 4735, 2042, 351, 285, 5374, 64, 1182, 11, 1049, 300, 4092, 13, 317, 42902, 318, 3729, 13354, 416, 11311, 11, 475, 3607, 835, 284, 7523, 290, 6891, 13, 770, 318, 257, 3892, 510, 6029, 11, 7209, 11, 12974, 39171, 351, 257, 1049, 11311, 5461, 11, 290, 329, 262, 3918, 11, 340, 338, 8082, 13, 2561, 307, 2045, 329, 428, 319, 262, 7627, 7051, 13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "[INFO|trainer.py:503] 2023-02-15 18:30:27,263 >> max_steps is given, it will override any value given in num_train_epochs\n",
            "[INFO|trainer.py:725] 2023-02-15 18:30:27,263 >> The following columns in the training set don't have a corresponding argument in `GPT2ForSequenceClassificationCustom.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassificationCustom.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "[INFO|trainer.py:1607] 2023-02-15 18:30:27,269 >> ***** Running training *****\n",
            "[INFO|trainer.py:1608] 2023-02-15 18:30:27,269 >>   Num examples = 9000\n",
            "[INFO|trainer.py:1609] 2023-02-15 18:30:27,269 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1610] 2023-02-15 18:30:27,269 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1611] 2023-02-15 18:30:27,269 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:1612] 2023-02-15 18:30:27,269 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1613] 2023-02-15 18:30:27,269 >>   Total optimization steps = 2500\n",
            "{'loss': 0.9419, 'learning_rate': 1.9200000000000003e-05, 'epoch': 0.09}\n",
            "{'loss': 0.9309, 'learning_rate': 1.8400000000000003e-05, 'epoch': 0.18}\n",
            " 10% 250/2500 [01:05<09:27,  3.97it/s][INFO|trainer.py:725] 2023-02-15 18:31:33,231 >> The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassificationCustom.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassificationCustom.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2907] 2023-02-15 18:31:33,233 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2023-02-15 18:31:33,233 >>   Num examples = 306\n",
            "[INFO|trainer.py:2912] 2023-02-15 18:31:33,233 >>   Batch size = 8\n",
            "\n",
            "  0% 0/39 [00:00<?, ?it/s]\u001b[A\n",
            "  8% 3/39 [00:00<00:01, 22.07it/s]\u001b[A\n",
            " 15% 6/39 [00:00<00:01, 17.01it/s]\u001b[A\n",
            " 21% 8/39 [00:00<00:01, 16.03it/s]\u001b[A\n",
            " 26% 10/39 [00:00<00:01, 15.35it/s]\u001b[A\n",
            " 31% 12/39 [00:00<00:01, 15.13it/s]\u001b[A\n",
            " 36% 14/39 [00:00<00:01, 14.93it/s]\u001b[A\n",
            " 41% 16/39 [00:01<00:01, 14.84it/s]\u001b[A\n",
            " 46% 18/39 [00:01<00:01, 14.82it/s]\u001b[A\n",
            " 51% 20/39 [00:01<00:01, 14.72it/s]\u001b[A\n",
            " 56% 22/39 [00:01<00:01, 14.70it/s]\u001b[A\n",
            " 62% 24/39 [00:01<00:01, 14.58it/s]\u001b[A\n",
            " 67% 26/39 [00:01<00:00, 14.50it/s]\u001b[A\n",
            " 72% 28/39 [00:01<00:00, 14.42it/s]\u001b[A\n",
            " 77% 30/39 [00:01<00:00, 14.40it/s]\u001b[A\n",
            " 82% 32/39 [00:02<00:00, 14.42it/s]\u001b[A\n",
            " 87% 34/39 [00:02<00:00, 14.44it/s]\u001b[A\n",
            " 92% 36/39 [00:02<00:00, 14.43it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.8695966005325317, 'eval_accuracy': 0.5915032625198364, 'eval_runtime': 2.6522, 'eval_samples_per_second': 115.377, 'eval_steps_per_second': 14.705, 'epoch': 0.22}\n",
            " 10% 250/2500 [01:08<09:27,  3.97it/s]\n",
            "100% 39/39 [00:02<00:00, 14.42it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2656] 2023-02-15 18:31:35,887 >> Saving model checkpoint to out/beer_reviews/gpt2_hidden_v1/checkpoint-250\n",
            "[INFO|configuration_utils.py:447] 2023-02-15 18:31:35,889 >> Configuration saved in out/beer_reviews/gpt2_hidden_v1/checkpoint-250/config.json\n",
            "[INFO|modeling_utils.py:1624] 2023-02-15 18:31:37,667 >> Model weights saved in out/beer_reviews/gpt2_hidden_v1/checkpoint-250/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2023-02-15 18:31:37,668 >> tokenizer config file saved in out/beer_reviews/gpt2_hidden_v1/checkpoint-250/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2023-02-15 18:31:37,668 >> Special tokens file saved in out/beer_reviews/gpt2_hidden_v1/checkpoint-250/special_tokens_map.json\n",
            "{'loss': 0.7796, 'learning_rate': 1.76e-05, 'epoch': 0.27}\n",
            "{'loss': 0.7461, 'learning_rate': 1.6800000000000002e-05, 'epoch': 0.36}\n",
            "{'loss': 0.7027, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.44}\n",
            " 20% 500/2500 [02:19<08:39,  3.85it/s][INFO|trainer.py:725] 2023-02-15 18:32:46,952 >> The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassificationCustom.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassificationCustom.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2907] 2023-02-15 18:32:46,953 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2023-02-15 18:32:46,953 >>   Num examples = 306\n",
            "[INFO|trainer.py:2912] 2023-02-15 18:32:46,953 >>   Batch size = 8\n",
            "\n",
            "  0% 0/39 [00:00<?, ?it/s]\u001b[A\n",
            "  8% 3/39 [00:00<00:01, 21.35it/s]\u001b[A\n",
            " 15% 6/39 [00:00<00:01, 16.64it/s]\u001b[A\n",
            " 21% 8/39 [00:00<00:01, 15.89it/s]\u001b[A\n",
            " 26% 10/39 [00:00<00:01, 15.19it/s]\u001b[A\n",
            " 31% 12/39 [00:00<00:01, 14.99it/s]\u001b[A\n",
            " 36% 14/39 [00:00<00:01, 14.66it/s]\u001b[A\n",
            " 41% 16/39 [00:01<00:01, 14.65it/s]\u001b[A\n",
            " 46% 18/39 [00:01<00:01, 14.62it/s]\u001b[A\n",
            " 51% 20/39 [00:01<00:01, 14.54it/s]\u001b[A\n",
            " 56% 22/39 [00:01<00:01, 14.48it/s]\u001b[A\n",
            " 62% 24/39 [00:01<00:01, 14.44it/s]\u001b[A\n",
            " 67% 26/39 [00:01<00:00, 14.40it/s]\u001b[A\n",
            " 72% 28/39 [00:01<00:00, 14.39it/s]\u001b[A\n",
            " 77% 30/39 [00:02<00:00, 14.38it/s]\u001b[A\n",
            " 82% 32/39 [00:02<00:00, 14.43it/s]\u001b[A\n",
            " 87% 34/39 [00:02<00:00, 14.48it/s]\u001b[A\n",
            " 92% 36/39 [00:02<00:00, 14.52it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.8299143314361572, 'eval_accuracy': 0.6143791079521179, 'eval_runtime': 2.6525, 'eval_samples_per_second': 115.364, 'eval_steps_per_second': 14.703, 'epoch': 0.44}\n",
            " 20% 500/2500 [02:22<08:39,  3.85it/s]\n",
            "100% 39/39 [00:02<00:00, 14.50it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2656] 2023-02-15 18:32:49,608 >> Saving model checkpoint to out/beer_reviews/gpt2_hidden_v1/checkpoint-500\n",
            "[INFO|configuration_utils.py:447] 2023-02-15 18:32:49,608 >> Configuration saved in out/beer_reviews/gpt2_hidden_v1/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:1624] 2023-02-15 18:32:51,231 >> Model weights saved in out/beer_reviews/gpt2_hidden_v1/checkpoint-500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2023-02-15 18:32:51,232 >> tokenizer config file saved in out/beer_reviews/gpt2_hidden_v1/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2023-02-15 18:32:51,232 >> Special tokens file saved in out/beer_reviews/gpt2_hidden_v1/checkpoint-500/special_tokens_map.json\n",
            "{'loss': 0.7181, 'learning_rate': 1.5200000000000002e-05, 'epoch': 0.53}\n",
            "{'loss': 0.7239, 'learning_rate': 1.4400000000000001e-05, 'epoch': 0.62}\n",
            " 30% 750/2500 [03:32<07:28,  3.90it/s][INFO|trainer.py:725] 2023-02-15 18:34:00,062 >> The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassificationCustom.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassificationCustom.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2907] 2023-02-15 18:34:00,064 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2023-02-15 18:34:00,064 >>   Num examples = 306\n",
            "[INFO|trainer.py:2912] 2023-02-15 18:34:00,064 >>   Batch size = 8\n",
            "\n",
            "  0% 0/39 [00:00<?, ?it/s]\u001b[A\n",
            "  8% 3/39 [00:00<00:01, 21.47it/s]\u001b[A\n",
            " 15% 6/39 [00:00<00:01, 16.68it/s]\u001b[A\n",
            " 21% 8/39 [00:00<00:01, 15.99it/s]\u001b[A\n",
            " 26% 10/39 [00:00<00:01, 15.30it/s]\u001b[A\n",
            " 31% 12/39 [00:00<00:01, 15.07it/s]\u001b[A\n",
            " 36% 14/39 [00:00<00:01, 14.75it/s]\u001b[A\n",
            " 41% 16/39 [00:01<00:01, 14.67it/s]\u001b[A\n",
            " 46% 18/39 [00:01<00:01, 14.66it/s]\u001b[A\n",
            " 51% 20/39 [00:01<00:01, 14.59it/s]\u001b[A\n",
            " 56% 22/39 [00:01<00:01, 14.58it/s]\u001b[A\n",
            " 62% 24/39 [00:01<00:01, 14.52it/s]\u001b[A\n",
            " 67% 26/39 [00:01<00:00, 14.56it/s]\u001b[A\n",
            " 72% 28/39 [00:01<00:00, 14.37it/s]\u001b[A\n",
            " 77% 30/39 [00:02<00:00, 14.33it/s]\u001b[A\n",
            " 82% 32/39 [00:02<00:00, 14.37it/s]\u001b[A\n",
            " 87% 34/39 [00:02<00:00, 14.42it/s]\u001b[A\n",
            " 92% 36/39 [00:02<00:00, 14.44it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.7869691252708435, 'eval_accuracy': 0.6209150552749634, 'eval_runtime': 2.6683, 'eval_samples_per_second': 114.681, 'eval_steps_per_second': 14.616, 'epoch': 0.67}\n",
            " 30% 750/2500 [03:35<07:28,  3.90it/s]\n",
            "100% 39/39 [00:02<00:00, 14.38it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2656] 2023-02-15 18:34:02,733 >> Saving model checkpoint to out/beer_reviews/gpt2_hidden_v1/checkpoint-750\n",
            "[INFO|configuration_utils.py:447] 2023-02-15 18:34:02,735 >> Configuration saved in out/beer_reviews/gpt2_hidden_v1/checkpoint-750/config.json\n",
            "[INFO|modeling_utils.py:1624] 2023-02-15 18:34:04,502 >> Model weights saved in out/beer_reviews/gpt2_hidden_v1/checkpoint-750/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2023-02-15 18:34:04,503 >> tokenizer config file saved in out/beer_reviews/gpt2_hidden_v1/checkpoint-750/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2023-02-15 18:34:04,503 >> Special tokens file saved in out/beer_reviews/gpt2_hidden_v1/checkpoint-750/special_tokens_map.json\n",
            "{'loss': 0.7222, 'learning_rate': 1.3600000000000002e-05, 'epoch': 0.71}\n",
            "{'loss': 0.7617, 'learning_rate': 1.2800000000000001e-05, 'epoch': 0.8}\n",
            "{'loss': 0.7169, 'learning_rate': 1.2e-05, 'epoch': 0.89}\n",
            " 40% 1000/2500 [04:46<06:30,  3.84it/s][INFO|trainer.py:725] 2023-02-15 18:35:13,558 >> The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassificationCustom.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassificationCustom.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2907] 2023-02-15 18:35:13,560 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2023-02-15 18:35:13,560 >>   Num examples = 306\n",
            "[INFO|trainer.py:2912] 2023-02-15 18:35:13,560 >>   Batch size = 8\n",
            "\n",
            "  0% 0/39 [00:00<?, ?it/s]\u001b[A\n",
            "  8% 3/39 [00:00<00:01, 21.19it/s]\u001b[A\n",
            " 15% 6/39 [00:00<00:01, 16.67it/s]\u001b[A\n",
            " 21% 8/39 [00:00<00:01, 15.87it/s]\u001b[A\n",
            " 26% 10/39 [00:00<00:01, 15.12it/s]\u001b[A\n",
            " 31% 12/39 [00:00<00:01, 15.00it/s]\u001b[A\n",
            " 36% 14/39 [00:00<00:01, 14.60it/s]\u001b[A\n",
            " 41% 16/39 [00:01<00:01, 14.55it/s]\u001b[A\n",
            " 46% 18/39 [00:01<00:01, 14.51it/s]\u001b[A\n",
            " 51% 20/39 [00:01<00:01, 14.54it/s]\u001b[A\n",
            " 56% 22/39 [00:01<00:01, 14.56it/s]\u001b[A\n",
            " 62% 24/39 [00:01<00:01, 14.39it/s]\u001b[A\n",
            " 67% 26/39 [00:01<00:00, 14.21it/s]\u001b[A\n",
            " 72% 28/39 [00:01<00:00, 13.89it/s]\u001b[A\n",
            " 77% 30/39 [00:02<00:00, 14.05it/s]\u001b[A\n",
            " 82% 32/39 [00:02<00:00, 14.21it/s]\u001b[A\n",
            " 87% 34/39 [00:02<00:00, 14.33it/s]\u001b[A\n",
            " 92% 36/39 [00:02<00:00, 14.26it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.7009358406066895, 'eval_accuracy': 0.6437908411026001, 'eval_runtime': 2.6811, 'eval_samples_per_second': 114.134, 'eval_steps_per_second': 14.546, 'epoch': 0.89}\n",
            " 40% 1000/2500 [04:48<06:30,  3.84it/s]\n",
            "100% 39/39 [00:02<00:00, 14.25it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2656] 2023-02-15 18:35:16,242 >> Saving model checkpoint to out/beer_reviews/gpt2_hidden_v1/checkpoint-1000\n",
            "[INFO|configuration_utils.py:447] 2023-02-15 18:35:16,243 >> Configuration saved in out/beer_reviews/gpt2_hidden_v1/checkpoint-1000/config.json\n",
            "[INFO|modeling_utils.py:1624] 2023-02-15 18:35:17,981 >> Model weights saved in out/beer_reviews/gpt2_hidden_v1/checkpoint-1000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2023-02-15 18:35:17,985 >> tokenizer config file saved in out/beer_reviews/gpt2_hidden_v1/checkpoint-1000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2023-02-15 18:35:17,986 >> Special tokens file saved in out/beer_reviews/gpt2_hidden_v1/checkpoint-1000/special_tokens_map.json\n",
            "{'loss': 0.6605, 'learning_rate': 1.1200000000000001e-05, 'epoch': 0.98}\n",
            "{'loss': 0.6613, 'learning_rate': 1.04e-05, 'epoch': 1.07}\n",
            " 50% 1250/2500 [05:59<05:21,  3.89it/s][INFO|trainer.py:725] 2023-02-15 18:36:26,904 >> The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassificationCustom.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassificationCustom.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2907] 2023-02-15 18:36:26,906 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2023-02-15 18:36:26,906 >>   Num examples = 306\n",
            "[INFO|trainer.py:2912] 2023-02-15 18:36:26,906 >>   Batch size = 8\n",
            "\n",
            "  0% 0/39 [00:00<?, ?it/s]\u001b[A\n",
            "  8% 3/39 [00:00<00:01, 21.92it/s]\u001b[A\n",
            " 15% 6/39 [00:00<00:01, 16.61it/s]\u001b[A\n",
            " 21% 8/39 [00:00<00:01, 15.80it/s]\u001b[A\n",
            " 26% 10/39 [00:00<00:01, 15.19it/s]\u001b[A\n",
            " 31% 12/39 [00:00<00:01, 15.04it/s]\u001b[A\n",
            " 36% 14/39 [00:00<00:01, 14.67it/s]\u001b[A\n",
            " 41% 16/39 [00:01<00:01, 14.62it/s]\u001b[A\n",
            " 46% 18/39 [00:01<00:01, 14.64it/s]\u001b[A\n",
            " 51% 20/39 [00:01<00:01, 14.54it/s]\u001b[A\n",
            " 56% 22/39 [00:01<00:01, 14.49it/s]\u001b[A\n",
            " 62% 24/39 [00:01<00:01, 14.42it/s]\u001b[A\n",
            " 67% 26/39 [00:01<00:00, 14.44it/s]\u001b[A\n",
            " 72% 28/39 [00:01<00:00, 14.37it/s]\u001b[A\n",
            " 77% 30/39 [00:02<00:00, 14.39it/s]\u001b[A\n",
            " 82% 32/39 [00:02<00:00, 14.46it/s]\u001b[A\n",
            " 87% 34/39 [00:02<00:00, 14.39it/s]\u001b[A\n",
            " 92% 36/39 [00:02<00:00, 14.41it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.7662166953086853, 'eval_accuracy': 0.6503267884254456, 'eval_runtime': 2.6701, 'eval_samples_per_second': 114.602, 'eval_steps_per_second': 14.606, 'epoch': 1.11}\n",
            " 50% 1250/2500 [06:02<05:21,  3.89it/s]\n",
            "100% 39/39 [00:02<00:00, 14.35it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2656] 2023-02-15 18:36:29,577 >> Saving model checkpoint to out/beer_reviews/gpt2_hidden_v1/checkpoint-1250\n",
            "[INFO|configuration_utils.py:447] 2023-02-15 18:36:29,578 >> Configuration saved in out/beer_reviews/gpt2_hidden_v1/checkpoint-1250/config.json\n",
            "[INFO|modeling_utils.py:1624] 2023-02-15 18:36:31,218 >> Model weights saved in out/beer_reviews/gpt2_hidden_v1/checkpoint-1250/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2023-02-15 18:36:31,219 >> tokenizer config file saved in out/beer_reviews/gpt2_hidden_v1/checkpoint-1250/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2023-02-15 18:36:31,219 >> Special tokens file saved in out/beer_reviews/gpt2_hidden_v1/checkpoint-1250/special_tokens_map.json\n",
            "{'loss': 0.6472, 'learning_rate': 9.600000000000001e-06, 'epoch': 1.16}\n",
            "{'loss': 0.6372, 'learning_rate': 8.8e-06, 'epoch': 1.24}\n",
            "{'loss': 0.614, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.33}\n",
            " 60% 1500/2500 [07:13<04:18,  3.88it/s][INFO|trainer.py:725] 2023-02-15 18:37:40,419 >> The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassificationCustom.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassificationCustom.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2907] 2023-02-15 18:37:40,422 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2023-02-15 18:37:40,422 >>   Num examples = 306\n",
            "[INFO|trainer.py:2912] 2023-02-15 18:37:40,422 >>   Batch size = 8\n",
            "\n",
            "  0% 0/39 [00:00<?, ?it/s]\u001b[A\n",
            "  8% 3/39 [00:00<00:01, 20.92it/s]\u001b[A\n",
            " 15% 6/39 [00:00<00:02, 16.30it/s]\u001b[A\n",
            " 21% 8/39 [00:00<00:01, 15.55it/s]\u001b[A\n",
            " 26% 10/39 [00:00<00:01, 15.05it/s]\u001b[A\n",
            " 31% 12/39 [00:00<00:01, 14.92it/s]\u001b[A\n",
            " 36% 14/39 [00:00<00:01, 14.56it/s]\u001b[A\n",
            " 41% 16/39 [00:01<00:01, 14.56it/s]\u001b[A\n",
            " 46% 18/39 [00:01<00:01, 14.47it/s]\u001b[A\n",
            " 51% 20/39 [00:01<00:01, 14.48it/s]\u001b[A\n",
            " 56% 22/39 [00:01<00:01, 14.51it/s]\u001b[A\n",
            " 62% 24/39 [00:01<00:01, 14.48it/s]\u001b[A\n",
            " 67% 26/39 [00:01<00:00, 14.48it/s]\u001b[A\n",
            " 72% 28/39 [00:01<00:00, 14.40it/s]\u001b[A\n",
            " 77% 30/39 [00:02<00:00, 14.36it/s]\u001b[A\n",
            " 82% 32/39 [00:02<00:00, 14.31it/s]\u001b[A\n",
            " 87% 34/39 [00:02<00:00, 14.33it/s]\u001b[A\n",
            " 92% 36/39 [00:02<00:00, 14.40it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.8014939427375793, 'eval_accuracy': 0.6274510025978088, 'eval_runtime': 2.6717, 'eval_samples_per_second': 114.535, 'eval_steps_per_second': 14.598, 'epoch': 1.33}\n",
            " 60% 1500/2500 [07:15<04:18,  3.88it/s]\n",
            "100% 39/39 [00:02<00:00, 14.42it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2656] 2023-02-15 18:37:43,095 >> Saving model checkpoint to out/beer_reviews/gpt2_hidden_v1/checkpoint-1500\n",
            "[INFO|configuration_utils.py:447] 2023-02-15 18:37:43,096 >> Configuration saved in out/beer_reviews/gpt2_hidden_v1/checkpoint-1500/config.json\n",
            "[INFO|modeling_utils.py:1624] 2023-02-15 18:37:44,744 >> Model weights saved in out/beer_reviews/gpt2_hidden_v1/checkpoint-1500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2023-02-15 18:37:44,745 >> tokenizer config file saved in out/beer_reviews/gpt2_hidden_v1/checkpoint-1500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2023-02-15 18:37:44,745 >> Special tokens file saved in out/beer_reviews/gpt2_hidden_v1/checkpoint-1500/special_tokens_map.json\n",
            "[INFO|trainer.py:2734] 2023-02-15 18:37:48,888 >> Deleting older checkpoint [out/beer_reviews/gpt2_hidden_v1/checkpoint-250] due to args.save_total_limit\n",
            "{'loss': 0.6244, 'learning_rate': 7.2000000000000005e-06, 'epoch': 1.42}\n",
            "{'loss': 0.6224, 'learning_rate': 6.4000000000000006e-06, 'epoch': 1.51}\n",
            " 70% 1750/2500 [08:26<03:11,  3.91it/s][INFO|trainer.py:725] 2023-02-15 18:38:53,648 >> The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassificationCustom.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassificationCustom.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2907] 2023-02-15 18:38:53,649 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2023-02-15 18:38:53,649 >>   Num examples = 306\n",
            "[INFO|trainer.py:2912] 2023-02-15 18:38:53,649 >>   Batch size = 8\n",
            "\n",
            "  0% 0/39 [00:00<?, ?it/s]\u001b[A\n",
            "  8% 3/39 [00:00<00:01, 21.84it/s]\u001b[A\n",
            " 15% 6/39 [00:00<00:01, 16.66it/s]\u001b[A\n",
            " 21% 8/39 [00:00<00:01, 15.87it/s]\u001b[A\n",
            " 26% 10/39 [00:00<00:01, 15.24it/s]\u001b[A\n",
            " 31% 12/39 [00:00<00:01, 15.03it/s]\u001b[A\n",
            " 36% 14/39 [00:00<00:01, 14.70it/s]\u001b[A\n",
            " 41% 16/39 [00:01<00:01, 14.65it/s]\u001b[A\n",
            " 46% 18/39 [00:01<00:01, 14.68it/s]\u001b[A\n",
            " 51% 20/39 [00:01<00:01, 14.63it/s]\u001b[A\n",
            " 56% 22/39 [00:01<00:01, 14.63it/s]\u001b[A\n",
            " 62% 24/39 [00:01<00:01, 14.49it/s]\u001b[A\n",
            " 67% 26/39 [00:01<00:00, 14.40it/s]\u001b[A\n",
            " 72% 28/39 [00:01<00:00, 14.33it/s]\u001b[A\n",
            " 77% 30/39 [00:02<00:00, 14.33it/s]\u001b[A\n",
            " 82% 32/39 [00:02<00:00, 14.43it/s]\u001b[A\n",
            " 87% 34/39 [00:02<00:00, 14.51it/s]\u001b[A\n",
            " 92% 36/39 [00:02<00:00, 14.56it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.8114284873008728, 'eval_accuracy': 0.6372548937797546, 'eval_runtime': 2.662, 'eval_samples_per_second': 114.949, 'eval_steps_per_second': 14.65, 'epoch': 1.56}\n",
            " 70% 1750/2500 [08:29<03:11,  3.91it/s]\n",
            "100% 39/39 [00:02<00:00, 14.51it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2656] 2023-02-15 18:38:56,312 >> Saving model checkpoint to out/beer_reviews/gpt2_hidden_v1/checkpoint-1750\n",
            "[INFO|configuration_utils.py:447] 2023-02-15 18:38:56,313 >> Configuration saved in out/beer_reviews/gpt2_hidden_v1/checkpoint-1750/config.json\n",
            "[INFO|modeling_utils.py:1624] 2023-02-15 18:38:57,952 >> Model weights saved in out/beer_reviews/gpt2_hidden_v1/checkpoint-1750/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2023-02-15 18:38:57,953 >> tokenizer config file saved in out/beer_reviews/gpt2_hidden_v1/checkpoint-1750/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2023-02-15 18:38:57,953 >> Special tokens file saved in out/beer_reviews/gpt2_hidden_v1/checkpoint-1750/special_tokens_map.json\n",
            "[INFO|trainer.py:2734] 2023-02-15 18:39:02,118 >> Deleting older checkpoint [out/beer_reviews/gpt2_hidden_v1/checkpoint-500] due to args.save_total_limit\n",
            "{'loss': 0.5986, 'learning_rate': 5.600000000000001e-06, 'epoch': 1.6}\n",
            "{'loss': 0.5988, 'learning_rate': 4.800000000000001e-06, 'epoch': 1.69}\n",
            "{'loss': 0.6088, 'learning_rate': 4.000000000000001e-06, 'epoch': 1.78}\n",
            " 80% 2000/2500 [09:39<02:09,  3.86it/s][INFO|trainer.py:725] 2023-02-15 18:40:06,959 >> The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassificationCustom.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassificationCustom.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2907] 2023-02-15 18:40:06,961 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2023-02-15 18:40:06,962 >>   Num examples = 306\n",
            "[INFO|trainer.py:2912] 2023-02-15 18:40:06,962 >>   Batch size = 8\n",
            "\n",
            "  0% 0/39 [00:00<?, ?it/s]\u001b[A\n",
            "  8% 3/39 [00:00<00:01, 21.36it/s]\u001b[A\n",
            " 15% 6/39 [00:00<00:01, 16.54it/s]\u001b[A\n",
            " 21% 8/39 [00:00<00:01, 15.61it/s]\u001b[A\n",
            " 26% 10/39 [00:00<00:01, 15.05it/s]\u001b[A\n",
            " 31% 12/39 [00:00<00:01, 14.75it/s]\u001b[A\n",
            " 36% 14/39 [00:00<00:01, 14.50it/s]\u001b[A\n",
            " 41% 16/39 [00:01<00:01, 14.36it/s]\u001b[A\n",
            " 46% 18/39 [00:01<00:01, 14.20it/s]\u001b[A\n",
            " 51% 20/39 [00:01<00:01, 14.14it/s]\u001b[A\n",
            " 56% 22/39 [00:01<00:01, 14.10it/s]\u001b[A\n",
            " 62% 24/39 [00:01<00:01, 14.15it/s]\u001b[A\n",
            " 67% 26/39 [00:01<00:00, 14.20it/s]\u001b[A\n",
            " 72% 28/39 [00:01<00:00, 14.26it/s]\u001b[A\n",
            " 77% 30/39 [00:02<00:00, 14.27it/s]\u001b[A\n",
            " 82% 32/39 [00:02<00:00, 14.29it/s]\u001b[A\n",
            " 87% 34/39 [00:02<00:00, 14.39it/s]\u001b[A\n",
            " 92% 36/39 [00:02<00:00, 14.41it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.7422252893447876, 'eval_accuracy': 0.673202633857727, 'eval_runtime': 2.6997, 'eval_samples_per_second': 113.348, 'eval_steps_per_second': 14.446, 'epoch': 1.78}\n",
            " 80% 2000/2500 [09:42<02:09,  3.86it/s]\n",
            "100% 39/39 [00:02<00:00, 14.36it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2656] 2023-02-15 18:40:09,662 >> Saving model checkpoint to out/beer_reviews/gpt2_hidden_v1/checkpoint-2000\n",
            "[INFO|configuration_utils.py:447] 2023-02-15 18:40:09,663 >> Configuration saved in out/beer_reviews/gpt2_hidden_v1/checkpoint-2000/config.json\n",
            "[INFO|modeling_utils.py:1624] 2023-02-15 18:40:11,307 >> Model weights saved in out/beer_reviews/gpt2_hidden_v1/checkpoint-2000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2023-02-15 18:40:11,308 >> tokenizer config file saved in out/beer_reviews/gpt2_hidden_v1/checkpoint-2000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2023-02-15 18:40:11,308 >> Special tokens file saved in out/beer_reviews/gpt2_hidden_v1/checkpoint-2000/special_tokens_map.json\n",
            "[INFO|trainer.py:2734] 2023-02-15 18:40:15,426 >> Deleting older checkpoint [out/beer_reviews/gpt2_hidden_v1/checkpoint-750] due to args.save_total_limit\n",
            "{'loss': 0.6041, 'learning_rate': 3.2000000000000003e-06, 'epoch': 1.87}\n",
            "{'loss': 0.6302, 'learning_rate': 2.4000000000000003e-06, 'epoch': 1.96}\n",
            " 90% 2250/2500 [10:52<01:03,  3.92it/s][INFO|trainer.py:725] 2023-02-15 18:41:20,193 >> The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassificationCustom.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassificationCustom.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2907] 2023-02-15 18:41:20,195 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2023-02-15 18:41:20,195 >>   Num examples = 306\n",
            "[INFO|trainer.py:2912] 2023-02-15 18:41:20,195 >>   Batch size = 8\n",
            "\n",
            "  0% 0/39 [00:00<?, ?it/s]\u001b[A\n",
            "  8% 3/39 [00:00<00:01, 21.55it/s]\u001b[A\n",
            " 15% 6/39 [00:00<00:01, 16.75it/s]\u001b[A\n",
            " 21% 8/39 [00:00<00:01, 15.89it/s]\u001b[A\n",
            " 26% 10/39 [00:00<00:01, 15.28it/s]\u001b[A\n",
            " 31% 12/39 [00:00<00:01, 15.12it/s]\u001b[A\n",
            " 36% 14/39 [00:00<00:01, 14.77it/s]\u001b[A\n",
            " 41% 16/39 [00:01<00:01, 14.68it/s]\u001b[A\n",
            " 46% 18/39 [00:01<00:01, 14.57it/s]\u001b[A\n",
            " 51% 20/39 [00:01<00:01, 14.51it/s]\u001b[A\n",
            " 56% 22/39 [00:01<00:01, 14.50it/s]\u001b[A\n",
            " 62% 24/39 [00:01<00:01, 14.41it/s]\u001b[A\n",
            " 67% 26/39 [00:01<00:00, 14.43it/s]\u001b[A\n",
            " 72% 28/39 [00:01<00:00, 14.38it/s]\u001b[A\n",
            " 77% 30/39 [00:02<00:00, 14.44it/s]\u001b[A\n",
            " 82% 32/39 [00:02<00:00, 14.38it/s]\u001b[A\n",
            " 87% 34/39 [00:02<00:00, 14.35it/s]\u001b[A\n",
            " 92% 36/39 [00:02<00:00, 14.37it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.7652809023857117, 'eval_accuracy': 0.6437908411026001, 'eval_runtime': 2.669, 'eval_samples_per_second': 114.651, 'eval_steps_per_second': 14.612, 'epoch': 2.0}\n",
            " 90% 2250/2500 [10:55<01:03,  3.92it/s]\n",
            "100% 39/39 [00:02<00:00, 14.41it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2656] 2023-02-15 18:41:22,865 >> Saving model checkpoint to out/beer_reviews/gpt2_hidden_v1/checkpoint-2250\n",
            "[INFO|configuration_utils.py:447] 2023-02-15 18:41:22,866 >> Configuration saved in out/beer_reviews/gpt2_hidden_v1/checkpoint-2250/config.json\n",
            "[INFO|modeling_utils.py:1624] 2023-02-15 18:41:24,497 >> Model weights saved in out/beer_reviews/gpt2_hidden_v1/checkpoint-2250/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2023-02-15 18:41:24,497 >> tokenizer config file saved in out/beer_reviews/gpt2_hidden_v1/checkpoint-2250/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2023-02-15 18:41:24,498 >> Special tokens file saved in out/beer_reviews/gpt2_hidden_v1/checkpoint-2250/special_tokens_map.json\n",
            "[INFO|trainer.py:2734] 2023-02-15 18:41:28,712 >> Deleting older checkpoint [out/beer_reviews/gpt2_hidden_v1/checkpoint-1000] due to args.save_total_limit\n",
            "{'loss': 0.563, 'learning_rate': 1.6000000000000001e-06, 'epoch': 2.04}\n",
            "{'loss': 0.5956, 'learning_rate': 8.000000000000001e-07, 'epoch': 2.13}\n",
            "{'loss': 0.5639, 'learning_rate': 0.0, 'epoch': 2.22}\n",
            "100% 2500/2500 [12:06<00:00,  3.85it/s][INFO|trainer.py:725] 2023-02-15 18:42:34,237 >> The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassificationCustom.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassificationCustom.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2907] 2023-02-15 18:42:34,239 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2023-02-15 18:42:34,240 >>   Num examples = 306\n",
            "[INFO|trainer.py:2912] 2023-02-15 18:42:34,240 >>   Batch size = 8\n",
            "\n",
            "  0% 0/39 [00:00<?, ?it/s]\u001b[A\n",
            "  8% 3/39 [00:00<00:01, 21.01it/s]\u001b[A\n",
            " 15% 6/39 [00:00<00:01, 16.56it/s]\u001b[A\n",
            " 21% 8/39 [00:00<00:01, 15.82it/s]\u001b[A\n",
            " 26% 10/39 [00:00<00:01, 14.99it/s]\u001b[A\n",
            " 31% 12/39 [00:00<00:01, 14.79it/s]\u001b[A\n",
            " 36% 14/39 [00:00<00:01, 14.42it/s]\u001b[A\n",
            " 41% 16/39 [00:01<00:01, 14.41it/s]\u001b[A\n",
            " 46% 18/39 [00:01<00:01, 14.40it/s]\u001b[A\n",
            " 51% 20/39 [00:01<00:01, 14.32it/s]\u001b[A\n",
            " 56% 22/39 [00:01<00:01, 14.34it/s]\u001b[A\n",
            " 62% 24/39 [00:01<00:01, 14.12it/s]\u001b[A\n",
            " 67% 26/39 [00:01<00:00, 14.18it/s]\u001b[A\n",
            " 72% 28/39 [00:01<00:00, 14.11it/s]\u001b[A\n",
            " 77% 30/39 [00:02<00:00, 14.27it/s]\u001b[A\n",
            " 82% 32/39 [00:02<00:00, 14.33it/s]\u001b[A\n",
            " 87% 34/39 [00:02<00:00, 14.36it/s]\u001b[A\n",
            " 92% 36/39 [00:02<00:00, 14.28it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.760033130645752, 'eval_accuracy': 0.673202633857727, 'eval_runtime': 2.6895, 'eval_samples_per_second': 113.774, 'eval_steps_per_second': 14.501, 'epoch': 2.22}\n",
            "100% 2500/2500 [12:09<00:00,  3.85it/s]\n",
            "100% 39/39 [00:02<00:00, 14.36it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2656] 2023-02-15 18:42:36,930 >> Saving model checkpoint to out/beer_reviews/gpt2_hidden_v1/checkpoint-2500\n",
            "[INFO|configuration_utils.py:447] 2023-02-15 18:42:36,931 >> Configuration saved in out/beer_reviews/gpt2_hidden_v1/checkpoint-2500/config.json\n",
            "[INFO|modeling_utils.py:1624] 2023-02-15 18:42:38,558 >> Model weights saved in out/beer_reviews/gpt2_hidden_v1/checkpoint-2500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2023-02-15 18:42:38,559 >> tokenizer config file saved in out/beer_reviews/gpt2_hidden_v1/checkpoint-2500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2023-02-15 18:42:38,559 >> Special tokens file saved in out/beer_reviews/gpt2_hidden_v1/checkpoint-2500/special_tokens_map.json\n",
            "[INFO|trainer.py:2734] 2023-02-15 18:42:42,698 >> Deleting older checkpoint [out/beer_reviews/gpt2_hidden_v1/checkpoint-1250] due to args.save_total_limit\n",
            "[INFO|trainer.py:1852] 2023-02-15 18:42:42,724 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[INFO|trainer.py:1946] 2023-02-15 18:42:42,724 >> Loading best model from out/beer_reviews/gpt2_hidden_v1/checkpoint-2000 (score: 0.673202633857727).\n",
            "{'train_runtime': 738.3007, 'train_samples_per_second': 27.089, 'train_steps_per_second': 3.386, 'train_loss': 0.6789592529296875, 'epoch': 2.22}\n",
            "100% 2500/2500 [12:18<00:00,  3.39it/s]\n",
            "[INFO|trainer.py:2656] 2023-02-15 18:42:45,572 >> Saving model checkpoint to out/beer_reviews/gpt2_hidden_v1\n",
            "[INFO|configuration_utils.py:447] 2023-02-15 18:42:45,573 >> Configuration saved in out/beer_reviews/gpt2_hidden_v1/config.json\n",
            "[INFO|modeling_utils.py:1624] 2023-02-15 18:42:47,294 >> Model weights saved in out/beer_reviews/gpt2_hidden_v1/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2023-02-15 18:42:47,295 >> tokenizer config file saved in out/beer_reviews/gpt2_hidden_v1/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2023-02-15 18:42:47,295 >> Special tokens file saved in out/beer_reviews/gpt2_hidden_v1/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       2.22\n",
            "  train_loss               =      0.679\n",
            "  train_runtime            = 0:12:18.30\n",
            "  train_samples            =       9000\n",
            "  train_samples_per_second =     27.089\n",
            "  train_steps_per_second   =      3.386\n",
            "INFO:__main__:*** Evaluate ***\n",
            "[INFO|trainer.py:725] 2023-02-15 18:42:47,510 >> The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassificationCustom.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassificationCustom.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2907] 2023-02-15 18:42:47,512 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2023-02-15 18:42:47,513 >>   Num examples = 306\n",
            "[INFO|trainer.py:2912] 2023-02-15 18:42:47,513 >>   Batch size = 8\n",
            "100% 39/39 [00:02<00:00, 15.03it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       2.22\n",
            "  eval_accuracy           =     0.6732\n",
            "  eval_loss               =     0.7422\n",
            "  eval_runtime            = 0:00:02.69\n",
            "  eval_samples            =        306\n",
            "  eval_samples_per_second =    113.468\n",
            "  eval_steps_per_second   =     14.462\n",
            "INFO:__main__:*** Predict ***\n",
            "[INFO|trainer.py:725] 2023-02-15 18:42:50,213 >> The following columns in the test set don't have a corresponding argument in `GPT2ForSequenceClassificationCustom.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassificationCustom.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2907] 2023-02-15 18:42:50,214 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:2909] 2023-02-15 18:42:50,214 >>   Num examples = 841\n",
            "[INFO|trainer.py:2912] 2023-02-15 18:42:50,214 >>   Batch size = 8\n",
            "100% 106/106 [00:07<00:00, 14.99it/s]\n",
            "INFO:__main__:***** Predict results None *****\n",
            "[INFO|modelcard.py:444] 2023-02-15 18:42:57,540 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.673202633857727}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Score : 0.673"
      ],
      "metadata": {
        "id": "-05WTB9LlUY1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python run_glue.py \\\n",
        "  --cache_dir .cache_training \\\n",
        "  --model_name_or_path 'out/beer_reviews/gpt2_hidden_v1' \\\n",
        "  --train_file data/production.json  \\\n",
        "  --validation_file data/production.json \\\n",
        "  --per_device_eval_batch_size 24 \\\n",
        "  --do_eval \\\n",
        "  --max_seq_length 128 \\\n",
        "  --ignore_mismatched_sizes=True\\\n",
        "  --output_dir 'out/beer_reviews/gpt2-evaluation'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bneOF5NbreG_",
        "outputId": "b99635e0-c9a7-42f9-9bbb-289f989c46e7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-15 18:43:02.664436: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-02-15 18:43:04.304120: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-15 18:43:04.304311: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-15 18:43:04.304335: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=passive,\n",
            "log_on_each_node=True,\n",
            "logging_dir=out/beer_reviews/gpt2-evaluation/runs/Feb15_18-43-07_ed170e621d72,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=out/beer_reviews/gpt2-evaluation,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=24,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=out/beer_reviews/gpt2-evaluation,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "INFO:__main__:load a local file for train: data/production.json\n",
            "INFO:__main__:load a local file for validation: data/production.json\n",
            "WARNING:datasets.builder:Using custom data configuration default-d5635c3fd9639ba6\n",
            "INFO:datasets.info:Loading Dataset Infos from /usr/local/lib/python3.8/dist-packages/datasets/packaged_modules/json\n",
            "INFO:datasets.builder:Overwrite dataset info from restored data version.\n",
            "INFO:datasets.info:Loading Dataset info from .cache_training/json/default-d5635c3fd9639ba6/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51\n",
            "WARNING:datasets.builder:Found cached dataset json (/content/uczenie_glebokie/.cache_training/json/default-d5635c3fd9639ba6/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n",
            "INFO:datasets.info:Loading Dataset info from /content/uczenie_glebokie/.cache_training/json/default-d5635c3fd9639ba6/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51\n",
            "100% 2/2 [00:00<00:00, 70.57it/s]\n",
            "[INFO|configuration_utils.py:651] 2023-02-15 18:43:08,095 >> loading configuration file out/beer_reviews/gpt2_hidden_v1/config.json\n",
            "[INFO|configuration_utils.py:705] 2023-02-15 18:43:08,096 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"out/beer_reviews/gpt2_hidden_v1\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2ForSequenceClassificationCustom\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"id2label\": {\n",
            "    \"0\": 0,\n",
            "    \"1\": 1,\n",
            "    \"2\": 2\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"0\": 0,\n",
            "    \"1\": 1,\n",
            "    \"2\": 2\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"pad_token_id\": 50256,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.23.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_hidden_states\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1771] 2023-02-15 18:43:08,113 >> loading file vocab.json\n",
            "[INFO|tokenization_utils_base.py:1771] 2023-02-15 18:43:08,113 >> loading file merges.txt\n",
            "[INFO|tokenization_utils_base.py:1771] 2023-02-15 18:43:08,113 >> loading file tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1771] 2023-02-15 18:43:08,113 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:1771] 2023-02-15 18:43:08,113 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:1771] 2023-02-15 18:43:08,113 >> loading file tokenizer_config.json\n",
            "INFO:__main__:Using implementation from class: AutoModelForSequenceClassification\n",
            "[INFO|modeling_utils.py:2153] 2023-02-15 18:43:08,248 >> loading weights file out/beer_reviews/gpt2_hidden_v1/pytorch_model.bin\n",
            "[WARNING|modeling_utils.py:2596] 2023-02-15 18:43:09,645 >> Some weights of the model checkpoint at out/beer_reviews/gpt2_hidden_v1 were not used when initializing GPT2ForSequenceClassification: ['score.dense_1_input.weight', 'score.dense_1_input.bias', 'score.dense_1_hidden.bias', 'score.dense_1_hidden.weight', 'score.dense_2.bias', 'score.out_proj.weight', 'score.dense_2.weight']\n",
            "- This IS expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:2608] 2023-02-15 18:43:09,645 >> Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at out/beer_reviews/gpt2_hidden_v1 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset:   0% 0/1 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /content/uczenie_glebokie/.cache_training/json/default-d5635c3fd9639ba6/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-fedfe4e420f090a4.arrow\n",
            "Running tokenizer on dataset: 100% 1/1 [00:00<00:00,  3.20ba/s]\n",
            "Running tokenizer on dataset:   0% 0/1 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /content/uczenie_glebokie/.cache_training/json/default-d5635c3fd9639ba6/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-e3d6c802fbc0e109.arrow\n",
            "Running tokenizer on dataset: 100% 1/1 [00:00<00:00,  3.33ba/s]\n",
            "INFO:__main__:*** Evaluate ***\n",
            "[INFO|trainer.py:725] 2023-02-15 18:43:13,263 >> The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2907] 2023-02-15 18:43:13,265 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2023-02-15 18:43:13,265 >>   Num examples = 841\n",
            "[INFO|trainer.py:2912] 2023-02-15 18:43:13,265 >>   Batch size = 24\n",
            "100% 36/36 [00:06<00:00,  5.30it/s]\n",
            "***** eval metrics *****\n",
            "  eval_accuracy           =     0.4673\n",
            "  eval_loss               =     1.2595\n",
            "  eval_runtime            = 0:00:07.91\n",
            "  eval_samples            =        841\n",
            "  eval_samples_per_second =     106.31\n",
            "  eval_steps_per_second   =      4.551\n",
            "[INFO|modelcard.py:444] 2023-02-15 18:43:21,178 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Text Classification', 'type': 'text-classification'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Freeze encoder weights - T5-base**"
      ],
      "metadata": {
        "id": "9AzFSp40lZd8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python run_translation.py \\\n",
        "  --cache_dir .cache_training \\\n",
        "  --model_name_or_path \"google/t5-v1_1-base\" \\\n",
        "  --train_file data/s2s-training.json \\\n",
        "  --validation_file data/s2s-validation.json \\\n",
        "  --test_file data/s2s-production.json \\\n",
        "  --per_device_train_batch_size 8 \\\n",
        "  --per_device_eval_batch_size 8 \\\n",
        "  --source_lang \"text\" \\\n",
        "  --target_lang \"label\" \\\n",
        "  --source_prefix \"beer classification\" \\\n",
        "  --max_source_length 256 \\\n",
        "  --max_target_length 128 \\\n",
        "  --generation_max_length 128 \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --do_predict \\\n",
        "  --predict_with_generate \\\n",
        "  --max_eval_samples 2000 \\\n",
        "  --num_train_epochs 5 \\\n",
        "  --save_strategy steps \\\n",
        "  --save_steps 250 \\\n",
        "  --save_total_limit 5 \\\n",
        "  --logging_strategy steps \\\n",
        "  --logging_steps 100 \\\n",
        "  --eval_steps 250 \\\n",
        "  --evaluation_strategy steps \\\n",
        "  --metric_for_best_model accuracy \\\n",
        "  --greater_is_better True \\\n",
        "  --load_best_model_at_end True \\\n",
        "  --output_dir out/beer_reviews/t5_v1_1_freeze_base"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRddaiUyhBqS",
        "outputId": "c50b52fa-70f1-450d-ab61-85821d6cf3a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-14 13:29:44.592712: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-02-14 13:29:46.403462: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-14 13:29:46.403641: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-14 13:29:46.403665: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "INFO:__main__:Training/evaluation parameters Seq2SeqTrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=250,\n",
            "evaluation_strategy=steps,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "generation_max_length=128,\n",
            "generation_num_beams=None,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=passive,\n",
            "log_on_each_node=True,\n",
            "logging_dir=out/beer_reviews/t5_v1_1_freeze_base/runs/Feb14_13-29-49_0af4fb0df49d,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=100,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=accuracy,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=5.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=out/beer_reviews/t5_v1_1_freeze_base,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "predict_with_generate=True,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=out/beer_reviews/t5_v1_1_freeze_base,\n",
            "save_on_each_node=False,\n",
            "save_steps=250,\n",
            "save_strategy=steps,\n",
            "save_total_limit=5,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "WARNING:datasets.builder:Using custom data configuration default-baeafae2fbdfeb25\n",
            "INFO:datasets.info:Loading Dataset Infos from /usr/local/lib/python3.8/dist-packages/datasets/packaged_modules/json\n",
            "INFO:datasets.builder:Generating dataset json (/content/uczenie_glebokie/.cache_training/json/default-baeafae2fbdfeb25/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n",
            "Downloading and preparing dataset json/default to /content/uczenie_glebokie/.cache_training/json/default-baeafae2fbdfeb25/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51...\n",
            "Downloading data files: 100% 3/3 [00:00<00:00, 14169.95it/s]\n",
            "INFO:datasets.download.download_manager:Downloading took 0.0 min\n",
            "INFO:datasets.download.download_manager:Checksum Computation took 0.0 min\n",
            "Extracting data files: 100% 3/3 [00:00<00:00, 2135.96it/s]\n",
            "INFO:datasets.utils.info_utils:Unable to verify checksums.\n",
            "INFO:datasets.builder:Generating train split\n",
            "INFO:datasets.builder:Generating validation split\n",
            "INFO:datasets.builder:Generating test split\n",
            "INFO:datasets.utils.info_utils:Unable to verify splits sizes.\n",
            "Dataset json downloaded and prepared to /content/uczenie_glebokie/.cache_training/json/default-baeafae2fbdfeb25/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51. Subsequent calls will reuse this data.\n",
            "100% 3/3 [00:00<00:00, 936.86it/s]\n",
            "Downloading (…)lve/main/config.json: 100% 605/605 [00:00<00:00, 86.5kB/s]\n",
            "[INFO|configuration_utils.py:653] 2023-02-14 13:29:51,184 >> loading configuration file config.json from cache at .cache_training/models--google--t5-v1_1-base/snapshots/b5fc947a416ea3cb079532cb3c2bbadeb7f800fc/config.json\n",
            "[INFO|configuration_utils.py:705] 2023-02-14 13:29:51,192 >> Model config T5Config {\n",
            "  \"_name_or_path\": \"google/t5-v1_1-base\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"transformers_version\": \"4.23.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "Downloading (…)okenizer_config.json: 100% 1.86k/1.86k [00:00<00:00, 688kB/s]\n",
            "[INFO|configuration_utils.py:653] 2023-02-14 13:29:51,460 >> loading configuration file config.json from cache at .cache_training/models--google--t5-v1_1-base/snapshots/b5fc947a416ea3cb079532cb3c2bbadeb7f800fc/config.json\n",
            "[INFO|configuration_utils.py:705] 2023-02-14 13:29:51,461 >> Model config T5Config {\n",
            "  \"_name_or_path\": \"google/t5-v1_1-base\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"transformers_version\": \"4.23.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "Downloading (…)ve/main/spiece.model: 100% 792k/792k [00:00<00:00, 5.29MB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100% 1.79k/1.79k [00:00<00:00, 761kB/s]\n",
            "[INFO|tokenization_utils_base.py:1773] 2023-02-14 13:29:52,408 >> loading file spiece.model from cache at .cache_training/models--google--t5-v1_1-base/snapshots/b5fc947a416ea3cb079532cb3c2bbadeb7f800fc/spiece.model\n",
            "[INFO|tokenization_utils_base.py:1773] 2023-02-14 13:29:52,408 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1773] 2023-02-14 13:29:52,408 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1773] 2023-02-14 13:29:52,408 >> loading file special_tokens_map.json from cache at .cache_training/models--google--t5-v1_1-base/snapshots/b5fc947a416ea3cb079532cb3c2bbadeb7f800fc/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:1773] 2023-02-14 13:29:52,408 >> loading file tokenizer_config.json from cache at .cache_training/models--google--t5-v1_1-base/snapshots/b5fc947a416ea3cb079532cb3c2bbadeb7f800fc/tokenizer_config.json\n",
            "[INFO|configuration_utils.py:653] 2023-02-14 13:29:52,409 >> loading configuration file config.json from cache at .cache_training/models--google--t5-v1_1-base/snapshots/b5fc947a416ea3cb079532cb3c2bbadeb7f800fc/config.json\n",
            "[INFO|configuration_utils.py:705] 2023-02-14 13:29:52,409 >> Model config T5Config {\n",
            "  \"_name_or_path\": \"google/t5-v1_1-base\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"transformers_version\": \"4.23.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:653] 2023-02-14 13:29:52,457 >> loading configuration file config.json from cache at .cache_training/models--google--t5-v1_1-base/snapshots/b5fc947a416ea3cb079532cb3c2bbadeb7f800fc/config.json\n",
            "[INFO|configuration_utils.py:705] 2023-02-14 13:29:52,458 >> Model config T5Config {\n",
            "  \"_name_or_path\": \"google/t5-v1_1-base\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"transformers_version\": \"4.23.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "Downloading (…)\"pytorch_model.bin\";: 100% 990M/990M [00:11<00:00, 88.8MB/s]\n",
            "[INFO|modeling_utils.py:2156] 2023-02-14 13:30:03,970 >> loading weights file pytorch_model.bin from cache at .cache_training/models--google--t5-v1_1-base/snapshots/b5fc947a416ea3cb079532cb3c2bbadeb7f800fc/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:2606] 2023-02-14 13:30:07,536 >> All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
            "\n",
            "[INFO|modeling_utils.py:2614] 2023-02-14 13:30:07,536 >> All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at google/t5-v1_1-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
            "\n",
            "\n",
            "\n",
            " frozen Layer\n",
            "[('shared.weight', True), ('encoder.block.0.layer.0.SelfAttention.q.weight', False), ('encoder.block.0.layer.0.SelfAttention.k.weight', False), ('encoder.block.0.layer.0.SelfAttention.v.weight', False), ('encoder.block.0.layer.0.SelfAttention.o.weight', False), ('encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', False), ('encoder.block.0.layer.0.layer_norm.weight', False), ('encoder.block.0.layer.1.DenseReluDense.wi_0.weight', False), ('encoder.block.0.layer.1.DenseReluDense.wi_1.weight', False), ('encoder.block.0.layer.1.DenseReluDense.wo.weight', False), ('encoder.block.0.layer.1.layer_norm.weight', False), ('encoder.block.1.layer.0.SelfAttention.q.weight', True), ('encoder.block.1.layer.0.SelfAttention.k.weight', True), ('encoder.block.1.layer.0.SelfAttention.v.weight', True), ('encoder.block.1.layer.0.SelfAttention.o.weight', True), ('encoder.block.1.layer.0.layer_norm.weight', True), ('encoder.block.1.layer.1.DenseReluDense.wi_0.weight', True), ('encoder.block.1.layer.1.DenseReluDense.wi_1.weight', True), ('encoder.block.1.layer.1.DenseReluDense.wo.weight', True), ('encoder.block.1.layer.1.layer_norm.weight', True), ('encoder.block.2.layer.0.SelfAttention.q.weight', True), ('encoder.block.2.layer.0.SelfAttention.k.weight', True), ('encoder.block.2.layer.0.SelfAttention.v.weight', True), ('encoder.block.2.layer.0.SelfAttention.o.weight', True), ('encoder.block.2.layer.0.layer_norm.weight', True), ('encoder.block.2.layer.1.DenseReluDense.wi_0.weight', True), ('encoder.block.2.layer.1.DenseReluDense.wi_1.weight', True), ('encoder.block.2.layer.1.DenseReluDense.wo.weight', True), ('encoder.block.2.layer.1.layer_norm.weight', True), ('encoder.block.3.layer.0.SelfAttention.q.weight', True), ('encoder.block.3.layer.0.SelfAttention.k.weight', True), ('encoder.block.3.layer.0.SelfAttention.v.weight', True), ('encoder.block.3.layer.0.SelfAttention.o.weight', True), ('encoder.block.3.layer.0.layer_norm.weight', True), ('encoder.block.3.layer.1.DenseReluDense.wi_0.weight', True), ('encoder.block.3.layer.1.DenseReluDense.wi_1.weight', True), ('encoder.block.3.layer.1.DenseReluDense.wo.weight', True), ('encoder.block.3.layer.1.layer_norm.weight', True), ('encoder.block.4.layer.0.SelfAttention.q.weight', True), ('encoder.block.4.layer.0.SelfAttention.k.weight', True), ('encoder.block.4.layer.0.SelfAttention.v.weight', True), ('encoder.block.4.layer.0.SelfAttention.o.weight', True), ('encoder.block.4.layer.0.layer_norm.weight', True), ('encoder.block.4.layer.1.DenseReluDense.wi_0.weight', True), ('encoder.block.4.layer.1.DenseReluDense.wi_1.weight', True), ('encoder.block.4.layer.1.DenseReluDense.wo.weight', True), ('encoder.block.4.layer.1.layer_norm.weight', True), ('encoder.block.5.layer.0.SelfAttention.q.weight', False), ('encoder.block.5.layer.0.SelfAttention.k.weight', False), ('encoder.block.5.layer.0.SelfAttention.v.weight', False), ('encoder.block.5.layer.0.SelfAttention.o.weight', False), ('encoder.block.5.layer.0.layer_norm.weight', False), ('encoder.block.5.layer.1.DenseReluDense.wi_0.weight', False), ('encoder.block.5.layer.1.DenseReluDense.wi_1.weight', False), ('encoder.block.5.layer.1.DenseReluDense.wo.weight', False), ('encoder.block.5.layer.1.layer_norm.weight', False), ('encoder.block.6.layer.0.SelfAttention.q.weight', True), ('encoder.block.6.layer.0.SelfAttention.k.weight', True), ('encoder.block.6.layer.0.SelfAttention.v.weight', True), ('encoder.block.6.layer.0.SelfAttention.o.weight', True), ('encoder.block.6.layer.0.layer_norm.weight', True), ('encoder.block.6.layer.1.DenseReluDense.wi_0.weight', True), ('encoder.block.6.layer.1.DenseReluDense.wi_1.weight', True), ('encoder.block.6.layer.1.DenseReluDense.wo.weight', True), ('encoder.block.6.layer.1.layer_norm.weight', True), ('encoder.block.7.layer.0.SelfAttention.q.weight', True), ('encoder.block.7.layer.0.SelfAttention.k.weight', True), ('encoder.block.7.layer.0.SelfAttention.v.weight', True), ('encoder.block.7.layer.0.SelfAttention.o.weight', True), ('encoder.block.7.layer.0.layer_norm.weight', True), ('encoder.block.7.layer.1.DenseReluDense.wi_0.weight', True), ('encoder.block.7.layer.1.DenseReluDense.wi_1.weight', True), ('encoder.block.7.layer.1.DenseReluDense.wo.weight', True), ('encoder.block.7.layer.1.layer_norm.weight', True), ('encoder.block.8.layer.0.SelfAttention.q.weight', True), ('encoder.block.8.layer.0.SelfAttention.k.weight', True), ('encoder.block.8.layer.0.SelfAttention.v.weight', True), ('encoder.block.8.layer.0.SelfAttention.o.weight', True), ('encoder.block.8.layer.0.layer_norm.weight', True), ('encoder.block.8.layer.1.DenseReluDense.wi_0.weight', True), ('encoder.block.8.layer.1.DenseReluDense.wi_1.weight', True), ('encoder.block.8.layer.1.DenseReluDense.wo.weight', True), ('encoder.block.8.layer.1.layer_norm.weight', True), ('encoder.block.9.layer.0.SelfAttention.q.weight', True), ('encoder.block.9.layer.0.SelfAttention.k.weight', True), ('encoder.block.9.layer.0.SelfAttention.v.weight', True), ('encoder.block.9.layer.0.SelfAttention.o.weight', True), ('encoder.block.9.layer.0.layer_norm.weight', True), ('encoder.block.9.layer.1.DenseReluDense.wi_0.weight', True), ('encoder.block.9.layer.1.DenseReluDense.wi_1.weight', True), ('encoder.block.9.layer.1.DenseReluDense.wo.weight', True), ('encoder.block.9.layer.1.layer_norm.weight', True), ('encoder.block.10.layer.0.SelfAttention.q.weight', False), ('encoder.block.10.layer.0.SelfAttention.k.weight', False), ('encoder.block.10.layer.0.SelfAttention.v.weight', False), ('encoder.block.10.layer.0.SelfAttention.o.weight', False), ('encoder.block.10.layer.0.layer_norm.weight', False), ('encoder.block.10.layer.1.DenseReluDense.wi_0.weight', False), ('encoder.block.10.layer.1.DenseReluDense.wi_1.weight', False), ('encoder.block.10.layer.1.DenseReluDense.wo.weight', False), ('encoder.block.10.layer.1.layer_norm.weight', False), ('encoder.block.11.layer.0.SelfAttention.q.weight', True), ('encoder.block.11.layer.0.SelfAttention.k.weight', True), ('encoder.block.11.layer.0.SelfAttention.v.weight', True), ('encoder.block.11.layer.0.SelfAttention.o.weight', True), ('encoder.block.11.layer.0.layer_norm.weight', True), ('encoder.block.11.layer.1.DenseReluDense.wi_0.weight', True), ('encoder.block.11.layer.1.DenseReluDense.wi_1.weight', True), ('encoder.block.11.layer.1.DenseReluDense.wo.weight', True), ('encoder.block.11.layer.1.layer_norm.weight', True), ('encoder.final_layer_norm.weight', True), ('decoder.block.0.layer.0.SelfAttention.q.weight', False), ('decoder.block.0.layer.0.SelfAttention.k.weight', False), ('decoder.block.0.layer.0.SelfAttention.v.weight', False), ('decoder.block.0.layer.0.SelfAttention.o.weight', False), ('decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', False), ('decoder.block.0.layer.0.layer_norm.weight', False), ('decoder.block.0.layer.1.EncDecAttention.q.weight', False), ('decoder.block.0.layer.1.EncDecAttention.k.weight', False), ('decoder.block.0.layer.1.EncDecAttention.v.weight', False), ('decoder.block.0.layer.1.EncDecAttention.o.weight', False), ('decoder.block.0.layer.1.layer_norm.weight', False), ('decoder.block.0.layer.2.DenseReluDense.wi_0.weight', False), ('decoder.block.0.layer.2.DenseReluDense.wi_1.weight', False), ('decoder.block.0.layer.2.DenseReluDense.wo.weight', False), ('decoder.block.0.layer.2.layer_norm.weight', False), ('decoder.block.1.layer.0.SelfAttention.q.weight', True), ('decoder.block.1.layer.0.SelfAttention.k.weight', True), ('decoder.block.1.layer.0.SelfAttention.v.weight', True), ('decoder.block.1.layer.0.SelfAttention.o.weight', True), ('decoder.block.1.layer.0.layer_norm.weight', True), ('decoder.block.1.layer.1.EncDecAttention.q.weight', True), ('decoder.block.1.layer.1.EncDecAttention.k.weight', True), ('decoder.block.1.layer.1.EncDecAttention.v.weight', True), ('decoder.block.1.layer.1.EncDecAttention.o.weight', True), ('decoder.block.1.layer.1.layer_norm.weight', True), ('decoder.block.1.layer.2.DenseReluDense.wi_0.weight', True), ('decoder.block.1.layer.2.DenseReluDense.wi_1.weight', True), ('decoder.block.1.layer.2.DenseReluDense.wo.weight', True), ('decoder.block.1.layer.2.layer_norm.weight', True), ('decoder.block.2.layer.0.SelfAttention.q.weight', True), ('decoder.block.2.layer.0.SelfAttention.k.weight', True), ('decoder.block.2.layer.0.SelfAttention.v.weight', True), ('decoder.block.2.layer.0.SelfAttention.o.weight', True), ('decoder.block.2.layer.0.layer_norm.weight', True), ('decoder.block.2.layer.1.EncDecAttention.q.weight', True), ('decoder.block.2.layer.1.EncDecAttention.k.weight', True), ('decoder.block.2.layer.1.EncDecAttention.v.weight', True), ('decoder.block.2.layer.1.EncDecAttention.o.weight', True), ('decoder.block.2.layer.1.layer_norm.weight', True), ('decoder.block.2.layer.2.DenseReluDense.wi_0.weight', True), ('decoder.block.2.layer.2.DenseReluDense.wi_1.weight', True), ('decoder.block.2.layer.2.DenseReluDense.wo.weight', True), ('decoder.block.2.layer.2.layer_norm.weight', True), ('decoder.block.3.layer.0.SelfAttention.q.weight', True), ('decoder.block.3.layer.0.SelfAttention.k.weight', True), ('decoder.block.3.layer.0.SelfAttention.v.weight', True), ('decoder.block.3.layer.0.SelfAttention.o.weight', True), ('decoder.block.3.layer.0.layer_norm.weight', True), ('decoder.block.3.layer.1.EncDecAttention.q.weight', True), ('decoder.block.3.layer.1.EncDecAttention.k.weight', True), ('decoder.block.3.layer.1.EncDecAttention.v.weight', True), ('decoder.block.3.layer.1.EncDecAttention.o.weight', True), ('decoder.block.3.layer.1.layer_norm.weight', True), ('decoder.block.3.layer.2.DenseReluDense.wi_0.weight', True), ('decoder.block.3.layer.2.DenseReluDense.wi_1.weight', True), ('decoder.block.3.layer.2.DenseReluDense.wo.weight', True), ('decoder.block.3.layer.2.layer_norm.weight', True), ('decoder.block.4.layer.0.SelfAttention.q.weight', True), ('decoder.block.4.layer.0.SelfAttention.k.weight', True), ('decoder.block.4.layer.0.SelfAttention.v.weight', True), ('decoder.block.4.layer.0.SelfAttention.o.weight', True), ('decoder.block.4.layer.0.layer_norm.weight', True), ('decoder.block.4.layer.1.EncDecAttention.q.weight', True), ('decoder.block.4.layer.1.EncDecAttention.k.weight', True), ('decoder.block.4.layer.1.EncDecAttention.v.weight', True), ('decoder.block.4.layer.1.EncDecAttention.o.weight', True), ('decoder.block.4.layer.1.layer_norm.weight', True), ('decoder.block.4.layer.2.DenseReluDense.wi_0.weight', True), ('decoder.block.4.layer.2.DenseReluDense.wi_1.weight', True), ('decoder.block.4.layer.2.DenseReluDense.wo.weight', True), ('decoder.block.4.layer.2.layer_norm.weight', True), ('decoder.block.5.layer.0.SelfAttention.q.weight', False), ('decoder.block.5.layer.0.SelfAttention.k.weight', False), ('decoder.block.5.layer.0.SelfAttention.v.weight', False), ('decoder.block.5.layer.0.SelfAttention.o.weight', False), ('decoder.block.5.layer.0.layer_norm.weight', False), ('decoder.block.5.layer.1.EncDecAttention.q.weight', False), ('decoder.block.5.layer.1.EncDecAttention.k.weight', False), ('decoder.block.5.layer.1.EncDecAttention.v.weight', False), ('decoder.block.5.layer.1.EncDecAttention.o.weight', False), ('decoder.block.5.layer.1.layer_norm.weight', False), ('decoder.block.5.layer.2.DenseReluDense.wi_0.weight', False), ('decoder.block.5.layer.2.DenseReluDense.wi_1.weight', False), ('decoder.block.5.layer.2.DenseReluDense.wo.weight', False), ('decoder.block.5.layer.2.layer_norm.weight', False), ('decoder.block.6.layer.0.SelfAttention.q.weight', True), ('decoder.block.6.layer.0.SelfAttention.k.weight', True), ('decoder.block.6.layer.0.SelfAttention.v.weight', True), ('decoder.block.6.layer.0.SelfAttention.o.weight', True), ('decoder.block.6.layer.0.layer_norm.weight', True), ('decoder.block.6.layer.1.EncDecAttention.q.weight', True), ('decoder.block.6.layer.1.EncDecAttention.k.weight', True), ('decoder.block.6.layer.1.EncDecAttention.v.weight', True), ('decoder.block.6.layer.1.EncDecAttention.o.weight', True), ('decoder.block.6.layer.1.layer_norm.weight', True), ('decoder.block.6.layer.2.DenseReluDense.wi_0.weight', True), ('decoder.block.6.layer.2.DenseReluDense.wi_1.weight', True), ('decoder.block.6.layer.2.DenseReluDense.wo.weight', True), ('decoder.block.6.layer.2.layer_norm.weight', True), ('decoder.block.7.layer.0.SelfAttention.q.weight', True), ('decoder.block.7.layer.0.SelfAttention.k.weight', True), ('decoder.block.7.layer.0.SelfAttention.v.weight', True), ('decoder.block.7.layer.0.SelfAttention.o.weight', True), ('decoder.block.7.layer.0.layer_norm.weight', True), ('decoder.block.7.layer.1.EncDecAttention.q.weight', True), ('decoder.block.7.layer.1.EncDecAttention.k.weight', True), ('decoder.block.7.layer.1.EncDecAttention.v.weight', True), ('decoder.block.7.layer.1.EncDecAttention.o.weight', True), ('decoder.block.7.layer.1.layer_norm.weight', True), ('decoder.block.7.layer.2.DenseReluDense.wi_0.weight', True), ('decoder.block.7.layer.2.DenseReluDense.wi_1.weight', True), ('decoder.block.7.layer.2.DenseReluDense.wo.weight', True), ('decoder.block.7.layer.2.layer_norm.weight', True), ('decoder.block.8.layer.0.SelfAttention.q.weight', True), ('decoder.block.8.layer.0.SelfAttention.k.weight', True), ('decoder.block.8.layer.0.SelfAttention.v.weight', True), ('decoder.block.8.layer.0.SelfAttention.o.weight', True), ('decoder.block.8.layer.0.layer_norm.weight', True), ('decoder.block.8.layer.1.EncDecAttention.q.weight', True), ('decoder.block.8.layer.1.EncDecAttention.k.weight', True), ('decoder.block.8.layer.1.EncDecAttention.v.weight', True), ('decoder.block.8.layer.1.EncDecAttention.o.weight', True), ('decoder.block.8.layer.1.layer_norm.weight', True), ('decoder.block.8.layer.2.DenseReluDense.wi_0.weight', True), ('decoder.block.8.layer.2.DenseReluDense.wi_1.weight', True), ('decoder.block.8.layer.2.DenseReluDense.wo.weight', True), ('decoder.block.8.layer.2.layer_norm.weight', True), ('decoder.block.9.layer.0.SelfAttention.q.weight', True), ('decoder.block.9.layer.0.SelfAttention.k.weight', True), ('decoder.block.9.layer.0.SelfAttention.v.weight', True), ('decoder.block.9.layer.0.SelfAttention.o.weight', True), ('decoder.block.9.layer.0.layer_norm.weight', True), ('decoder.block.9.layer.1.EncDecAttention.q.weight', True), ('decoder.block.9.layer.1.EncDecAttention.k.weight', True), ('decoder.block.9.layer.1.EncDecAttention.v.weight', True), ('decoder.block.9.layer.1.EncDecAttention.o.weight', True), ('decoder.block.9.layer.1.layer_norm.weight', True), ('decoder.block.9.layer.2.DenseReluDense.wi_0.weight', True), ('decoder.block.9.layer.2.DenseReluDense.wi_1.weight', True), ('decoder.block.9.layer.2.DenseReluDense.wo.weight', True), ('decoder.block.9.layer.2.layer_norm.weight', True), ('decoder.block.10.layer.0.SelfAttention.q.weight', False), ('decoder.block.10.layer.0.SelfAttention.k.weight', False), ('decoder.block.10.layer.0.SelfAttention.v.weight', False), ('decoder.block.10.layer.0.SelfAttention.o.weight', False), ('decoder.block.10.layer.0.layer_norm.weight', False), ('decoder.block.10.layer.1.EncDecAttention.q.weight', False), ('decoder.block.10.layer.1.EncDecAttention.k.weight', False), ('decoder.block.10.layer.1.EncDecAttention.v.weight', False), ('decoder.block.10.layer.1.EncDecAttention.o.weight', False), ('decoder.block.10.layer.1.layer_norm.weight', False), ('decoder.block.10.layer.2.DenseReluDense.wi_0.weight', False), ('decoder.block.10.layer.2.DenseReluDense.wi_1.weight', False), ('decoder.block.10.layer.2.DenseReluDense.wo.weight', False), ('decoder.block.10.layer.2.layer_norm.weight', False), ('decoder.block.11.layer.0.SelfAttention.q.weight', True), ('decoder.block.11.layer.0.SelfAttention.k.weight', True), ('decoder.block.11.layer.0.SelfAttention.v.weight', True), ('decoder.block.11.layer.0.SelfAttention.o.weight', True), ('decoder.block.11.layer.0.layer_norm.weight', True), ('decoder.block.11.layer.1.EncDecAttention.q.weight', True), ('decoder.block.11.layer.1.EncDecAttention.k.weight', True), ('decoder.block.11.layer.1.EncDecAttention.v.weight', True), ('decoder.block.11.layer.1.EncDecAttention.o.weight', True), ('decoder.block.11.layer.1.layer_norm.weight', True), ('decoder.block.11.layer.2.DenseReluDense.wi_0.weight', True), ('decoder.block.11.layer.2.DenseReluDense.wi_1.weight', True), ('decoder.block.11.layer.2.DenseReluDense.wo.weight', True), ('decoder.block.11.layer.2.layer_norm.weight', True), ('decoder.final_layer_norm.weight', True), ('lm_head.weight', True)]\n",
            "INFO:__main__:Using translation prefix: \"beer classification: \"\n",
            "Running tokenizer on train dataset:   0% 0/9 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /content/uczenie_glebokie/.cache_training/json/default-baeafae2fbdfeb25/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-53b218ff202b1d6e.arrow\n",
            "Running tokenizer on train dataset: 100% 9/9 [00:03<00:00,  2.61ba/s]\n",
            "INFO:__main__:Set 139 samples for negative-class\n",
            "INFO:__main__:Set 139 samples for neutral-class\n",
            "INFO:__main__:Set 139 samples for positive-class\n",
            "Running tokenizer on validation dataset:   0% 0/1 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /content/uczenie_glebokie/.cache_training/json/default-baeafae2fbdfeb25/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-deca904dc14a5b89.arrow\n",
            "Running tokenizer on validation dataset: 100% 1/1 [00:00<00:00,  8.67ba/s]\n",
            "Running tokenizer on prediction dataset:   0% 0/1 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /content/uczenie_glebokie/.cache_training/json/default-baeafae2fbdfeb25/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-a46508fd520c2840.arrow\n",
            "Running tokenizer on prediction dataset: 100% 1/1 [00:00<00:00,  3.18ba/s]\n",
            "Downloading builder script: 100% 8.15k/8.15k [00:00<00:00, 5.05MB/s]\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "[INFO|trainer.py:1607] 2023-02-14 13:30:19,665 >> ***** Running training *****\n",
            "[INFO|trainer.py:1608] 2023-02-14 13:30:19,665 >>   Num examples = 9000\n",
            "[INFO|trainer.py:1609] 2023-02-14 13:30:19,665 >>   Num Epochs = 5\n",
            "[INFO|trainer.py:1610] 2023-02-14 13:30:19,665 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1611] 2023-02-14 13:30:19,665 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:1612] 2023-02-14 13:30:19,665 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1613] 2023-02-14 13:30:19,665 >>   Total optimization steps = 5625\n",
            "  0% 0/5625 [00:00<?, ?it/s][WARNING|logging.py:281] 2023-02-14 13:30:19,683 >> You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "{'loss': 17.7149, 'learning_rate': 4.9111111111111114e-05, 'epoch': 0.09}\n",
            "{'loss': 11.4757, 'learning_rate': 4.8222222222222225e-05, 'epoch': 0.18}\n",
            "  4% 250/5625 [01:57<40:17,  2.22it/s][INFO|trainer.py:2907] 2023-02-14 13:32:17,317 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2023-02-14 13:32:17,317 >>   Num examples = 306\n",
            "[INFO|trainer.py:2912] 2023-02-14 13:32:17,317 >>   Batch size = 8\n",
            "\n",
            "  0% 0/39 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 2/39 [00:00<00:05,  6.29it/s]\u001b[A\n",
            "  8% 3/39 [00:00<00:07,  4.50it/s]\u001b[A\n",
            " 10% 4/39 [00:00<00:08,  4.04it/s]\u001b[A\n",
            " 13% 5/39 [00:01<00:08,  3.79it/s]\u001b[A\n",
            " 15% 6/39 [00:01<00:09,  3.57it/s]\u001b[A\n",
            " 18% 7/39 [00:01<00:09,  3.43it/s]\u001b[A\n",
            " 21% 8/39 [00:02<00:09,  3.34it/s]\u001b[A\n",
            " 23% 9/39 [00:02<00:09,  3.28it/s]\u001b[A\n",
            " 26% 10/39 [00:02<00:08,  3.24it/s]\u001b[A\n",
            " 28% 11/39 [00:03<00:08,  3.21it/s]\u001b[A\n",
            " 31% 12/39 [00:03<00:08,  3.21it/s]\u001b[A\n",
            " 33% 13/39 [00:03<00:07,  3.33it/s]\u001b[A\n",
            " 36% 14/39 [00:04<00:07,  3.28it/s]\u001b[A\n",
            " 38% 15/39 [00:04<00:07,  3.24it/s]\u001b[A\n",
            " 41% 16/39 [00:04<00:07,  3.22it/s]\u001b[A\n",
            " 44% 17/39 [00:04<00:06,  3.21it/s]\u001b[A\n",
            " 46% 18/39 [00:05<00:06,  3.19it/s]\u001b[A\n",
            " 49% 19/39 [00:05<00:06,  3.19it/s]\u001b[A\n",
            " 51% 20/39 [00:05<00:05,  3.18it/s]\u001b[A\n",
            " 54% 21/39 [00:06<00:05,  3.25it/s]\u001b[A\n",
            " 56% 22/39 [00:06<00:05,  3.22it/s]\u001b[A\n",
            " 59% 23/39 [00:06<00:04,  3.21it/s]\u001b[A\n",
            " 62% 24/39 [00:07<00:04,  3.20it/s]\u001b[A\n",
            " 64% 25/39 [00:07<00:04,  3.18it/s]\u001b[A\n",
            " 67% 26/39 [00:07<00:04,  3.15it/s]\u001b[A\n",
            " 69% 27/39 [00:08<00:03,  3.16it/s]\u001b[A\n",
            " 72% 28/39 [00:08<00:03,  3.18it/s]\u001b[A\n",
            " 74% 29/39 [00:08<00:03,  3.18it/s]\u001b[A\n",
            " 77% 30/39 [00:09<00:02,  3.18it/s]\u001b[A\n",
            " 79% 31/39 [00:09<00:02,  3.18it/s]\u001b[A\n",
            " 82% 32/39 [00:09<00:02,  3.18it/s]\u001b[A\n",
            " 85% 33/39 [00:10<00:01,  3.14it/s]\u001b[A\n",
            " 87% 34/39 [00:10<00:01,  3.12it/s]\u001b[A\n",
            " 90% 35/39 [00:10<00:01,  3.12it/s]\u001b[A\n",
            " 92% 36/39 [00:10<00:00,  3.18it/s]\u001b[A\n",
            " 95% 37/39 [00:11<00:00,  3.25it/s]\u001b[A\n",
            " 97% 38/39 [00:11<00:00,  3.21it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.6369016766548157, 'eval_bleu': 0.0, 'eval_accuracy': 0.4542, 'eval_gen_len': 2.0, 'eval_runtime': 12.0773, 'eval_samples_per_second': 25.337, 'eval_steps_per_second': 3.229, 'epoch': 0.22}\n",
            "  4% 250/5625 [02:09<40:17,  2.22it/s]\n",
            "100% 39/39 [00:11<00:00,  3.92it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2656] 2023-02-14 13:32:29,397 >> Saving model checkpoint to out/beer_reviews/t5_v1_1_freeze_base/checkpoint-250\n",
            "[INFO|configuration_utils.py:447] 2023-02-14 13:32:29,398 >> Configuration saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-250/config.json\n",
            "[INFO|modeling_utils.py:1624] 2023-02-14 13:32:32,863 >> Model weights saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-250/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2023-02-14 13:32:32,864 >> tokenizer config file saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-250/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2023-02-14 13:32:32,864 >> Special tokens file saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-250/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:187] 2023-02-14 13:32:32,912 >> Copy vocab file to out/beer_reviews/t5_v1_1_freeze_base/checkpoint-250/spiece.model\n",
            "{'loss': 3.5655, 'learning_rate': 4.7333333333333336e-05, 'epoch': 0.27}\n",
            "{'loss': 1.1179, 'learning_rate': 4.644444444444445e-05, 'epoch': 0.36}\n",
            "{'loss': 0.8998, 'learning_rate': 4.555555555555556e-05, 'epoch': 0.44}\n",
            "  9% 500/5625 [04:15<38:13,  2.23it/s][INFO|trainer.py:2907] 2023-02-14 13:34:35,049 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2023-02-14 13:34:35,049 >>   Num examples = 306\n",
            "[INFO|trainer.py:2912] 2023-02-14 13:34:35,049 >>   Batch size = 8\n",
            "\n",
            "  0% 0/39 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 2/39 [00:00<00:05,  6.35it/s]\u001b[A\n",
            "  8% 3/39 [00:00<00:08,  4.47it/s]\u001b[A\n",
            " 10% 4/39 [00:00<00:08,  4.01it/s]\u001b[A\n",
            " 13% 5/39 [00:01<00:08,  3.79it/s]\u001b[A\n",
            " 15% 6/39 [00:01<00:09,  3.57it/s]\u001b[A\n",
            " 18% 7/39 [00:01<00:09,  3.43it/s]\u001b[A\n",
            " 21% 8/39 [00:02<00:09,  3.34it/s]\u001b[A\n",
            " 23% 9/39 [00:02<00:09,  3.29it/s]\u001b[A\n",
            " 26% 10/39 [00:02<00:08,  3.26it/s]\u001b[A\n",
            " 28% 11/39 [00:03<00:08,  3.24it/s]\u001b[A\n",
            " 31% 12/39 [00:03<00:08,  3.22it/s]\u001b[A\n",
            " 33% 13/39 [00:03<00:07,  3.34it/s]\u001b[A\n",
            " 36% 14/39 [00:04<00:07,  3.29it/s]\u001b[A\n",
            " 38% 15/39 [00:04<00:07,  3.24it/s]\u001b[A\n",
            " 41% 16/39 [00:04<00:07,  3.21it/s]\u001b[A\n",
            " 44% 17/39 [00:04<00:06,  3.20it/s]\u001b[A\n",
            " 46% 18/39 [00:05<00:06,  3.19it/s]\u001b[A\n",
            " 49% 19/39 [00:05<00:06,  3.19it/s]\u001b[A\n",
            " 51% 20/39 [00:05<00:05,  3.19it/s]\u001b[A\n",
            " 54% 21/39 [00:06<00:05,  3.26it/s]\u001b[A\n",
            " 56% 22/39 [00:06<00:05,  3.23it/s]\u001b[A\n",
            " 59% 23/39 [00:06<00:05,  3.18it/s]\u001b[A\n",
            " 62% 24/39 [00:07<00:04,  3.16it/s]\u001b[A\n",
            " 64% 25/39 [00:07<00:04,  3.14it/s]\u001b[A\n",
            " 67% 26/39 [00:07<00:04,  3.12it/s]\u001b[A\n",
            " 69% 27/39 [00:08<00:03,  3.11it/s]\u001b[A\n",
            " 72% 28/39 [00:08<00:03,  3.10it/s]\u001b[A\n",
            " 74% 29/39 [00:08<00:03,  3.09it/s]\u001b[A\n",
            " 77% 30/39 [00:09<00:02,  3.08it/s]\u001b[A\n",
            " 79% 31/39 [00:09<00:02,  3.07it/s]\u001b[A\n",
            " 82% 32/39 [00:09<00:02,  3.07it/s]\u001b[A\n",
            " 85% 33/39 [00:10<00:01,  3.10it/s]\u001b[A\n",
            " 87% 34/39 [00:10<00:01,  3.11it/s]\u001b[A\n",
            " 90% 35/39 [00:10<00:01,  3.14it/s]\u001b[A\n",
            " 92% 36/39 [00:10<00:00,  3.22it/s]\u001b[A\n",
            " 95% 37/39 [00:11<00:00,  3.30it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.5512716174125671, 'eval_bleu': 0.0, 'eval_accuracy': 0.451, 'eval_gen_len': 2.0, 'eval_runtime': 12.0142, 'eval_samples_per_second': 25.47, 'eval_steps_per_second': 3.246, 'epoch': 0.44}\n",
            "  9% 500/5625 [04:27<38:13,  2.23it/s]\n",
            "100% 39/39 [00:11<00:00,  3.28it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2656] 2023-02-14 13:34:47,064 >> Saving model checkpoint to out/beer_reviews/t5_v1_1_freeze_base/checkpoint-500\n",
            "[INFO|configuration_utils.py:447] 2023-02-14 13:34:47,065 >> Configuration saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:1624] 2023-02-14 13:34:50,480 >> Model weights saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2023-02-14 13:34:50,481 >> tokenizer config file saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2023-02-14 13:34:50,481 >> Special tokens file saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:187] 2023-02-14 13:34:50,528 >> Copy vocab file to out/beer_reviews/t5_v1_1_freeze_base/checkpoint-500/spiece.model\n",
            "{'loss': 0.8133, 'learning_rate': 4.466666666666667e-05, 'epoch': 0.53}\n",
            "{'loss': 0.7388, 'learning_rate': 4.377777777777778e-05, 'epoch': 0.62}\n",
            " 13% 750/5625 [06:34<38:01,  2.14it/s][INFO|trainer.py:2907] 2023-02-14 13:36:53,787 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2023-02-14 13:36:53,788 >>   Num examples = 306\n",
            "[INFO|trainer.py:2912] 2023-02-14 13:36:53,788 >>   Batch size = 8\n",
            "\n",
            "  0% 0/39 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 2/39 [00:00<00:05,  6.31it/s]\u001b[A\n",
            "  8% 3/39 [00:00<00:08,  4.49it/s]\u001b[A\n",
            " 10% 4/39 [00:00<00:08,  4.01it/s]\u001b[A\n",
            " 13% 5/39 [00:01<00:09,  3.76it/s]\u001b[A\n",
            " 15% 6/39 [00:01<00:09,  3.55it/s]\u001b[A\n",
            " 18% 7/39 [00:01<00:09,  3.42it/s]\u001b[A\n",
            " 21% 8/39 [00:02<00:09,  3.33it/s]\u001b[A\n",
            " 23% 9/39 [00:02<00:09,  3.28it/s]\u001b[A\n",
            " 26% 10/39 [00:02<00:09,  3.21it/s]\u001b[A\n",
            " 28% 11/39 [00:03<00:08,  3.17it/s]\u001b[A\n",
            " 31% 12/39 [00:03<00:08,  3.16it/s]\u001b[A\n",
            " 33% 13/39 [00:03<00:07,  3.26it/s]\u001b[A\n",
            " 36% 14/39 [00:04<00:07,  3.21it/s]\u001b[A\n",
            " 38% 15/39 [00:04<00:07,  3.18it/s]\u001b[A\n",
            " 41% 16/39 [00:04<00:07,  3.16it/s]\u001b[A\n",
            " 44% 17/39 [00:05<00:07,  3.11it/s]\u001b[A\n",
            " 46% 18/39 [00:05<00:06,  3.09it/s]\u001b[A\n",
            " 49% 19/39 [00:05<00:06,  3.06it/s]\u001b[A\n",
            " 51% 20/39 [00:06<00:06,  3.06it/s]\u001b[A\n",
            " 54% 21/39 [00:06<00:05,  3.12it/s]\u001b[A\n",
            " 56% 22/39 [00:06<00:05,  3.14it/s]\u001b[A\n",
            " 59% 23/39 [00:06<00:05,  3.12it/s]\u001b[A\n",
            " 62% 24/39 [00:07<00:04,  3.13it/s]\u001b[A\n",
            " 64% 25/39 [00:07<00:04,  3.14it/s]\u001b[A\n",
            " 67% 26/39 [00:07<00:04,  3.15it/s]\u001b[A\n",
            " 69% 27/39 [00:08<00:03,  3.15it/s]\u001b[A\n",
            " 72% 28/39 [00:08<00:03,  3.17it/s]\u001b[A\n",
            " 74% 29/39 [00:08<00:03,  3.15it/s]\u001b[A\n",
            " 77% 30/39 [00:09<00:02,  3.15it/s]\u001b[A\n",
            " 79% 31/39 [00:09<00:02,  3.17it/s]\u001b[A\n",
            " 82% 32/39 [00:09<00:02,  3.17it/s]\u001b[A\n",
            " 85% 33/39 [00:10<00:01,  3.17it/s]\u001b[A\n",
            " 87% 34/39 [00:10<00:01,  3.17it/s]\u001b[A\n",
            " 90% 35/39 [00:10<00:01,  3.19it/s]\u001b[A\n",
            " 92% 36/39 [00:11<00:00,  3.25it/s]\u001b[A\n",
            " 95% 37/39 [00:11<00:00,  3.32it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.5039864182472229, 'eval_bleu': 0.0, 'eval_accuracy': 0.451, 'eval_gen_len': 2.0, 'eval_runtime': 12.0976, 'eval_samples_per_second': 25.294, 'eval_steps_per_second': 3.224, 'epoch': 0.67}\n",
            " 13% 750/5625 [06:46<38:01,  2.14it/s]\n",
            "100% 39/39 [00:11<00:00,  3.28it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2656] 2023-02-14 13:37:05,886 >> Saving model checkpoint to out/beer_reviews/t5_v1_1_freeze_base/checkpoint-750\n",
            "[INFO|configuration_utils.py:447] 2023-02-14 13:37:05,887 >> Configuration saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-750/config.json\n",
            "[INFO|modeling_utils.py:1624] 2023-02-14 13:37:09,355 >> Model weights saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-750/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2023-02-14 13:37:09,355 >> tokenizer config file saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-750/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2023-02-14 13:37:09,356 >> Special tokens file saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-750/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:187] 2023-02-14 13:37:09,401 >> Copy vocab file to out/beer_reviews/t5_v1_1_freeze_base/checkpoint-750/spiece.model\n",
            "{'loss': 0.7633, 'learning_rate': 4.2888888888888886e-05, 'epoch': 0.71}\n",
            "{'loss': 0.727, 'learning_rate': 4.2e-05, 'epoch': 0.8}\n",
            "{'loss': 0.6701, 'learning_rate': 4.111111111111111e-05, 'epoch': 0.89}\n",
            " 18% 1000/5625 [08:53<33:25,  2.31it/s][INFO|trainer.py:2907] 2023-02-14 13:39:13,073 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2023-02-14 13:39:13,073 >>   Num examples = 306\n",
            "[INFO|trainer.py:2912] 2023-02-14 13:39:13,073 >>   Batch size = 8\n",
            "\n",
            "  0% 0/39 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 2/39 [00:00<00:05,  6.22it/s]\u001b[A\n",
            "  8% 3/39 [00:00<00:08,  4.40it/s]\u001b[A\n",
            " 10% 4/39 [00:00<00:08,  3.93it/s]\u001b[A\n",
            " 13% 5/39 [00:01<00:09,  3.69it/s]\u001b[A\n",
            " 15% 6/39 [00:01<00:09,  3.45it/s]\u001b[A\n",
            " 18% 7/39 [00:01<00:09,  3.31it/s]\u001b[A\n",
            " 21% 8/39 [00:02<00:09,  3.21it/s]\u001b[A\n",
            " 23% 9/39 [00:02<00:09,  3.16it/s]\u001b[A\n",
            " 26% 10/39 [00:02<00:09,  3.17it/s]\u001b[A\n",
            " 28% 11/39 [00:03<00:08,  3.17it/s]\u001b[A\n",
            " 31% 12/39 [00:03<00:08,  3.16it/s]\u001b[A\n",
            " 33% 13/39 [00:03<00:07,  3.29it/s]\u001b[A\n",
            " 36% 14/39 [00:04<00:07,  3.25it/s]\u001b[A\n",
            " 38% 15/39 [00:04<00:07,  3.23it/s]\u001b[A\n",
            " 41% 16/39 [00:04<00:07,  3.21it/s]\u001b[A\n",
            " 44% 17/39 [00:05<00:06,  3.19it/s]\u001b[A\n",
            " 46% 18/39 [00:05<00:06,  3.18it/s]\u001b[A\n",
            " 49% 19/39 [00:05<00:06,  3.18it/s]\u001b[A\n",
            " 51% 20/39 [00:05<00:05,  3.18it/s]\u001b[A\n",
            " 54% 21/39 [00:06<00:05,  3.25it/s]\u001b[A\n",
            " 56% 22/39 [00:06<00:05,  3.23it/s]\u001b[A\n",
            " 59% 23/39 [00:06<00:05,  3.20it/s]\u001b[A\n",
            " 62% 24/39 [00:07<00:04,  3.19it/s]\u001b[A\n",
            " 64% 25/39 [00:07<00:04,  3.17it/s]\u001b[A\n",
            " 67% 26/39 [00:07<00:04,  3.17it/s]\u001b[A\n",
            " 69% 27/39 [00:08<00:03,  3.18it/s]\u001b[A\n",
            " 72% 28/39 [00:08<00:03,  3.17it/s]\u001b[A\n",
            " 74% 29/39 [00:08<00:03,  3.17it/s]\u001b[A\n",
            " 77% 30/39 [00:09<00:02,  3.17it/s]\u001b[A\n",
            " 79% 31/39 [00:09<00:02,  3.16it/s]\u001b[A\n",
            " 82% 32/39 [00:09<00:02,  3.16it/s]\u001b[A\n",
            " 85% 33/39 [00:10<00:01,  3.16it/s]\u001b[A\n",
            " 87% 34/39 [00:10<00:01,  3.17it/s]\u001b[A\n",
            " 90% 35/39 [00:10<00:01,  3.18it/s]\u001b[A\n",
            " 92% 36/39 [00:10<00:00,  3.24it/s]\u001b[A\n",
            " 95% 37/39 [00:11<00:00,  3.33it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.4202306270599365, 'eval_bleu': 0.0, 'eval_accuracy': 0.6176, 'eval_gen_len': 2.0, 'eval_runtime': 12.0365, 'eval_samples_per_second': 25.423, 'eval_steps_per_second': 3.24, 'epoch': 0.89}\n",
            " 18% 1000/5625 [09:05<33:25,  2.31it/s]\n",
            "100% 39/39 [00:11<00:00,  3.29it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2656] 2023-02-14 13:39:25,114 >> Saving model checkpoint to out/beer_reviews/t5_v1_1_freeze_base/checkpoint-1000\n",
            "[INFO|configuration_utils.py:447] 2023-02-14 13:39:25,115 >> Configuration saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-1000/config.json\n",
            "[INFO|modeling_utils.py:1624] 2023-02-14 13:39:28,554 >> Model weights saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-1000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2023-02-14 13:39:28,555 >> tokenizer config file saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-1000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2023-02-14 13:39:28,556 >> Special tokens file saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-1000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:187] 2023-02-14 13:39:28,667 >> Copy vocab file to out/beer_reviews/t5_v1_1_freeze_base/checkpoint-1000/spiece.model\n",
            "{'loss': 0.5676, 'learning_rate': 4.022222222222222e-05, 'epoch': 0.98}\n",
            "{'loss': 0.5685, 'learning_rate': 3.933333333333333e-05, 'epoch': 1.07}\n",
            " 22% 1250/5625 [11:12<34:44,  2.10it/s][INFO|trainer.py:2907] 2023-02-14 13:41:32,130 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2023-02-14 13:41:32,130 >>   Num examples = 306\n",
            "[INFO|trainer.py:2912] 2023-02-14 13:41:32,130 >>   Batch size = 8\n",
            "\n",
            "  0% 0/39 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 2/39 [00:00<00:05,  6.31it/s]\u001b[A\n",
            "  8% 3/39 [00:00<00:08,  4.49it/s]\u001b[A\n",
            " 10% 4/39 [00:00<00:08,  4.04it/s]\u001b[A\n",
            " 13% 5/39 [00:01<00:08,  3.80it/s]\u001b[A\n",
            " 15% 6/39 [00:01<00:09,  3.57it/s]\u001b[A\n",
            " 18% 7/39 [00:01<00:09,  3.43it/s]\u001b[A\n",
            " 21% 8/39 [00:02<00:09,  3.34it/s]\u001b[A\n",
            " 23% 9/39 [00:02<00:09,  3.28it/s]\u001b[A\n",
            " 26% 10/39 [00:02<00:08,  3.24it/s]\u001b[A\n",
            " 28% 11/39 [00:03<00:08,  3.22it/s]\u001b[A\n",
            " 31% 12/39 [00:03<00:08,  3.20it/s]\u001b[A\n",
            " 33% 13/39 [00:03<00:07,  3.33it/s]\u001b[A\n",
            " 36% 14/39 [00:04<00:07,  3.28it/s]\u001b[A\n",
            " 38% 15/39 [00:04<00:07,  3.25it/s]\u001b[A\n",
            " 41% 16/39 [00:04<00:07,  3.23it/s]\u001b[A\n",
            " 44% 17/39 [00:04<00:06,  3.21it/s]\u001b[A\n",
            " 46% 18/39 [00:05<00:06,  3.19it/s]\u001b[A\n",
            " 49% 19/39 [00:05<00:06,  3.19it/s]\u001b[A\n",
            " 51% 20/39 [00:05<00:05,  3.17it/s]\u001b[A\n",
            " 54% 21/39 [00:06<00:05,  3.25it/s]\u001b[A\n",
            " 56% 22/39 [00:06<00:05,  3.23it/s]\u001b[A\n",
            " 59% 23/39 [00:06<00:04,  3.22it/s]\u001b[A\n",
            " 62% 24/39 [00:07<00:04,  3.19it/s]\u001b[A\n",
            " 64% 25/39 [00:07<00:04,  3.19it/s]\u001b[A\n",
            " 67% 26/39 [00:07<00:04,  3.18it/s]\u001b[A\n",
            " 69% 27/39 [00:08<00:03,  3.17it/s]\u001b[A\n",
            " 72% 28/39 [00:08<00:03,  3.18it/s]\u001b[A\n",
            " 74% 29/39 [00:08<00:03,  3.12it/s]\u001b[A\n",
            " 77% 30/39 [00:09<00:02,  3.08it/s]\u001b[A\n",
            " 79% 31/39 [00:09<00:02,  3.09it/s]\u001b[A\n",
            " 82% 32/39 [00:09<00:02,  3.08it/s]\u001b[A\n",
            " 85% 33/39 [00:10<00:01,  3.08it/s]\u001b[A\n",
            " 87% 34/39 [00:10<00:01,  3.08it/s]\u001b[A\n",
            " 90% 35/39 [00:10<00:01,  3.06it/s]\u001b[A\n",
            " 92% 36/39 [00:11<00:00,  3.14it/s]\u001b[A\n",
            " 95% 37/39 [00:11<00:00,  3.19it/s]\u001b[A\n",
            " 97% 38/39 [00:11<00:00,  3.15it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.5437268018722534, 'eval_bleu': 0.0, 'eval_accuracy': 0.6176, 'eval_gen_len': 2.0, 'eval_runtime': 12.1579, 'eval_samples_per_second': 25.169, 'eval_steps_per_second': 3.208, 'epoch': 1.11}\n",
            " 22% 1250/5625 [11:24<34:44,  2.10it/s]\n",
            "100% 39/39 [00:11<00:00,  3.78it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2656] 2023-02-14 13:41:44,290 >> Saving model checkpoint to out/beer_reviews/t5_v1_1_freeze_base/checkpoint-1250\n",
            "[INFO|configuration_utils.py:447] 2023-02-14 13:41:44,291 >> Configuration saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-1250/config.json\n",
            "[INFO|modeling_utils.py:1624] 2023-02-14 13:41:47,788 >> Model weights saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-1250/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2023-02-14 13:41:47,789 >> tokenizer config file saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-1250/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2023-02-14 13:41:47,790 >> Special tokens file saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-1250/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:187] 2023-02-14 13:41:47,838 >> Copy vocab file to out/beer_reviews/t5_v1_1_freeze_base/checkpoint-1250/spiece.model\n",
            "{'loss': 0.5279, 'learning_rate': 3.844444444444444e-05, 'epoch': 1.16}\n",
            "{'loss': 0.5141, 'learning_rate': 3.7555555555555554e-05, 'epoch': 1.24}\n",
            "{'loss': 0.4571, 'learning_rate': 3.6666666666666666e-05, 'epoch': 1.33}\n",
            " 27% 1500/5625 [13:30<30:55,  2.22it/s][INFO|trainer.py:2907] 2023-02-14 13:43:50,568 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2023-02-14 13:43:50,569 >>   Num examples = 306\n",
            "[INFO|trainer.py:2912] 2023-02-14 13:43:50,569 >>   Batch size = 8\n",
            "\n",
            "  0% 0/39 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 2/39 [00:00<00:05,  6.35it/s]\u001b[A\n",
            "  8% 3/39 [00:00<00:08,  4.47it/s]\u001b[A\n",
            " 10% 4/39 [00:00<00:08,  4.01it/s]\u001b[A\n",
            " 13% 5/39 [00:01<00:09,  3.77it/s]\u001b[A\n",
            " 15% 6/39 [00:01<00:09,  3.55it/s]\u001b[A\n",
            " 18% 7/39 [00:01<00:09,  3.41it/s]\u001b[A\n",
            " 21% 8/39 [00:02<00:09,  3.33it/s]\u001b[A\n",
            " 23% 9/39 [00:02<00:09,  3.29it/s]\u001b[A\n",
            " 26% 10/39 [00:02<00:08,  3.25it/s]\u001b[A\n",
            " 28% 11/39 [00:03<00:08,  3.22it/s]\u001b[A\n",
            " 31% 12/39 [00:03<00:08,  3.21it/s]\u001b[A\n",
            " 33% 13/39 [00:03<00:07,  3.34it/s]\u001b[A\n",
            " 36% 14/39 [00:04<00:07,  3.28it/s]\u001b[A\n",
            " 38% 15/39 [00:04<00:07,  3.24it/s]\u001b[A\n",
            " 41% 16/39 [00:04<00:07,  3.22it/s]\u001b[A\n",
            " 44% 17/39 [00:04<00:06,  3.21it/s]\u001b[A\n",
            " 46% 18/39 [00:05<00:06,  3.19it/s]\u001b[A\n",
            " 49% 19/39 [00:05<00:06,  3.17it/s]\u001b[A\n",
            " 51% 20/39 [00:05<00:06,  3.14it/s]\u001b[A\n",
            " 54% 21/39 [00:06<00:05,  3.20it/s]\u001b[A\n",
            " 56% 22/39 [00:06<00:05,  3.16it/s]\u001b[A\n",
            " 59% 23/39 [00:06<00:05,  3.15it/s]\u001b[A\n",
            " 62% 24/39 [00:07<00:04,  3.13it/s]\u001b[A\n",
            " 64% 25/39 [00:07<00:04,  3.11it/s]\u001b[A\n",
            " 67% 26/39 [00:07<00:04,  3.10it/s]\u001b[A\n",
            " 69% 27/39 [00:08<00:03,  3.06it/s]\u001b[A\n",
            " 72% 28/39 [00:08<00:03,  3.06it/s]\u001b[A\n",
            " 74% 29/39 [00:08<00:03,  3.06it/s]\u001b[A\n",
            " 77% 30/39 [00:09<00:02,  3.08it/s]\u001b[A\n",
            " 79% 31/39 [00:09<00:02,  3.11it/s]\u001b[A\n",
            " 82% 32/39 [00:09<00:02,  3.13it/s]\u001b[A\n",
            " 85% 33/39 [00:10<00:01,  3.14it/s]\u001b[A\n",
            " 87% 34/39 [00:10<00:01,  3.14it/s]\u001b[A\n",
            " 90% 35/39 [00:10<00:01,  3.16it/s]\u001b[A\n",
            " 92% 36/39 [00:11<00:00,  3.23it/s]\u001b[A\n",
            " 95% 37/39 [00:11<00:00,  3.31it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.5717930793762207, 'eval_bleu': 0.0, 'eval_accuracy': 0.6373, 'eval_gen_len': 2.0, 'eval_runtime': 12.0457, 'eval_samples_per_second': 25.403, 'eval_steps_per_second': 3.238, 'epoch': 1.33}\n",
            " 27% 1500/5625 [13:42<30:55,  2.22it/s]\n",
            "100% 39/39 [00:11<00:00,  3.28it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2656] 2023-02-14 13:44:02,616 >> Saving model checkpoint to out/beer_reviews/t5_v1_1_freeze_base/checkpoint-1500\n",
            "[INFO|configuration_utils.py:447] 2023-02-14 13:44:02,617 >> Configuration saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-1500/config.json\n",
            "[INFO|modeling_utils.py:1624] 2023-02-14 13:44:06,082 >> Model weights saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-1500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2023-02-14 13:44:06,083 >> tokenizer config file saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-1500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2023-02-14 13:44:06,084 >> Special tokens file saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-1500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:187] 2023-02-14 13:44:06,137 >> Copy vocab file to out/beer_reviews/t5_v1_1_freeze_base/checkpoint-1500/spiece.model\n",
            "[INFO|trainer.py:2734] 2023-02-14 13:44:12,358 >> Deleting older checkpoint [out/beer_reviews/t5_v1_1_freeze_base/checkpoint-250] due to args.save_total_limit\n",
            "{'loss': 0.4786, 'learning_rate': 3.577777777777778e-05, 'epoch': 1.42}\n",
            "{'loss': 0.4495, 'learning_rate': 3.4888888888888895e-05, 'epoch': 1.51}\n",
            " 31% 1750/5625 [15:49<29:26,  2.19it/s][INFO|trainer.py:2907] 2023-02-14 13:46:09,008 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2023-02-14 13:46:09,009 >>   Num examples = 306\n",
            "[INFO|trainer.py:2912] 2023-02-14 13:46:09,009 >>   Batch size = 8\n",
            "\n",
            "  0% 0/39 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 2/39 [00:00<00:05,  6.34it/s]\u001b[A\n",
            "  8% 3/39 [00:00<00:08,  4.47it/s]\u001b[A\n",
            " 10% 4/39 [00:00<00:08,  4.04it/s]\u001b[A\n",
            " 13% 5/39 [00:01<00:08,  3.80it/s]\u001b[A\n",
            " 15% 6/39 [00:01<00:09,  3.57it/s]\u001b[A\n",
            " 18% 7/39 [00:01<00:09,  3.43it/s]\u001b[A\n",
            " 21% 8/39 [00:02<00:09,  3.34it/s]\u001b[A\n",
            " 23% 9/39 [00:02<00:09,  3.24it/s]\u001b[A\n",
            " 26% 10/39 [00:02<00:09,  3.20it/s]\u001b[A\n",
            " 28% 11/39 [00:03<00:08,  3.17it/s]\u001b[A\n",
            " 31% 12/39 [00:03<00:08,  3.13it/s]\u001b[A\n",
            " 33% 13/39 [00:03<00:07,  3.26it/s]\u001b[A\n",
            " 36% 14/39 [00:04<00:07,  3.21it/s]\u001b[A\n",
            " 38% 15/39 [00:04<00:07,  3.17it/s]\u001b[A\n",
            " 41% 16/39 [00:04<00:07,  3.12it/s]\u001b[A\n",
            " 44% 17/39 [00:05<00:07,  3.09it/s]\u001b[A\n",
            " 46% 18/39 [00:05<00:06,  3.08it/s]\u001b[A\n",
            " 49% 19/39 [00:05<00:06,  3.07it/s]\u001b[A\n",
            " 51% 20/39 [00:06<00:06,  3.10it/s]\u001b[A\n",
            " 54% 21/39 [00:06<00:05,  3.20it/s]\u001b[A\n",
            " 56% 22/39 [00:06<00:05,  3.19it/s]\u001b[A\n",
            " 59% 23/39 [00:06<00:05,  3.17it/s]\u001b[A\n",
            " 62% 24/39 [00:07<00:04,  3.18it/s]\u001b[A\n",
            " 64% 25/39 [00:07<00:04,  3.17it/s]\u001b[A\n",
            " 67% 26/39 [00:07<00:04,  3.17it/s]\u001b[A\n",
            " 69% 27/39 [00:08<00:03,  3.18it/s]\u001b[A\n",
            " 72% 28/39 [00:08<00:03,  3.17it/s]\u001b[A\n",
            " 74% 29/39 [00:08<00:03,  3.17it/s]\u001b[A\n",
            " 77% 30/39 [00:09<00:02,  3.17it/s]\u001b[A\n",
            " 79% 31/39 [00:09<00:02,  3.17it/s]\u001b[A\n",
            " 82% 32/39 [00:09<00:02,  3.18it/s]\u001b[A\n",
            " 85% 33/39 [00:10<00:01,  3.18it/s]\u001b[A\n",
            " 87% 34/39 [00:10<00:01,  3.17it/s]\u001b[A\n",
            " 90% 35/39 [00:10<00:01,  3.19it/s]\u001b[A\n",
            " 92% 36/39 [00:11<00:00,  3.25it/s]\u001b[A\n",
            " 95% 37/39 [00:11<00:00,  3.33it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.40965503454208374, 'eval_bleu': 0.0, 'eval_accuracy': 0.6863, 'eval_gen_len': 2.0, 'eval_runtime': 12.0709, 'eval_samples_per_second': 25.35, 'eval_steps_per_second': 3.231, 'epoch': 1.56}\n",
            " 31% 1750/5625 [16:01<29:26,  2.19it/s]\n",
            "100% 39/39 [00:11<00:00,  3.28it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2656] 2023-02-14 13:46:21,081 >> Saving model checkpoint to out/beer_reviews/t5_v1_1_freeze_base/checkpoint-1750\n",
            "[INFO|configuration_utils.py:447] 2023-02-14 13:46:21,082 >> Configuration saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-1750/config.json\n",
            "[INFO|modeling_utils.py:1624] 2023-02-14 13:46:24,527 >> Model weights saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-1750/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2023-02-14 13:46:24,528 >> tokenizer config file saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-1750/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2023-02-14 13:46:24,529 >> Special tokens file saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-1750/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:187] 2023-02-14 13:46:24,578 >> Copy vocab file to out/beer_reviews/t5_v1_1_freeze_base/checkpoint-1750/spiece.model\n",
            "[INFO|trainer.py:2734] 2023-02-14 13:46:30,818 >> Deleting older checkpoint [out/beer_reviews/t5_v1_1_freeze_base/checkpoint-500] due to args.save_total_limit\n",
            "{'loss': 0.4252, 'learning_rate': 3.4000000000000007e-05, 'epoch': 1.6}\n",
            "{'loss': 0.4047, 'learning_rate': 3.311111111111112e-05, 'epoch': 1.69}\n",
            "{'loss': 0.414, 'learning_rate': 3.222222222222223e-05, 'epoch': 1.78}\n",
            " 36% 2000/5625 [18:08<28:46,  2.10it/s][INFO|trainer.py:2907] 2023-02-14 13:48:28,215 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2023-02-14 13:48:28,216 >>   Num examples = 306\n",
            "[INFO|trainer.py:2912] 2023-02-14 13:48:28,216 >>   Batch size = 8\n",
            "\n",
            "  0% 0/39 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 2/39 [00:00<00:06,  6.07it/s]\u001b[A\n",
            "  8% 3/39 [00:00<00:08,  4.28it/s]\u001b[A\n",
            " 10% 4/39 [00:00<00:09,  3.85it/s]\u001b[A\n",
            " 13% 5/39 [00:01<00:09,  3.62it/s]\u001b[A\n",
            " 15% 6/39 [00:01<00:09,  3.46it/s]\u001b[A\n",
            " 18% 7/39 [00:01<00:09,  3.35it/s]\u001b[A\n",
            " 21% 8/39 [00:02<00:09,  3.28it/s]\u001b[A\n",
            " 23% 9/39 [00:02<00:09,  3.25it/s]\u001b[A\n",
            " 26% 10/39 [00:02<00:09,  3.22it/s]\u001b[A\n",
            " 28% 11/39 [00:03<00:08,  3.20it/s]\u001b[A\n",
            " 31% 12/39 [00:03<00:08,  3.18it/s]\u001b[A\n",
            " 33% 13/39 [00:03<00:07,  3.32it/s]\u001b[A\n",
            " 36% 14/39 [00:04<00:07,  3.27it/s]\u001b[A\n",
            " 38% 15/39 [00:04<00:07,  3.24it/s]\u001b[A\n",
            " 41% 16/39 [00:04<00:07,  3.21it/s]\u001b[A\n",
            " 44% 17/39 [00:05<00:06,  3.19it/s]\u001b[A\n",
            " 46% 18/39 [00:05<00:06,  3.18it/s]\u001b[A\n",
            " 49% 19/39 [00:05<00:06,  3.18it/s]\u001b[A\n",
            " 51% 20/39 [00:05<00:06,  3.17it/s]\u001b[A\n",
            " 54% 21/39 [00:06<00:05,  3.25it/s]\u001b[A\n",
            " 56% 22/39 [00:06<00:05,  3.22it/s]\u001b[A\n",
            " 59% 23/39 [00:06<00:04,  3.21it/s]\u001b[A\n",
            " 62% 24/39 [00:07<00:04,  3.21it/s]\u001b[A\n",
            " 64% 25/39 [00:07<00:04,  3.19it/s]\u001b[A\n",
            " 67% 26/39 [00:07<00:04,  3.19it/s]\u001b[A\n",
            " 69% 27/39 [00:08<00:03,  3.19it/s]\u001b[A\n",
            " 72% 28/39 [00:08<00:03,  3.19it/s]\u001b[A\n",
            " 74% 29/39 [00:08<00:03,  3.18it/s]\u001b[A\n",
            " 77% 30/39 [00:09<00:02,  3.19it/s]\u001b[A\n",
            " 79% 31/39 [00:09<00:02,  3.19it/s]\u001b[A\n",
            " 82% 32/39 [00:09<00:02,  3.19it/s]\u001b[A\n",
            " 85% 33/39 [00:10<00:01,  3.16it/s]\u001b[A\n",
            " 87% 34/39 [00:10<00:01,  3.16it/s]\u001b[A\n",
            " 90% 35/39 [00:10<00:01,  3.17it/s]\u001b[A\n",
            " 92% 36/39 [00:10<00:00,  3.25it/s]\u001b[A\n",
            " 95% 37/39 [00:11<00:00,  3.33it/s]\u001b[A\n",
            " 97% 38/39 [00:11<00:00,  3.26it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.39482563734054565, 'eval_bleu': 0.0, 'eval_accuracy': 0.6699, 'eval_gen_len': 2.0, 'eval_runtime': 12.0629, 'eval_samples_per_second': 25.367, 'eval_steps_per_second': 3.233, 'epoch': 1.78}\n",
            " 36% 2000/5625 [18:20<28:46,  2.10it/s]\n",
            "100% 39/39 [00:11<00:00,  3.99it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2656] 2023-02-14 13:48:40,280 >> Saving model checkpoint to out/beer_reviews/t5_v1_1_freeze_base/checkpoint-2000\n",
            "[INFO|configuration_utils.py:447] 2023-02-14 13:48:40,282 >> Configuration saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-2000/config.json\n",
            "[INFO|modeling_utils.py:1624] 2023-02-14 13:48:43,884 >> Model weights saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-2000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2023-02-14 13:48:43,885 >> tokenizer config file saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-2000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2023-02-14 13:48:43,885 >> Special tokens file saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-2000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:187] 2023-02-14 13:48:43,925 >> Copy vocab file to out/beer_reviews/t5_v1_1_freeze_base/checkpoint-2000/spiece.model\n",
            "[INFO|trainer.py:2734] 2023-02-14 13:48:50,349 >> Deleting older checkpoint [out/beer_reviews/t5_v1_1_freeze_base/checkpoint-750] due to args.save_total_limit\n",
            "{'loss': 0.4195, 'learning_rate': 3.1333333333333334e-05, 'epoch': 1.87}\n",
            "{'loss': 0.4066, 'learning_rate': 3.044444444444445e-05, 'epoch': 1.96}\n",
            " 40% 2250/5625 [20:26<26:43,  2.10it/s][INFO|trainer.py:2907] 2023-02-14 13:50:46,665 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2023-02-14 13:50:46,665 >>   Num examples = 306\n",
            "[INFO|trainer.py:2912] 2023-02-14 13:50:46,665 >>   Batch size = 8\n",
            "\n",
            "  0% 0/39 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 2/39 [00:00<00:05,  6.33it/s]\u001b[A\n",
            "  8% 3/39 [00:00<00:08,  4.45it/s]\u001b[A\n",
            " 10% 4/39 [00:00<00:08,  4.03it/s]\u001b[A\n",
            " 13% 5/39 [00:01<00:08,  3.80it/s]\u001b[A\n",
            " 15% 6/39 [00:01<00:09,  3.58it/s]\u001b[A\n",
            " 18% 7/39 [00:01<00:09,  3.43it/s]\u001b[A\n",
            " 21% 8/39 [00:02<00:09,  3.33it/s]\u001b[A\n",
            " 23% 9/39 [00:02<00:09,  3.29it/s]\u001b[A\n",
            " 26% 10/39 [00:02<00:08,  3.24it/s]\u001b[A\n",
            " 28% 11/39 [00:03<00:08,  3.22it/s]\u001b[A\n",
            " 31% 12/39 [00:03<00:08,  3.21it/s]\u001b[A\n",
            " 33% 13/39 [00:03<00:07,  3.34it/s]\u001b[A\n",
            " 36% 14/39 [00:04<00:07,  3.30it/s]\u001b[A\n",
            " 38% 15/39 [00:04<00:07,  3.27it/s]\u001b[A\n",
            " 41% 16/39 [00:04<00:07,  3.24it/s]\u001b[A\n",
            " 44% 17/39 [00:04<00:06,  3.21it/s]\u001b[A\n",
            " 46% 18/39 [00:05<00:06,  3.20it/s]\u001b[A\n",
            " 49% 19/39 [00:05<00:06,  3.18it/s]\u001b[A\n",
            " 51% 20/39 [00:05<00:05,  3.19it/s]\u001b[A\n",
            " 54% 21/39 [00:06<00:05,  3.26it/s]\u001b[A\n",
            " 56% 22/39 [00:06<00:05,  3.25it/s]\u001b[A\n",
            " 59% 23/39 [00:06<00:04,  3.24it/s]\u001b[A\n",
            " 62% 24/39 [00:07<00:04,  3.22it/s]\u001b[A\n",
            " 64% 25/39 [00:07<00:04,  3.17it/s]\u001b[A\n",
            " 67% 26/39 [00:07<00:04,  3.14it/s]\u001b[A\n",
            " 69% 27/39 [00:08<00:03,  3.13it/s]\u001b[A\n",
            " 72% 28/39 [00:08<00:03,  3.11it/s]\u001b[A\n",
            " 74% 29/39 [00:08<00:03,  3.11it/s]\u001b[A\n",
            " 77% 30/39 [00:09<00:02,  3.11it/s]\u001b[A\n",
            " 79% 31/39 [00:09<00:02,  3.11it/s]\u001b[A\n",
            " 82% 32/39 [00:09<00:02,  3.09it/s]\u001b[A\n",
            " 85% 33/39 [00:10<00:01,  3.08it/s]\u001b[A\n",
            " 87% 34/39 [00:10<00:01,  3.07it/s]\u001b[A\n",
            " 90% 35/39 [00:10<00:01,  3.07it/s]\u001b[A\n",
            " 92% 36/39 [00:10<00:00,  3.16it/s]\u001b[A\n",
            " 95% 37/39 [00:11<00:00,  3.25it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.4504011571407318, 'eval_bleu': 0.0, 'eval_accuracy': 0.6699, 'eval_gen_len': 2.0, 'eval_runtime': 12.038, 'eval_samples_per_second': 25.42, 'eval_steps_per_second': 3.24, 'epoch': 2.0}\n",
            " 40% 2250/5625 [20:39<26:43,  2.10it/s]\n",
            "100% 39/39 [00:11<00:00,  3.23it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2656] 2023-02-14 13:50:58,704 >> Saving model checkpoint to out/beer_reviews/t5_v1_1_freeze_base/checkpoint-2250\n",
            "[INFO|configuration_utils.py:447] 2023-02-14 13:50:58,705 >> Configuration saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-2250/config.json\n",
            "[INFO|modeling_utils.py:1624] 2023-02-14 13:51:02,119 >> Model weights saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-2250/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2023-02-14 13:51:02,120 >> tokenizer config file saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-2250/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2023-02-14 13:51:02,120 >> Special tokens file saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-2250/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:187] 2023-02-14 13:51:02,163 >> Copy vocab file to out/beer_reviews/t5_v1_1_freeze_base/checkpoint-2250/spiece.model\n",
            "[INFO|trainer.py:2734] 2023-02-14 13:51:08,771 >> Deleting older checkpoint [out/beer_reviews/t5_v1_1_freeze_base/checkpoint-1000] due to args.save_total_limit\n",
            "{'loss': 0.3803, 'learning_rate': 2.955555555555556e-05, 'epoch': 2.04}\n",
            "{'loss': 0.4106, 'learning_rate': 2.8666666666666668e-05, 'epoch': 2.13}\n",
            "{'loss': 0.3926, 'learning_rate': 2.777777777777778e-05, 'epoch': 2.22}\n",
            " 44% 2500/5625 [22:46<24:48,  2.10it/s][INFO|trainer.py:2907] 2023-02-14 13:53:05,694 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2023-02-14 13:53:05,695 >>   Num examples = 306\n",
            "[INFO|trainer.py:2912] 2023-02-14 13:53:05,695 >>   Batch size = 8\n",
            "\n",
            "  0% 0/39 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 2/39 [00:00<00:05,  6.28it/s]\u001b[A\n",
            "  8% 3/39 [00:00<00:08,  4.47it/s]\u001b[A\n",
            " 10% 4/39 [00:00<00:08,  4.03it/s]\u001b[A\n",
            " 13% 5/39 [00:01<00:08,  3.79it/s]\u001b[A\n",
            " 15% 6/39 [00:01<00:09,  3.57it/s]\u001b[A\n",
            " 18% 7/39 [00:01<00:09,  3.44it/s]\u001b[A\n",
            " 21% 8/39 [00:02<00:09,  3.34it/s]\u001b[A\n",
            " 23% 9/39 [00:02<00:09,  3.28it/s]\u001b[A\n",
            " 26% 10/39 [00:02<00:09,  3.20it/s]\u001b[A\n",
            " 28% 11/39 [00:03<00:08,  3.16it/s]\u001b[A\n",
            " 31% 12/39 [00:03<00:08,  3.15it/s]\u001b[A\n",
            " 33% 13/39 [00:03<00:07,  3.27it/s]\u001b[A\n",
            " 36% 14/39 [00:04<00:07,  3.20it/s]\u001b[A\n",
            " 38% 15/39 [00:04<00:07,  3.17it/s]\u001b[A\n",
            " 41% 16/39 [00:04<00:07,  3.15it/s]\u001b[A\n",
            " 44% 17/39 [00:05<00:07,  3.13it/s]\u001b[A\n",
            " 46% 18/39 [00:05<00:06,  3.10it/s]\u001b[A\n",
            " 49% 19/39 [00:05<00:06,  3.08it/s]\u001b[A\n",
            " 51% 20/39 [00:06<00:06,  3.07it/s]\u001b[A\n",
            " 54% 21/39 [00:06<00:05,  3.14it/s]\u001b[A\n",
            " 56% 22/39 [00:06<00:05,  3.15it/s]\u001b[A\n",
            " 59% 23/39 [00:06<00:05,  3.16it/s]\u001b[A\n",
            " 62% 24/39 [00:07<00:04,  3.17it/s]\u001b[A\n",
            " 64% 25/39 [00:07<00:04,  3.17it/s]\u001b[A\n",
            " 67% 26/39 [00:07<00:04,  3.17it/s]\u001b[A\n",
            " 69% 27/39 [00:08<00:03,  3.17it/s]\u001b[A\n",
            " 72% 28/39 [00:08<00:03,  3.17it/s]\u001b[A\n",
            " 74% 29/39 [00:08<00:03,  3.17it/s]\u001b[A\n",
            " 77% 30/39 [00:09<00:02,  3.17it/s]\u001b[A\n",
            " 79% 31/39 [00:09<00:02,  3.17it/s]\u001b[A\n",
            " 82% 32/39 [00:09<00:02,  3.17it/s]\u001b[A\n",
            " 85% 33/39 [00:10<00:01,  3.17it/s]\u001b[A\n",
            " 87% 34/39 [00:10<00:01,  3.15it/s]\u001b[A\n",
            " 90% 35/39 [00:10<00:01,  3.17it/s]\u001b[A\n",
            " 92% 36/39 [00:11<00:00,  3.24it/s]\u001b[A\n",
            " 95% 37/39 [00:11<00:00,  3.32it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.35998284816741943, 'eval_bleu': 0.0, 'eval_accuracy': 0.6928, 'eval_gen_len': 2.0, 'eval_runtime': 12.0448, 'eval_samples_per_second': 25.405, 'eval_steps_per_second': 3.238, 'epoch': 2.22}\n",
            " 44% 2500/5625 [22:58<24:48,  2.10it/s]\n",
            "100% 39/39 [00:11<00:00,  3.29it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2656] 2023-02-14 13:53:17,744 >> Saving model checkpoint to out/beer_reviews/t5_v1_1_freeze_base/checkpoint-2500\n",
            "[INFO|configuration_utils.py:447] 2023-02-14 13:53:17,745 >> Configuration saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-2500/config.json\n",
            "[INFO|modeling_utils.py:1624] 2023-02-14 13:53:21,186 >> Model weights saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-2500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2023-02-14 13:53:21,187 >> tokenizer config file saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-2500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2023-02-14 13:53:21,187 >> Special tokens file saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-2500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:187] 2023-02-14 13:53:21,232 >> Copy vocab file to out/beer_reviews/t5_v1_1_freeze_base/checkpoint-2500/spiece.model\n",
            "[INFO|trainer.py:2734] 2023-02-14 13:53:27,485 >> Deleting older checkpoint [out/beer_reviews/t5_v1_1_freeze_base/checkpoint-1250] due to args.save_total_limit\n",
            "{'loss': 0.37, 'learning_rate': 2.688888888888889e-05, 'epoch': 2.31}\n",
            "{'loss': 0.3913, 'learning_rate': 2.6000000000000002e-05, 'epoch': 2.4}\n",
            " 49% 2750/5625 [25:04<22:29,  2.13it/s][INFO|trainer.py:2907] 2023-02-14 13:55:24,133 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2023-02-14 13:55:24,134 >>   Num examples = 306\n",
            "[INFO|trainer.py:2912] 2023-02-14 13:55:24,134 >>   Batch size = 8\n",
            "\n",
            "  0% 0/39 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 2/39 [00:00<00:06,  6.03it/s]\u001b[A\n",
            "  8% 3/39 [00:00<00:08,  4.35it/s]\u001b[A\n",
            " 10% 4/39 [00:00<00:08,  3.91it/s]\u001b[A\n",
            " 13% 5/39 [00:01<00:09,  3.68it/s]\u001b[A\n",
            " 15% 6/39 [00:01<00:09,  3.46it/s]\u001b[A\n",
            " 18% 7/39 [00:01<00:09,  3.32it/s]\u001b[A\n",
            " 21% 8/39 [00:02<00:09,  3.22it/s]\u001b[A\n",
            " 23% 9/39 [00:02<00:09,  3.17it/s]\u001b[A\n",
            " 26% 10/39 [00:02<00:09,  3.17it/s]\u001b[A\n",
            " 28% 11/39 [00:03<00:08,  3.17it/s]\u001b[A\n",
            " 31% 12/39 [00:03<00:08,  3.17it/s]\u001b[A\n",
            " 33% 13/39 [00:03<00:07,  3.31it/s]\u001b[A\n",
            " 36% 14/39 [00:04<00:07,  3.27it/s]\u001b[A\n",
            " 38% 15/39 [00:04<00:07,  3.24it/s]\u001b[A\n",
            " 41% 16/39 [00:04<00:07,  3.18it/s]\u001b[A\n",
            " 44% 17/39 [00:05<00:06,  3.18it/s]\u001b[A\n",
            " 46% 18/39 [00:05<00:06,  3.18it/s]\u001b[A\n",
            " 49% 19/39 [00:05<00:06,  3.17it/s]\u001b[A\n",
            " 51% 20/39 [00:05<00:05,  3.18it/s]\u001b[A\n",
            " 54% 21/39 [00:06<00:05,  3.25it/s]\u001b[A\n",
            " 56% 22/39 [00:06<00:05,  3.23it/s]\u001b[A\n",
            " 59% 23/39 [00:06<00:04,  3.21it/s]\u001b[A\n",
            " 62% 24/39 [00:07<00:04,  3.20it/s]\u001b[A\n",
            " 64% 25/39 [00:07<00:04,  3.18it/s]\u001b[A\n",
            " 67% 26/39 [00:07<00:04,  3.17it/s]\u001b[A\n",
            " 69% 27/39 [00:08<00:03,  3.18it/s]\u001b[A\n",
            " 72% 28/39 [00:08<00:03,  3.18it/s]\u001b[A\n",
            " 74% 29/39 [00:08<00:03,  3.17it/s]\u001b[A\n",
            " 77% 30/39 [00:09<00:02,  3.17it/s]\u001b[A\n",
            " 79% 31/39 [00:09<00:02,  3.18it/s]\u001b[A\n",
            " 82% 32/39 [00:09<00:02,  3.16it/s]\u001b[A\n",
            " 85% 33/39 [00:10<00:01,  3.17it/s]\u001b[A\n",
            " 87% 34/39 [00:10<00:01,  3.17it/s]\u001b[A\n",
            " 90% 35/39 [00:10<00:01,  3.19it/s]\u001b[A\n",
            " 92% 36/39 [00:10<00:00,  3.25it/s]\u001b[A\n",
            " 95% 37/39 [00:11<00:00,  3.33it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.3781893253326416, 'eval_bleu': 0.0, 'eval_accuracy': 0.6895, 'eval_gen_len': 2.0, 'eval_runtime': 12.0394, 'eval_samples_per_second': 25.417, 'eval_steps_per_second': 3.239, 'epoch': 2.44}\n",
            " 49% 2750/5625 [25:16<22:29,  2.13it/s]\n",
            "100% 39/39 [00:11<00:00,  3.29it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2656] 2023-02-14 13:55:36,175 >> Saving model checkpoint to out/beer_reviews/t5_v1_1_freeze_base/checkpoint-2750\n",
            "[INFO|configuration_utils.py:447] 2023-02-14 13:55:36,176 >> Configuration saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-2750/config.json\n",
            "[INFO|modeling_utils.py:1624] 2023-02-14 13:55:39,620 >> Model weights saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-2750/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2023-02-14 13:55:39,622 >> tokenizer config file saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-2750/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2023-02-14 13:55:39,622 >> Special tokens file saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-2750/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:187] 2023-02-14 13:55:39,702 >> Copy vocab file to out/beer_reviews/t5_v1_1_freeze_base/checkpoint-2750/spiece.model\n",
            "[INFO|trainer.py:2734] 2023-02-14 13:55:45,943 >> Deleting older checkpoint [out/beer_reviews/t5_v1_1_freeze_base/checkpoint-1500] due to args.save_total_limit\n",
            "{'loss': 0.3638, 'learning_rate': 2.5111111111111113e-05, 'epoch': 2.49}\n",
            "{'loss': 0.3637, 'learning_rate': 2.4222222222222224e-05, 'epoch': 2.58}\n",
            "{'loss': 0.3558, 'learning_rate': 2.3333333333333336e-05, 'epoch': 2.67}\n",
            " 53% 3000/5625 [27:22<20:47,  2.10it/s][INFO|trainer.py:2907] 2023-02-14 13:57:41,832 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2023-02-14 13:57:41,832 >>   Num examples = 306\n",
            "[INFO|trainer.py:2912] 2023-02-14 13:57:41,833 >>   Batch size = 8\n",
            "\n",
            "  0% 0/39 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 2/39 [00:00<00:05,  6.30it/s]\u001b[A\n",
            "  8% 3/39 [00:00<00:07,  4.50it/s]\u001b[A\n",
            " 10% 4/39 [00:00<00:08,  4.06it/s]\u001b[A\n",
            " 13% 5/39 [00:01<00:08,  3.81it/s]\u001b[A\n",
            " 15% 6/39 [00:01<00:09,  3.58it/s]\u001b[A\n",
            " 18% 7/39 [00:01<00:09,  3.45it/s]\u001b[A\n",
            " 21% 8/39 [00:02<00:09,  3.35it/s]\u001b[A\n",
            " 23% 9/39 [00:02<00:09,  3.30it/s]\u001b[A\n",
            " 26% 10/39 [00:02<00:08,  3.26it/s]\u001b[A\n",
            " 28% 11/39 [00:03<00:08,  3.24it/s]\u001b[A\n",
            " 31% 12/39 [00:03<00:08,  3.23it/s]\u001b[A\n",
            " 33% 13/39 [00:03<00:07,  3.36it/s]\u001b[A\n",
            " 36% 14/39 [00:03<00:07,  3.30it/s]\u001b[A\n",
            " 38% 15/39 [00:04<00:07,  3.27it/s]\u001b[A\n",
            " 41% 16/39 [00:04<00:07,  3.24it/s]\u001b[A\n",
            " 44% 17/39 [00:04<00:06,  3.22it/s]\u001b[A\n",
            " 46% 18/39 [00:05<00:06,  3.20it/s]\u001b[A\n",
            " 49% 19/39 [00:05<00:06,  3.18it/s]\u001b[A\n",
            " 51% 20/39 [00:05<00:05,  3.18it/s]\u001b[A\n",
            " 54% 21/39 [00:06<00:05,  3.26it/s]\u001b[A\n",
            " 56% 22/39 [00:06<00:05,  3.25it/s]\u001b[A\n",
            " 59% 23/39 [00:06<00:04,  3.23it/s]\u001b[A\n",
            " 62% 24/39 [00:07<00:04,  3.22it/s]\u001b[A\n",
            " 64% 25/39 [00:07<00:04,  3.20it/s]\u001b[A\n",
            " 67% 26/39 [00:07<00:04,  3.19it/s]\u001b[A\n",
            " 69% 27/39 [00:08<00:03,  3.19it/s]\u001b[A\n",
            " 72% 28/39 [00:08<00:03,  3.19it/s]\u001b[A\n",
            " 74% 29/39 [00:08<00:03,  3.19it/s]\u001b[A\n",
            " 77% 30/39 [00:08<00:02,  3.18it/s]\u001b[A\n",
            " 79% 31/39 [00:09<00:02,  3.19it/s]\u001b[A\n",
            " 82% 32/39 [00:09<00:02,  3.17it/s]\u001b[A\n",
            " 85% 33/39 [00:09<00:01,  3.15it/s]\u001b[A\n",
            " 87% 34/39 [00:10<00:01,  3.13it/s]\u001b[A\n",
            " 90% 35/39 [00:10<00:01,  3.13it/s]\u001b[A\n",
            " 92% 36/39 [00:10<00:00,  3.18it/s]\u001b[A\n",
            " 95% 37/39 [00:11<00:00,  3.26it/s]\u001b[A\n",
            " 97% 38/39 [00:11<00:00,  3.22it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.349224328994751, 'eval_bleu': 0.0, 'eval_accuracy': 0.6895, 'eval_gen_len': 2.0, 'eval_runtime': 12.0028, 'eval_samples_per_second': 25.494, 'eval_steps_per_second': 3.249, 'epoch': 2.67}\n",
            " 53% 3000/5625 [27:34<20:47,  2.10it/s]\n",
            "100% 39/39 [00:11<00:00,  3.88it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2656] 2023-02-14 13:57:53,837 >> Saving model checkpoint to out/beer_reviews/t5_v1_1_freeze_base/checkpoint-3000\n",
            "[INFO|configuration_utils.py:447] 2023-02-14 13:57:53,838 >> Configuration saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-3000/config.json\n",
            "[INFO|modeling_utils.py:1624] 2023-02-14 13:57:57,353 >> Model weights saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-3000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2023-02-14 13:57:57,354 >> tokenizer config file saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-3000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2023-02-14 13:57:57,355 >> Special tokens file saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-3000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:187] 2023-02-14 13:57:57,401 >> Copy vocab file to out/beer_reviews/t5_v1_1_freeze_base/checkpoint-3000/spiece.model\n",
            "[INFO|trainer.py:2734] 2023-02-14 13:58:03,670 >> Deleting older checkpoint [out/beer_reviews/t5_v1_1_freeze_base/checkpoint-1750] due to args.save_total_limit\n",
            "{'loss': 0.3527, 'learning_rate': 2.2444444444444447e-05, 'epoch': 2.76}\n",
            "{'loss': 0.3955, 'learning_rate': 2.1555555555555555e-05, 'epoch': 2.84}\n",
            " 58% 3250/5625 [29:40<18:03,  2.19it/s][INFO|trainer.py:2907] 2023-02-14 14:00:00,108 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2023-02-14 14:00:00,108 >>   Num examples = 306\n",
            "[INFO|trainer.py:2912] 2023-02-14 14:00:00,108 >>   Batch size = 8\n",
            "\n",
            "  0% 0/39 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 2/39 [00:00<00:05,  6.28it/s]\u001b[A\n",
            "  8% 3/39 [00:00<00:08,  4.48it/s]\u001b[A\n",
            " 10% 4/39 [00:00<00:08,  4.02it/s]\u001b[A\n",
            " 13% 5/39 [00:01<00:08,  3.80it/s]\u001b[A\n",
            " 15% 6/39 [00:01<00:09,  3.58it/s]\u001b[A\n",
            " 18% 7/39 [00:01<00:09,  3.44it/s]\u001b[A\n",
            " 21% 8/39 [00:02<00:09,  3.34it/s]\u001b[A\n",
            " 23% 9/39 [00:02<00:09,  3.29it/s]\u001b[A\n",
            " 26% 10/39 [00:02<00:08,  3.26it/s]\u001b[A\n",
            " 28% 11/39 [00:03<00:08,  3.23it/s]\u001b[A\n",
            " 31% 12/39 [00:03<00:08,  3.21it/s]\u001b[A\n",
            " 33% 13/39 [00:03<00:07,  3.33it/s]\u001b[A\n",
            " 36% 14/39 [00:04<00:07,  3.29it/s]\u001b[A\n",
            " 38% 15/39 [00:04<00:07,  3.25it/s]\u001b[A\n",
            " 41% 16/39 [00:04<00:07,  3.21it/s]\u001b[A\n",
            " 44% 17/39 [00:04<00:06,  3.20it/s]\u001b[A\n",
            " 46% 18/39 [00:05<00:06,  3.19it/s]\u001b[A\n",
            " 49% 19/39 [00:05<00:06,  3.19it/s]\u001b[A\n",
            " 51% 20/39 [00:05<00:05,  3.18it/s]\u001b[A\n",
            " 54% 21/39 [00:06<00:05,  3.26it/s]\u001b[A\n",
            " 56% 22/39 [00:06<00:05,  3.23it/s]\u001b[A\n",
            " 59% 23/39 [00:06<00:04,  3.23it/s]\u001b[A\n",
            " 62% 24/39 [00:07<00:04,  3.18it/s]\u001b[A\n",
            " 64% 25/39 [00:07<00:04,  3.15it/s]\u001b[A\n",
            " 67% 26/39 [00:07<00:04,  3.14it/s]\u001b[A\n",
            " 69% 27/39 [00:08<00:03,  3.12it/s]\u001b[A\n",
            " 72% 28/39 [00:08<00:03,  3.12it/s]\u001b[A\n",
            " 74% 29/39 [00:08<00:03,  3.12it/s]\u001b[A\n",
            " 77% 30/39 [00:09<00:02,  3.12it/s]\u001b[A\n",
            " 79% 31/39 [00:09<00:02,  3.10it/s]\u001b[A\n",
            " 82% 32/39 [00:09<00:02,  3.07it/s]\u001b[A\n",
            " 85% 33/39 [00:10<00:01,  3.07it/s]\u001b[A\n",
            " 87% 34/39 [00:10<00:01,  3.06it/s]\u001b[A\n",
            " 90% 35/39 [00:10<00:01,  3.11it/s]\u001b[A\n",
            " 92% 36/39 [00:10<00:00,  3.19it/s]\u001b[A\n",
            " 95% 37/39 [00:11<00:00,  3.29it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.36569979786872864, 'eval_bleu': 0.0, 'eval_accuracy': 0.6732, 'eval_gen_len': 2.0, 'eval_runtime': 12.0509, 'eval_samples_per_second': 25.392, 'eval_steps_per_second': 3.236, 'epoch': 2.89}\n",
            " 58% 3250/5625 [29:52<18:03,  2.19it/s]\n",
            "100% 39/39 [00:11<00:00,  3.26it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2656] 2023-02-14 14:00:12,160 >> Saving model checkpoint to out/beer_reviews/t5_v1_1_freeze_base/checkpoint-3250\n",
            "[INFO|configuration_utils.py:447] 2023-02-14 14:00:12,161 >> Configuration saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-3250/config.json\n",
            "[INFO|modeling_utils.py:1624] 2023-02-14 14:00:15,590 >> Model weights saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-3250/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2023-02-14 14:00:15,590 >> tokenizer config file saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-3250/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2023-02-14 14:00:15,591 >> Special tokens file saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-3250/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:187] 2023-02-14 14:00:15,632 >> Copy vocab file to out/beer_reviews/t5_v1_1_freeze_base/checkpoint-3250/spiece.model\n",
            "[INFO|trainer.py:2734] 2023-02-14 14:00:21,904 >> Deleting older checkpoint [out/beer_reviews/t5_v1_1_freeze_base/checkpoint-2000] due to args.save_total_limit\n",
            "{'loss': 0.3674, 'learning_rate': 2.0666666666666666e-05, 'epoch': 2.93}\n",
            "{'loss': 0.3459, 'learning_rate': 1.9777777777777778e-05, 'epoch': 3.02}\n",
            "{'loss': 0.3496, 'learning_rate': 1.888888888888889e-05, 'epoch': 3.11}\n",
            " 62% 3500/5625 [31:59<16:44,  2.12it/s][INFO|trainer.py:2907] 2023-02-14 14:02:18,738 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2023-02-14 14:02:18,739 >>   Num examples = 306\n",
            "[INFO|trainer.py:2912] 2023-02-14 14:02:18,739 >>   Batch size = 8\n",
            "\n",
            "  0% 0/39 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 2/39 [00:00<00:05,  6.31it/s]\u001b[A\n",
            "  8% 3/39 [00:00<00:08,  4.47it/s]\u001b[A\n",
            " 10% 4/39 [00:00<00:08,  4.03it/s]\u001b[A\n",
            " 13% 5/39 [00:01<00:08,  3.79it/s]\u001b[A\n",
            " 15% 6/39 [00:01<00:09,  3.57it/s]\u001b[A\n",
            " 18% 7/39 [00:01<00:09,  3.43it/s]\u001b[A\n",
            " 21% 8/39 [00:02<00:09,  3.33it/s]\u001b[A\n",
            " 23% 9/39 [00:02<00:09,  3.29it/s]\u001b[A\n",
            " 26% 10/39 [00:02<00:08,  3.25it/s]\u001b[A\n",
            " 28% 11/39 [00:03<00:08,  3.23it/s]\u001b[A\n",
            " 31% 12/39 [00:03<00:08,  3.19it/s]\u001b[A\n",
            " 33% 13/39 [00:03<00:07,  3.29it/s]\u001b[A\n",
            " 36% 14/39 [00:04<00:07,  3.23it/s]\u001b[A\n",
            " 38% 15/39 [00:04<00:07,  3.20it/s]\u001b[A\n",
            " 41% 16/39 [00:04<00:07,  3.16it/s]\u001b[A\n",
            " 44% 17/39 [00:05<00:07,  3.13it/s]\u001b[A\n",
            " 46% 18/39 [00:05<00:06,  3.11it/s]\u001b[A\n",
            " 49% 19/39 [00:05<00:06,  3.09it/s]\u001b[A\n",
            " 51% 20/39 [00:05<00:06,  3.08it/s]\u001b[A\n",
            " 54% 21/39 [00:06<00:05,  3.13it/s]\u001b[A\n",
            " 56% 22/39 [00:06<00:05,  3.13it/s]\u001b[A\n",
            " 59% 23/39 [00:06<00:05,  3.15it/s]\u001b[A\n",
            " 62% 24/39 [00:07<00:04,  3.16it/s]\u001b[A\n",
            " 64% 25/39 [00:07<00:04,  3.16it/s]\u001b[A\n",
            " 67% 26/39 [00:07<00:04,  3.16it/s]\u001b[A\n",
            " 69% 27/39 [00:08<00:03,  3.17it/s]\u001b[A\n",
            " 72% 28/39 [00:08<00:03,  3.18it/s]\u001b[A\n",
            " 74% 29/39 [00:08<00:03,  3.17it/s]\u001b[A\n",
            " 77% 30/39 [00:09<00:02,  3.17it/s]\u001b[A\n",
            " 79% 31/39 [00:09<00:02,  3.18it/s]\u001b[A\n",
            " 82% 32/39 [00:09<00:02,  3.18it/s]\u001b[A\n",
            " 85% 33/39 [00:10<00:01,  3.17it/s]\u001b[A\n",
            " 87% 34/39 [00:10<00:01,  3.17it/s]\u001b[A\n",
            " 90% 35/39 [00:10<00:01,  3.18it/s]\u001b[A\n",
            " 92% 36/39 [00:10<00:00,  3.25it/s]\u001b[A\n",
            " 95% 37/39 [00:11<00:00,  3.34it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.361651748418808, 'eval_bleu': 0.0, 'eval_accuracy': 0.6961, 'eval_gen_len': 2.0, 'eval_runtime': 12.0093, 'eval_samples_per_second': 25.48, 'eval_steps_per_second': 3.247, 'epoch': 3.11}\n",
            " 62% 3500/5625 [32:11<16:44,  2.12it/s]\n",
            "100% 39/39 [00:11<00:00,  3.30it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2656] 2023-02-14 14:02:30,749 >> Saving model checkpoint to out/beer_reviews/t5_v1_1_freeze_base/checkpoint-3500\n",
            "[INFO|configuration_utils.py:447] 2023-02-14 14:02:30,750 >> Configuration saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-3500/config.json\n",
            "[INFO|modeling_utils.py:1624] 2023-02-14 14:02:34,181 >> Model weights saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-3500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2023-02-14 14:02:34,182 >> tokenizer config file saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-3500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2023-02-14 14:02:34,183 >> Special tokens file saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-3500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:187] 2023-02-14 14:02:34,227 >> Copy vocab file to out/beer_reviews/t5_v1_1_freeze_base/checkpoint-3500/spiece.model\n",
            "[INFO|trainer.py:2734] 2023-02-14 14:02:40,497 >> Deleting older checkpoint [out/beer_reviews/t5_v1_1_freeze_base/checkpoint-2250] due to args.save_total_limit\n",
            "{'loss': 0.3861, 'learning_rate': 1.8e-05, 'epoch': 3.2}\n",
            "{'loss': 0.3452, 'learning_rate': 1.7111111111111112e-05, 'epoch': 3.29}\n",
            " 67% 3750/5625 [34:17<14:48,  2.11it/s][INFO|trainer.py:2907] 2023-02-14 14:04:36,779 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2023-02-14 14:04:36,779 >>   Num examples = 306\n",
            "[INFO|trainer.py:2912] 2023-02-14 14:04:36,779 >>   Batch size = 8\n",
            "\n",
            "  0% 0/39 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 2/39 [00:00<00:05,  6.34it/s]\u001b[A\n",
            "  8% 3/39 [00:00<00:08,  4.39it/s]\u001b[A\n",
            " 10% 4/39 [00:00<00:08,  3.92it/s]\u001b[A\n",
            " 13% 5/39 [00:01<00:09,  3.70it/s]\u001b[A\n",
            " 15% 6/39 [00:01<00:09,  3.46it/s]\u001b[A\n",
            " 18% 7/39 [00:01<00:09,  3.34it/s]\u001b[A\n",
            " 21% 8/39 [00:02<00:09,  3.25it/s]\u001b[A\n",
            " 23% 9/39 [00:02<00:09,  3.20it/s]\u001b[A\n",
            " 26% 10/39 [00:02<00:09,  3.15it/s]\u001b[A\n",
            " 28% 11/39 [00:03<00:09,  3.11it/s]\u001b[A\n",
            " 31% 12/39 [00:03<00:08,  3.10it/s]\u001b[A\n",
            " 33% 13/39 [00:03<00:08,  3.20it/s]\u001b[A\n",
            " 36% 14/39 [00:04<00:07,  3.16it/s]\u001b[A\n",
            " 38% 15/39 [00:04<00:07,  3.17it/s]\u001b[A\n",
            " 41% 16/39 [00:04<00:07,  3.16it/s]\u001b[A\n",
            " 44% 17/39 [00:05<00:06,  3.16it/s]\u001b[A\n",
            " 46% 18/39 [00:05<00:06,  3.17it/s]\u001b[A\n",
            " 49% 19/39 [00:05<00:06,  3.17it/s]\u001b[A\n",
            " 51% 20/39 [00:06<00:05,  3.18it/s]\u001b[A\n",
            " 54% 21/39 [00:06<00:05,  3.24it/s]\u001b[A\n",
            " 56% 22/39 [00:06<00:05,  3.22it/s]\u001b[A\n",
            " 59% 23/39 [00:06<00:04,  3.21it/s]\u001b[A\n",
            " 62% 24/39 [00:07<00:04,  3.20it/s]\u001b[A\n",
            " 64% 25/39 [00:07<00:04,  3.20it/s]\u001b[A\n",
            " 67% 26/39 [00:07<00:04,  3.19it/s]\u001b[A\n",
            " 69% 27/39 [00:08<00:03,  3.19it/s]\u001b[A\n",
            " 72% 28/39 [00:08<00:03,  3.19it/s]\u001b[A\n",
            " 74% 29/39 [00:08<00:03,  3.19it/s]\u001b[A\n",
            " 77% 30/39 [00:09<00:02,  3.18it/s]\u001b[A\n",
            " 79% 31/39 [00:09<00:02,  3.18it/s]\u001b[A\n",
            " 82% 32/39 [00:09<00:02,  3.19it/s]\u001b[A\n",
            " 85% 33/39 [00:10<00:01,  3.18it/s]\u001b[A\n",
            " 87% 34/39 [00:10<00:01,  3.18it/s]\u001b[A\n",
            " 90% 35/39 [00:10<00:01,  3.19it/s]\u001b[A\n",
            " 92% 36/39 [00:11<00:00,  3.26it/s]\u001b[A\n",
            " 95% 37/39 [00:11<00:00,  3.33it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.3703864514827728, 'eval_bleu': 0.0, 'eval_accuracy': 0.7059, 'eval_gen_len': 2.0, 'eval_runtime': 12.0494, 'eval_samples_per_second': 25.395, 'eval_steps_per_second': 3.237, 'epoch': 3.33}\n",
            " 67% 3750/5625 [34:29<14:48,  2.11it/s]\n",
            "100% 39/39 [00:11<00:00,  3.29it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2656] 2023-02-14 14:04:48,829 >> Saving model checkpoint to out/beer_reviews/t5_v1_1_freeze_base/checkpoint-3750\n",
            "[INFO|configuration_utils.py:447] 2023-02-14 14:04:48,830 >> Configuration saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-3750/config.json\n",
            "[INFO|modeling_utils.py:1624] 2023-02-14 14:04:52,265 >> Model weights saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-3750/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2023-02-14 14:04:52,266 >> tokenizer config file saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-3750/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2023-02-14 14:04:52,267 >> Special tokens file saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-3750/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:187] 2023-02-14 14:04:52,379 >> Copy vocab file to out/beer_reviews/t5_v1_1_freeze_base/checkpoint-3750/spiece.model\n",
            "[INFO|trainer.py:2734] 2023-02-14 14:04:58,578 >> Deleting older checkpoint [out/beer_reviews/t5_v1_1_freeze_base/checkpoint-2500] due to args.save_total_limit\n",
            "{'loss': 0.3383, 'learning_rate': 1.6222222222222223e-05, 'epoch': 3.38}\n",
            "{'loss': 0.3508, 'learning_rate': 1.5333333333333334e-05, 'epoch': 3.47}\n",
            "{'loss': 0.3647, 'learning_rate': 1.4444444444444444e-05, 'epoch': 3.56}\n",
            " 71% 4000/5625 [36:35<12:45,  2.12it/s][INFO|trainer.py:2907] 2023-02-14 14:06:55,133 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2023-02-14 14:06:55,133 >>   Num examples = 306\n",
            "[INFO|trainer.py:2912] 2023-02-14 14:06:55,133 >>   Batch size = 8\n",
            "\n",
            "  0% 0/39 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 2/39 [00:00<00:06,  6.06it/s]\u001b[A\n",
            "  8% 3/39 [00:00<00:08,  4.30it/s]\u001b[A\n",
            " 10% 4/39 [00:00<00:09,  3.87it/s]\u001b[A\n",
            " 13% 5/39 [00:01<00:09,  3.65it/s]\u001b[A\n",
            " 15% 6/39 [00:01<00:09,  3.50it/s]\u001b[A\n",
            " 18% 7/39 [00:01<00:09,  3.40it/s]\u001b[A\n",
            " 21% 8/39 [00:02<00:09,  3.32it/s]\u001b[A\n",
            " 23% 9/39 [00:02<00:09,  3.28it/s]\u001b[A\n",
            " 26% 10/39 [00:02<00:08,  3.24it/s]\u001b[A\n",
            " 28% 11/39 [00:03<00:08,  3.23it/s]\u001b[A\n",
            " 31% 12/39 [00:03<00:08,  3.20it/s]\u001b[A\n",
            " 33% 13/39 [00:03<00:07,  3.33it/s]\u001b[A\n",
            " 36% 14/39 [00:04<00:07,  3.28it/s]\u001b[A\n",
            " 38% 15/39 [00:04<00:07,  3.24it/s]\u001b[A\n",
            " 41% 16/39 [00:04<00:07,  3.23it/s]\u001b[A\n",
            " 44% 17/39 [00:04<00:06,  3.21it/s]\u001b[A\n",
            " 46% 18/39 [00:05<00:06,  3.17it/s]\u001b[A\n",
            " 49% 19/39 [00:05<00:06,  3.17it/s]\u001b[A\n",
            " 51% 20/39 [00:05<00:06,  3.17it/s]\u001b[A\n",
            " 54% 21/39 [00:06<00:05,  3.25it/s]\u001b[A\n",
            " 56% 22/39 [00:06<00:05,  3.22it/s]\u001b[A\n",
            " 59% 23/39 [00:06<00:04,  3.21it/s]\u001b[A\n",
            " 62% 24/39 [00:07<00:04,  3.19it/s]\u001b[A\n",
            " 64% 25/39 [00:07<00:04,  3.19it/s]\u001b[A\n",
            " 67% 26/39 [00:07<00:04,  3.18it/s]\u001b[A\n",
            " 69% 27/39 [00:08<00:03,  3.19it/s]\u001b[A\n",
            " 72% 28/39 [00:08<00:03,  3.20it/s]\u001b[A\n",
            " 74% 29/39 [00:08<00:03,  3.19it/s]\u001b[A\n",
            " 77% 30/39 [00:09<00:02,  3.18it/s]\u001b[A\n",
            " 79% 31/39 [00:09<00:02,  3.18it/s]\u001b[A\n",
            " 82% 32/39 [00:09<00:02,  3.18it/s]\u001b[A\n",
            " 85% 33/39 [00:10<00:01,  3.19it/s]\u001b[A\n",
            " 87% 34/39 [00:10<00:01,  3.18it/s]\u001b[A\n",
            " 90% 35/39 [00:10<00:01,  3.19it/s]\u001b[A\n",
            " 92% 36/39 [00:10<00:00,  3.25it/s]\u001b[A\n",
            " 95% 37/39 [00:11<00:00,  3.30it/s]\u001b[A\n",
            " 97% 38/39 [00:11<00:00,  3.24it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.3521196246147156, 'eval_bleu': 0.0, 'eval_accuracy': 0.7059, 'eval_gen_len': 2.0, 'eval_runtime': 12.0371, 'eval_samples_per_second': 25.421, 'eval_steps_per_second': 3.24, 'epoch': 3.56}\n",
            " 71% 4000/5625 [36:47<12:45,  2.12it/s]\n",
            "100% 39/39 [00:11<00:00,  3.99it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2656] 2023-02-14 14:07:07,171 >> Saving model checkpoint to out/beer_reviews/t5_v1_1_freeze_base/checkpoint-4000\n",
            "[INFO|configuration_utils.py:447] 2023-02-14 14:07:07,173 >> Configuration saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-4000/config.json\n",
            "[INFO|modeling_utils.py:1624] 2023-02-14 14:07:10,699 >> Model weights saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-4000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2023-02-14 14:07:10,700 >> tokenizer config file saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-4000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2023-02-14 14:07:10,700 >> Special tokens file saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-4000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:187] 2023-02-14 14:07:10,743 >> Copy vocab file to out/beer_reviews/t5_v1_1_freeze_base/checkpoint-4000/spiece.model\n",
            "[INFO|trainer.py:2734] 2023-02-14 14:07:17,014 >> Deleting older checkpoint [out/beer_reviews/t5_v1_1_freeze_base/checkpoint-2750] due to args.save_total_limit\n",
            "{'loss': 0.3583, 'learning_rate': 1.3555555555555557e-05, 'epoch': 3.64}\n",
            "{'loss': 0.3322, 'learning_rate': 1.2666666666666668e-05, 'epoch': 3.73}\n",
            " 76% 4250/5625 [38:53<10:50,  2.11it/s][INFO|trainer.py:2907] 2023-02-14 14:09:13,431 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2023-02-14 14:09:13,431 >>   Num examples = 306\n",
            "[INFO|trainer.py:2912] 2023-02-14 14:09:13,431 >>   Batch size = 8\n",
            "\n",
            "  0% 0/39 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 2/39 [00:00<00:05,  6.30it/s]\u001b[A\n",
            "  8% 3/39 [00:00<00:08,  4.45it/s]\u001b[A\n",
            " 10% 4/39 [00:00<00:08,  4.01it/s]\u001b[A\n",
            " 13% 5/39 [00:01<00:08,  3.79it/s]\u001b[A\n",
            " 15% 6/39 [00:01<00:09,  3.57it/s]\u001b[A\n",
            " 18% 7/39 [00:01<00:09,  3.43it/s]\u001b[A\n",
            " 21% 8/39 [00:02<00:09,  3.32it/s]\u001b[A\n",
            " 23% 9/39 [00:02<00:09,  3.28it/s]\u001b[A\n",
            " 26% 10/39 [00:02<00:08,  3.25it/s]\u001b[A\n",
            " 28% 11/39 [00:03<00:08,  3.22it/s]\u001b[A\n",
            " 31% 12/39 [00:03<00:08,  3.21it/s]\u001b[A\n",
            " 33% 13/39 [00:03<00:07,  3.34it/s]\u001b[A\n",
            " 36% 14/39 [00:04<00:07,  3.29it/s]\u001b[A\n",
            " 38% 15/39 [00:04<00:07,  3.26it/s]\u001b[A\n",
            " 41% 16/39 [00:04<00:07,  3.24it/s]\u001b[A\n",
            " 44% 17/39 [00:04<00:06,  3.22it/s]\u001b[A\n",
            " 46% 18/39 [00:05<00:06,  3.21it/s]\u001b[A\n",
            " 49% 19/39 [00:05<00:06,  3.19it/s]\u001b[A\n",
            " 51% 20/39 [00:05<00:05,  3.18it/s]\u001b[A\n",
            " 54% 21/39 [00:06<00:05,  3.26it/s]\u001b[A\n",
            " 56% 22/39 [00:06<00:05,  3.21it/s]\u001b[A\n",
            " 59% 23/39 [00:06<00:04,  3.21it/s]\u001b[A\n",
            " 62% 24/39 [00:07<00:04,  3.21it/s]\u001b[A\n",
            " 64% 25/39 [00:07<00:04,  3.19it/s]\u001b[A\n",
            " 67% 26/39 [00:07<00:04,  3.19it/s]\u001b[A\n",
            " 69% 27/39 [00:08<00:03,  3.15it/s]\u001b[A\n",
            " 72% 28/39 [00:08<00:03,  3.14it/s]\u001b[A\n",
            " 74% 29/39 [00:08<00:03,  3.14it/s]\u001b[A\n",
            " 77% 30/39 [00:09<00:02,  3.12it/s]\u001b[A\n",
            " 79% 31/39 [00:09<00:02,  3.11it/s]\u001b[A\n",
            " 82% 32/39 [00:09<00:02,  3.12it/s]\u001b[A\n",
            " 85% 33/39 [00:10<00:01,  3.11it/s]\u001b[A\n",
            " 87% 34/39 [00:10<00:01,  3.10it/s]\u001b[A\n",
            " 90% 35/39 [00:10<00:01,  3.10it/s]\u001b[A\n",
            " 92% 36/39 [00:10<00:00,  3.14it/s]\u001b[A\n",
            " 95% 37/39 [00:11<00:00,  3.21it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.3256322741508484, 'eval_bleu': 0.0, 'eval_accuracy': 0.7092, 'eval_gen_len': 2.0, 'eval_runtime': 12.0412, 'eval_samples_per_second': 25.413, 'eval_steps_per_second': 3.239, 'epoch': 3.78}\n",
            " 76% 4250/5625 [39:05<10:50,  2.11it/s]\n",
            "100% 39/39 [00:11<00:00,  3.20it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2656] 2023-02-14 14:09:25,474 >> Saving model checkpoint to out/beer_reviews/t5_v1_1_freeze_base/checkpoint-4250\n",
            "[INFO|configuration_utils.py:447] 2023-02-14 14:09:25,474 >> Configuration saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-4250/config.json\n",
            "[INFO|modeling_utils.py:1624] 2023-02-14 14:09:28,918 >> Model weights saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-4250/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2023-02-14 14:09:28,919 >> tokenizer config file saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-4250/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2023-02-14 14:09:28,920 >> Special tokens file saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-4250/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:187] 2023-02-14 14:09:28,968 >> Copy vocab file to out/beer_reviews/t5_v1_1_freeze_base/checkpoint-4250/spiece.model\n",
            "[INFO|trainer.py:2734] 2023-02-14 14:09:35,243 >> Deleting older checkpoint [out/beer_reviews/t5_v1_1_freeze_base/checkpoint-3000] due to args.save_total_limit\n",
            "{'loss': 0.3513, 'learning_rate': 1.1777777777777778e-05, 'epoch': 3.82}\n",
            "{'loss': 0.3307, 'learning_rate': 1.088888888888889e-05, 'epoch': 3.91}\n",
            "{'loss': 0.3208, 'learning_rate': 1e-05, 'epoch': 4.0}\n",
            " 80% 4500/5625 [41:12<08:29,  2.21it/s][INFO|trainer.py:2907] 2023-02-14 14:11:31,821 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2023-02-14 14:11:31,821 >>   Num examples = 306\n",
            "[INFO|trainer.py:2912] 2023-02-14 14:11:31,821 >>   Batch size = 8\n",
            "\n",
            "  0% 0/39 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 2/39 [00:00<00:05,  6.38it/s]\u001b[A\n",
            "  8% 3/39 [00:00<00:08,  4.50it/s]\u001b[A\n",
            " 10% 4/39 [00:00<00:08,  4.04it/s]\u001b[A\n",
            " 13% 5/39 [00:01<00:08,  3.81it/s]\u001b[A\n",
            " 15% 6/39 [00:01<00:09,  3.59it/s]\u001b[A\n",
            " 18% 7/39 [00:01<00:09,  3.44it/s]\u001b[A\n",
            " 21% 8/39 [00:02<00:09,  3.34it/s]\u001b[A\n",
            " 23% 9/39 [00:02<00:09,  3.29it/s]\u001b[A\n",
            " 26% 10/39 [00:02<00:08,  3.26it/s]\u001b[A\n",
            " 28% 11/39 [00:03<00:08,  3.24it/s]\u001b[A\n",
            " 31% 12/39 [00:03<00:08,  3.22it/s]\u001b[A\n",
            " 33% 13/39 [00:03<00:07,  3.35it/s]\u001b[A\n",
            " 36% 14/39 [00:03<00:07,  3.30it/s]\u001b[A\n",
            " 38% 15/39 [00:04<00:07,  3.23it/s]\u001b[A\n",
            " 41% 16/39 [00:04<00:07,  3.20it/s]\u001b[A\n",
            " 44% 17/39 [00:04<00:06,  3.17it/s]\u001b[A\n",
            " 46% 18/39 [00:05<00:06,  3.13it/s]\u001b[A\n",
            " 49% 19/39 [00:05<00:06,  3.12it/s]\u001b[A\n",
            " 51% 20/39 [00:05<00:06,  3.11it/s]\u001b[A\n",
            " 54% 21/39 [00:06<00:05,  3.18it/s]\u001b[A\n",
            " 56% 22/39 [00:06<00:05,  3.14it/s]\u001b[A\n",
            " 59% 23/39 [00:06<00:05,  3.12it/s]\u001b[A\n",
            " 62% 24/39 [00:07<00:04,  3.10it/s]\u001b[A\n",
            " 64% 25/39 [00:07<00:04,  3.08it/s]\u001b[A\n",
            " 67% 26/39 [00:07<00:04,  3.11it/s]\u001b[A\n",
            " 69% 27/39 [00:08<00:03,  3.13it/s]\u001b[A\n",
            " 72% 28/39 [00:08<00:03,  3.14it/s]\u001b[A\n",
            " 74% 29/39 [00:08<00:03,  3.15it/s]\u001b[A\n",
            " 77% 30/39 [00:09<00:02,  3.16it/s]\u001b[A\n",
            " 79% 31/39 [00:09<00:02,  3.16it/s]\u001b[A\n",
            " 82% 32/39 [00:09<00:02,  3.16it/s]\u001b[A\n",
            " 85% 33/39 [00:10<00:01,  3.17it/s]\u001b[A\n",
            " 87% 34/39 [00:10<00:01,  3.16it/s]\u001b[A\n",
            " 90% 35/39 [00:10<00:01,  3.18it/s]\u001b[A\n",
            " 92% 36/39 [00:10<00:00,  3.25it/s]\u001b[A\n",
            " 95% 37/39 [00:11<00:00,  3.33it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.3617289960384369, 'eval_bleu': 0.0, 'eval_accuracy': 0.6928, 'eval_gen_len': 2.0, 'eval_runtime': 12.0143, 'eval_samples_per_second': 25.47, 'eval_steps_per_second': 3.246, 'epoch': 4.0}\n",
            " 80% 4500/5625 [41:24<08:29,  2.21it/s]\n",
            "100% 39/39 [00:11<00:00,  3.29it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2656] 2023-02-14 14:11:43,837 >> Saving model checkpoint to out/beer_reviews/t5_v1_1_freeze_base/checkpoint-4500\n",
            "[INFO|configuration_utils.py:447] 2023-02-14 14:11:43,837 >> Configuration saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-4500/config.json\n",
            "[INFO|modeling_utils.py:1624] 2023-02-14 14:11:47,260 >> Model weights saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-4500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2023-02-14 14:11:47,261 >> tokenizer config file saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-4500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2023-02-14 14:11:47,261 >> Special tokens file saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-4500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:187] 2023-02-14 14:11:47,314 >> Copy vocab file to out/beer_reviews/t5_v1_1_freeze_base/checkpoint-4500/spiece.model\n",
            "[INFO|trainer.py:2734] 2023-02-14 14:11:53,562 >> Deleting older checkpoint [out/beer_reviews/t5_v1_1_freeze_base/checkpoint-3250] due to args.save_total_limit\n",
            "{'loss': 0.3173, 'learning_rate': 9.111111111111112e-06, 'epoch': 4.09}\n",
            "{'loss': 0.3382, 'learning_rate': 8.222222222222223e-06, 'epoch': 4.18}\n",
            " 84% 4750/5625 [43:29<06:50,  2.13it/s][INFO|trainer.py:2907] 2023-02-14 14:13:49,402 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2023-02-14 14:13:49,402 >>   Num examples = 306\n",
            "[INFO|trainer.py:2912] 2023-02-14 14:13:49,402 >>   Batch size = 8\n",
            "\n",
            "  0% 0/39 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 2/39 [00:00<00:05,  6.26it/s]\u001b[A\n",
            "  8% 3/39 [00:00<00:08,  4.49it/s]\u001b[A\n",
            " 10% 4/39 [00:00<00:08,  4.02it/s]\u001b[A\n",
            " 13% 5/39 [00:01<00:09,  3.76it/s]\u001b[A\n",
            " 15% 6/39 [00:01<00:09,  3.53it/s]\u001b[A\n",
            " 18% 7/39 [00:01<00:09,  3.38it/s]\u001b[A\n",
            " 21% 8/39 [00:02<00:09,  3.27it/s]\u001b[A\n",
            " 23% 9/39 [00:02<00:09,  3.21it/s]\u001b[A\n",
            " 26% 10/39 [00:02<00:09,  3.17it/s]\u001b[A\n",
            " 28% 11/39 [00:03<00:08,  3.13it/s]\u001b[A\n",
            " 31% 12/39 [00:03<00:08,  3.12it/s]\u001b[A\n",
            " 33% 13/39 [00:03<00:08,  3.22it/s]\u001b[A\n",
            " 36% 14/39 [00:04<00:07,  3.16it/s]\u001b[A\n",
            " 38% 15/39 [00:04<00:07,  3.12it/s]\u001b[A\n",
            " 41% 16/39 [00:04<00:07,  3.10it/s]\u001b[A\n",
            " 44% 17/39 [00:05<00:07,  3.12it/s]\u001b[A\n",
            " 46% 18/39 [00:05<00:06,  3.13it/s]\u001b[A\n",
            " 49% 19/39 [00:05<00:06,  3.15it/s]\u001b[A\n",
            " 51% 20/39 [00:06<00:06,  3.16it/s]\u001b[A\n",
            " 54% 21/39 [00:06<00:05,  3.24it/s]\u001b[A\n",
            " 56% 22/39 [00:06<00:05,  3.22it/s]\u001b[A\n",
            " 59% 23/39 [00:06<00:04,  3.21it/s]\u001b[A\n",
            " 62% 24/39 [00:07<00:04,  3.21it/s]\u001b[A\n",
            " 64% 25/39 [00:07<00:04,  3.21it/s]\u001b[A\n",
            " 67% 26/39 [00:07<00:04,  3.20it/s]\u001b[A\n",
            " 69% 27/39 [00:08<00:03,  3.20it/s]\u001b[A\n",
            " 72% 28/39 [00:08<00:03,  3.20it/s]\u001b[A\n",
            " 74% 29/39 [00:08<00:03,  3.19it/s]\u001b[A\n",
            " 77% 30/39 [00:09<00:02,  3.18it/s]\u001b[A\n",
            " 79% 31/39 [00:09<00:02,  3.17it/s]\u001b[A\n",
            " 82% 32/39 [00:09<00:02,  3.17it/s]\u001b[A\n",
            " 85% 33/39 [00:10<00:01,  3.16it/s]\u001b[A\n",
            " 87% 34/39 [00:10<00:01,  3.17it/s]\u001b[A\n",
            " 90% 35/39 [00:10<00:01,  3.18it/s]\u001b[A\n",
            " 92% 36/39 [00:11<00:00,  3.26it/s]\u001b[A\n",
            " 95% 37/39 [00:11<00:00,  3.33it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.33816438913345337, 'eval_bleu': 0.0, 'eval_accuracy': 0.7026, 'eval_gen_len': 2.0, 'eval_runtime': 12.0507, 'eval_samples_per_second': 25.393, 'eval_steps_per_second': 3.236, 'epoch': 4.22}\n",
            " 84% 4750/5625 [43:41<06:50,  2.13it/s]\n",
            "100% 39/39 [00:11<00:00,  3.30it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2656] 2023-02-14 14:14:01,454 >> Saving model checkpoint to out/beer_reviews/t5_v1_1_freeze_base/checkpoint-4750\n",
            "[INFO|configuration_utils.py:447] 2023-02-14 14:14:01,454 >> Configuration saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-4750/config.json\n",
            "[INFO|modeling_utils.py:1624] 2023-02-14 14:14:04,872 >> Model weights saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-4750/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2023-02-14 14:14:04,873 >> tokenizer config file saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-4750/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2023-02-14 14:14:04,873 >> Special tokens file saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-4750/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:187] 2023-02-14 14:14:04,942 >> Copy vocab file to out/beer_reviews/t5_v1_1_freeze_base/checkpoint-4750/spiece.model\n",
            "[INFO|trainer.py:2734] 2023-02-14 14:14:11,188 >> Deleting older checkpoint [out/beer_reviews/t5_v1_1_freeze_base/checkpoint-3500] due to args.save_total_limit\n",
            "{'loss': 0.3572, 'learning_rate': 7.333333333333334e-06, 'epoch': 4.27}\n",
            "{'loss': 0.3326, 'learning_rate': 6.4444444444444445e-06, 'epoch': 4.36}\n",
            "{'loss': 0.3355, 'learning_rate': 5.555555555555556e-06, 'epoch': 4.44}\n",
            " 89% 5000/5625 [45:48<04:59,  2.09it/s][INFO|trainer.py:2907] 2023-02-14 14:16:08,472 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2023-02-14 14:16:08,472 >>   Num examples = 306\n",
            "[INFO|trainer.py:2912] 2023-02-14 14:16:08,472 >>   Batch size = 8\n",
            "\n",
            "  0% 0/39 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 2/39 [00:00<00:06,  5.96it/s]\u001b[A\n",
            "  8% 3/39 [00:00<00:08,  4.20it/s]\u001b[A\n",
            " 10% 4/39 [00:00<00:09,  3.81it/s]\u001b[A\n",
            " 13% 5/39 [00:01<00:09,  3.62it/s]\u001b[A\n",
            " 15% 6/39 [00:01<00:09,  3.47it/s]\u001b[A\n",
            " 18% 7/39 [00:01<00:09,  3.36it/s]\u001b[A\n",
            " 21% 8/39 [00:02<00:09,  3.30it/s]\u001b[A\n",
            " 23% 9/39 [00:02<00:09,  3.26it/s]\u001b[A\n",
            " 26% 10/39 [00:02<00:08,  3.23it/s]\u001b[A\n",
            " 28% 11/39 [00:03<00:08,  3.21it/s]\u001b[A\n",
            " 31% 12/39 [00:03<00:08,  3.20it/s]\u001b[A\n",
            " 33% 13/39 [00:03<00:07,  3.33it/s]\u001b[A\n",
            " 36% 14/39 [00:04<00:07,  3.29it/s]\u001b[A\n",
            " 38% 15/39 [00:04<00:07,  3.26it/s]\u001b[A\n",
            " 41% 16/39 [00:04<00:07,  3.24it/s]\u001b[A\n",
            " 44% 17/39 [00:05<00:06,  3.21it/s]\u001b[A\n",
            " 46% 18/39 [00:05<00:06,  3.21it/s]\u001b[A\n",
            " 49% 19/39 [00:05<00:06,  3.19it/s]\u001b[A\n",
            " 51% 20/39 [00:05<00:05,  3.18it/s]\u001b[A\n",
            " 54% 21/39 [00:06<00:05,  3.23it/s]\u001b[A\n",
            " 56% 22/39 [00:06<00:05,  3.22it/s]\u001b[A\n",
            " 59% 23/39 [00:06<00:04,  3.21it/s]\u001b[A\n",
            " 62% 24/39 [00:07<00:04,  3.20it/s]\u001b[A\n",
            " 64% 25/39 [00:07<00:04,  3.20it/s]\u001b[A\n",
            " 67% 26/39 [00:07<00:04,  3.16it/s]\u001b[A\n",
            " 69% 27/39 [00:08<00:03,  3.17it/s]\u001b[A\n",
            " 72% 28/39 [00:08<00:03,  3.19it/s]\u001b[A\n",
            " 74% 29/39 [00:08<00:03,  3.18it/s]\u001b[A\n",
            " 77% 30/39 [00:09<00:02,  3.18it/s]\u001b[A\n",
            " 79% 31/39 [00:09<00:02,  3.18it/s]\u001b[A\n",
            " 82% 32/39 [00:09<00:02,  3.18it/s]\u001b[A\n",
            " 85% 33/39 [00:10<00:01,  3.18it/s]\u001b[A\n",
            " 87% 34/39 [00:10<00:01,  3.18it/s]\u001b[A\n",
            " 90% 35/39 [00:10<00:01,  3.19it/s]\u001b[A\n",
            " 92% 36/39 [00:10<00:00,  3.26it/s]\u001b[A\n",
            " 95% 37/39 [00:11<00:00,  3.33it/s]\u001b[A\n",
            " 97% 38/39 [00:11<00:00,  3.25it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.3296903371810913, 'eval_bleu': 0.0, 'eval_accuracy': 0.7092, 'eval_gen_len': 2.0, 'eval_runtime': 12.0416, 'eval_samples_per_second': 25.412, 'eval_steps_per_second': 3.239, 'epoch': 4.44}\n",
            " 89% 5000/5625 [46:00<04:59,  2.09it/s]\n",
            "100% 39/39 [00:11<00:00,  4.00it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2656] 2023-02-14 14:16:20,515 >> Saving model checkpoint to out/beer_reviews/t5_v1_1_freeze_base/checkpoint-5000\n",
            "[INFO|configuration_utils.py:447] 2023-02-14 14:16:20,516 >> Configuration saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-5000/config.json\n",
            "[INFO|modeling_utils.py:1624] 2023-02-14 14:16:24,043 >> Model weights saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-5000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2023-02-14 14:16:24,044 >> tokenizer config file saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-5000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2023-02-14 14:16:24,044 >> Special tokens file saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-5000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:187] 2023-02-14 14:16:24,094 >> Copy vocab file to out/beer_reviews/t5_v1_1_freeze_base/checkpoint-5000/spiece.model\n",
            "[INFO|trainer.py:2734] 2023-02-14 14:16:30,362 >> Deleting older checkpoint [out/beer_reviews/t5_v1_1_freeze_base/checkpoint-3750] due to args.save_total_limit\n",
            "{'loss': 0.3118, 'learning_rate': 4.666666666666667e-06, 'epoch': 4.53}\n",
            "{'loss': 0.3303, 'learning_rate': 3.777777777777778e-06, 'epoch': 4.62}\n",
            " 93% 5250/5625 [48:07<02:57,  2.11it/s][INFO|trainer.py:2907] 2023-02-14 14:18:27,566 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2023-02-14 14:18:27,566 >>   Num examples = 306\n",
            "[INFO|trainer.py:2912] 2023-02-14 14:18:27,566 >>   Batch size = 8\n",
            "\n",
            "  0% 0/39 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 2/39 [00:00<00:05,  6.32it/s]\u001b[A\n",
            "  8% 3/39 [00:00<00:07,  4.50it/s]\u001b[A\n",
            " 10% 4/39 [00:00<00:08,  4.04it/s]\u001b[A\n",
            " 13% 5/39 [00:01<00:08,  3.80it/s]\u001b[A\n",
            " 15% 6/39 [00:01<00:09,  3.57it/s]\u001b[A\n",
            " 18% 7/39 [00:01<00:09,  3.42it/s]\u001b[A\n",
            " 21% 8/39 [00:02<00:09,  3.34it/s]\u001b[A\n",
            " 23% 9/39 [00:02<00:09,  3.29it/s]\u001b[A\n",
            " 26% 10/39 [00:02<00:08,  3.26it/s]\u001b[A\n",
            " 28% 11/39 [00:03<00:08,  3.23it/s]\u001b[A\n",
            " 31% 12/39 [00:03<00:08,  3.22it/s]\u001b[A\n",
            " 33% 13/39 [00:03<00:07,  3.35it/s]\u001b[A\n",
            " 36% 14/39 [00:04<00:07,  3.29it/s]\u001b[A\n",
            " 38% 15/39 [00:04<00:07,  3.26it/s]\u001b[A\n",
            " 41% 16/39 [00:04<00:07,  3.24it/s]\u001b[A\n",
            " 44% 17/39 [00:04<00:06,  3.22it/s]\u001b[A\n",
            " 46% 18/39 [00:05<00:06,  3.21it/s]\u001b[A\n",
            " 49% 19/39 [00:05<00:06,  3.21it/s]\u001b[A\n",
            " 51% 20/39 [00:05<00:05,  3.19it/s]\u001b[A\n",
            " 54% 21/39 [00:06<00:05,  3.26it/s]\u001b[A\n",
            " 56% 22/39 [00:06<00:05,  3.25it/s]\u001b[A\n",
            " 59% 23/39 [00:06<00:04,  3.23it/s]\u001b[A\n",
            " 62% 24/39 [00:07<00:04,  3.20it/s]\u001b[A\n",
            " 64% 25/39 [00:07<00:04,  3.19it/s]\u001b[A\n",
            " 67% 26/39 [00:07<00:04,  3.18it/s]\u001b[A\n",
            " 69% 27/39 [00:08<00:03,  3.16it/s]\u001b[A\n",
            " 72% 28/39 [00:08<00:03,  3.15it/s]\u001b[A\n",
            " 74% 29/39 [00:08<00:03,  3.14it/s]\u001b[A\n",
            " 77% 30/39 [00:09<00:02,  3.10it/s]\u001b[A\n",
            " 79% 31/39 [00:09<00:02,  3.11it/s]\u001b[A\n",
            " 82% 32/39 [00:09<00:02,  3.10it/s]\u001b[A\n",
            " 85% 33/39 [00:10<00:01,  3.09it/s]\u001b[A\n",
            " 87% 34/39 [00:10<00:01,  3.07it/s]\u001b[A\n",
            " 90% 35/39 [00:10<00:01,  3.08it/s]\u001b[A\n",
            " 92% 36/39 [00:10<00:00,  3.15it/s]\u001b[A\n",
            " 95% 37/39 [00:11<00:00,  3.20it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.3462481200695038, 'eval_bleu': 0.0, 'eval_accuracy': 0.6928, 'eval_gen_len': 2.0, 'eval_runtime': 12.0252, 'eval_samples_per_second': 25.447, 'eval_steps_per_second': 3.243, 'epoch': 4.67}\n",
            " 93% 5250/5625 [48:19<02:57,  2.11it/s]\n",
            "100% 39/39 [00:11<00:00,  3.20it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2656] 2023-02-14 14:18:39,592 >> Saving model checkpoint to out/beer_reviews/t5_v1_1_freeze_base/checkpoint-5250\n",
            "[INFO|configuration_utils.py:447] 2023-02-14 14:18:39,593 >> Configuration saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-5250/config.json\n",
            "[INFO|modeling_utils.py:1624] 2023-02-14 14:18:43,032 >> Model weights saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-5250/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2023-02-14 14:18:43,033 >> tokenizer config file saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-5250/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2023-02-14 14:18:43,033 >> Special tokens file saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-5250/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:187] 2023-02-14 14:18:43,075 >> Copy vocab file to out/beer_reviews/t5_v1_1_freeze_base/checkpoint-5250/spiece.model\n",
            "[INFO|trainer.py:2734] 2023-02-14 14:18:49,348 >> Deleting older checkpoint [out/beer_reviews/t5_v1_1_freeze_base/checkpoint-4000] due to args.save_total_limit\n",
            "{'loss': 0.3135, 'learning_rate': 2.888888888888889e-06, 'epoch': 4.71}\n",
            "{'loss': 0.3195, 'learning_rate': 2.0000000000000003e-06, 'epoch': 4.8}\n",
            "{'loss': 0.3305, 'learning_rate': 1.1111111111111112e-06, 'epoch': 4.89}\n",
            " 98% 5500/5625 [50:26<00:57,  2.18it/s][INFO|trainer.py:2907] 2023-02-14 14:20:45,968 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2023-02-14 14:20:45,968 >>   Num examples = 306\n",
            "[INFO|trainer.py:2912] 2023-02-14 14:20:45,968 >>   Batch size = 8\n",
            "\n",
            "  0% 0/39 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 2/39 [00:00<00:05,  6.26it/s]\u001b[A\n",
            "  8% 3/39 [00:00<00:08,  4.46it/s]\u001b[A\n",
            " 10% 4/39 [00:00<00:08,  4.03it/s]\u001b[A\n",
            " 13% 5/39 [00:01<00:08,  3.79it/s]\u001b[A\n",
            " 15% 6/39 [00:01<00:09,  3.55it/s]\u001b[A\n",
            " 18% 7/39 [00:01<00:09,  3.41it/s]\u001b[A\n",
            " 21% 8/39 [00:02<00:09,  3.33it/s]\u001b[A\n",
            " 23% 9/39 [00:02<00:09,  3.27it/s]\u001b[A\n",
            " 26% 10/39 [00:02<00:08,  3.23it/s]\u001b[A\n",
            " 28% 11/39 [00:03<00:08,  3.21it/s]\u001b[A\n",
            " 31% 12/39 [00:03<00:08,  3.20it/s]\u001b[A\n",
            " 33% 13/39 [00:03<00:07,  3.34it/s]\u001b[A\n",
            " 36% 14/39 [00:04<00:07,  3.29it/s]\u001b[A\n",
            " 38% 15/39 [00:04<00:07,  3.21it/s]\u001b[A\n",
            " 41% 16/39 [00:04<00:07,  3.18it/s]\u001b[A\n",
            " 44% 17/39 [00:04<00:06,  3.17it/s]\u001b[A\n",
            " 46% 18/39 [00:05<00:06,  3.13it/s]\u001b[A\n",
            " 49% 19/39 [00:05<00:06,  3.12it/s]\u001b[A\n",
            " 51% 20/39 [00:05<00:06,  3.12it/s]\u001b[A\n",
            " 54% 21/39 [00:06<00:05,  3.17it/s]\u001b[A\n",
            " 56% 22/39 [00:06<00:05,  3.15it/s]\u001b[A\n",
            " 59% 23/39 [00:06<00:05,  3.12it/s]\u001b[A\n",
            " 62% 24/39 [00:07<00:04,  3.10it/s]\u001b[A\n",
            " 64% 25/39 [00:07<00:04,  3.09it/s]\u001b[A\n",
            " 67% 26/39 [00:07<00:04,  3.12it/s]\u001b[A\n",
            " 69% 27/39 [00:08<00:03,  3.13it/s]\u001b[A\n",
            " 72% 28/39 [00:08<00:03,  3.14it/s]\u001b[A\n",
            " 74% 29/39 [00:08<00:03,  3.16it/s]\u001b[A\n",
            " 77% 30/39 [00:09<00:02,  3.16it/s]\u001b[A\n",
            " 79% 31/39 [00:09<00:02,  3.16it/s]\u001b[A\n",
            " 82% 32/39 [00:09<00:02,  3.17it/s]\u001b[A\n",
            " 85% 33/39 [00:10<00:01,  3.16it/s]\u001b[A\n",
            " 87% 34/39 [00:10<00:01,  3.17it/s]\u001b[A\n",
            " 90% 35/39 [00:10<00:01,  3.17it/s]\u001b[A\n",
            " 92% 36/39 [00:11<00:00,  3.25it/s]\u001b[A\n",
            " 95% 37/39 [00:11<00:00,  3.32it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.3412187695503235, 'eval_bleu': 0.0, 'eval_accuracy': 0.6928, 'eval_gen_len': 2.0, 'eval_runtime': 12.0227, 'eval_samples_per_second': 25.452, 'eval_steps_per_second': 3.244, 'epoch': 4.89}\n",
            " 98% 5500/5625 [50:38<00:57,  2.18it/s]\n",
            "100% 39/39 [00:11<00:00,  3.29it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2656] 2023-02-14 14:20:57,991 >> Saving model checkpoint to out/beer_reviews/t5_v1_1_freeze_base/checkpoint-5500\n",
            "[INFO|configuration_utils.py:447] 2023-02-14 14:20:57,992 >> Configuration saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-5500/config.json\n",
            "[INFO|modeling_utils.py:1624] 2023-02-14 14:21:01,412 >> Model weights saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-5500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2023-02-14 14:21:01,413 >> tokenizer config file saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-5500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2023-02-14 14:21:01,413 >> Special tokens file saved in out/beer_reviews/t5_v1_1_freeze_base/checkpoint-5500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:187] 2023-02-14 14:21:01,459 >> Copy vocab file to out/beer_reviews/t5_v1_1_freeze_base/checkpoint-5500/spiece.model\n",
            "[INFO|trainer.py:2734] 2023-02-14 14:21:07,721 >> Deleting older checkpoint [out/beer_reviews/t5_v1_1_freeze_base/checkpoint-4500] due to args.save_total_limit\n",
            "{'loss': 0.3329, 'learning_rate': 2.2222222222222224e-07, 'epoch': 4.98}\n",
            "100% 5625/5625 [51:46<00:00,  2.24it/s][INFO|trainer.py:1852] 2023-02-14 14:22:06,406 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[INFO|trainer.py:1946] 2023-02-14 14:22:06,406 >> Loading best model from out/beer_reviews/t5_v1_1_freeze_base/checkpoint-4250 (score: 0.7092).\n",
            "{'train_runtime': 3111.8143, 'train_samples_per_second': 14.461, 'train_steps_per_second': 1.808, 'train_loss': 0.9980600355360243, 'epoch': 5.0}\n",
            "100% 5625/5625 [51:51<00:00,  1.81it/s]\n",
            "[INFO|trainer.py:2656] 2023-02-14 14:22:11,482 >> Saving model checkpoint to out/beer_reviews/t5_v1_1_freeze_base\n",
            "[INFO|configuration_utils.py:447] 2023-02-14 14:22:11,483 >> Configuration saved in out/beer_reviews/t5_v1_1_freeze_base/config.json\n",
            "[INFO|modeling_utils.py:1624] 2023-02-14 14:22:14,913 >> Model weights saved in out/beer_reviews/t5_v1_1_freeze_base/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2023-02-14 14:22:14,914 >> tokenizer config file saved in out/beer_reviews/t5_v1_1_freeze_base/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2023-02-14 14:22:14,914 >> Special tokens file saved in out/beer_reviews/t5_v1_1_freeze_base/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:187] 2023-02-14 14:22:14,990 >> Copy vocab file to out/beer_reviews/t5_v1_1_freeze_base/spiece.model\n",
            "***** train metrics *****\n",
            "  epoch                    =        5.0\n",
            "  train_loss               =     0.9981\n",
            "  train_runtime            = 0:51:51.81\n",
            "  train_samples            =       9000\n",
            "  train_samples_per_second =     14.461\n",
            "  train_steps_per_second   =      1.808\n",
            "INFO:__main__:*** Evaluate ***\n",
            "[INFO|trainer.py:2907] 2023-02-14 14:22:15,028 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2023-02-14 14:22:15,028 >>   Num examples = 306\n",
            "[INFO|trainer.py:2912] 2023-02-14 14:22:15,028 >>   Batch size = 8\n",
            "100% 39/39 [00:11<00:00,  3.37it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        5.0\n",
            "  eval_accuracy           =     0.7092\n",
            "  eval_bleu               =        0.0\n",
            "  eval_gen_len            =        2.0\n",
            "  eval_loss               =     0.3256\n",
            "  eval_runtime            = 0:00:11.95\n",
            "  eval_samples            =        306\n",
            "  eval_samples_per_second =     25.602\n",
            "  eval_steps_per_second   =      3.263\n",
            "INFO:__main__:*** Predict ***\n",
            "[INFO|trainer.py:2907] 2023-02-14 14:22:26,985 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:2909] 2023-02-14 14:22:26,985 >>   Num examples = 841\n",
            "[INFO|trainer.py:2912] 2023-02-14 14:22:26,985 >>   Batch size = 8\n",
            "100% 106/106 [00:32<00:00,  3.26it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.7717\n",
            "  predict_bleu               =        0.0\n",
            "  predict_gen_len            =        2.0\n",
            "  predict_loss               =      0.287\n",
            "  predict_runtime            = 0:00:32.78\n",
            "  predict_samples            =        841\n",
            "  predict_samples_per_second =     25.649\n",
            "  predict_steps_per_second   =      3.233\n",
            "[INFO|modelcard.py:444] 2023-02-14 14:22:59,948 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Translation', 'type': 'translation'}, 'metrics': [{'name': 'Bleu', 'type': 'bleu', 'value': 0.0}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.7092}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Score : 0.709"
      ],
      "metadata": {
        "id": "-I00rr9XlhqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Flan5-small**"
      ],
      "metadata": {
        "id": "xqmumFkzlm4M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python run_translation.py \\\n",
        "  --cache_dir .cache_training \\\n",
        "  --model_name_or_path \"google/flan-t5-small\" \\\n",
        "  --train_file data/s2s-training.json \\\n",
        "  --validation_file data/s2s-validation.json \\\n",
        "  --test_file data/s2s-production.json \\\n",
        "  --per_device_train_batch_size 8 \\\n",
        "  --per_device_eval_batch_size 8 \\\n",
        "  --source_lang \"text\" \\\n",
        "  --target_lang \"label\" \\\n",
        "  --source_prefix \"beer classification\" \\\n",
        "  --max_source_length 256 \\\n",
        "  --max_target_length 128 \\\n",
        "  --generation_max_length 128 \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --do_predict \\\n",
        "  --predict_with_generate \\\n",
        "  --max_eval_samples 2000 \\\n",
        "  --max_steps 2500 \\\n",
        "  --num_train_epochs 1 \\\n",
        "  --save_strategy steps \\\n",
        "  --save_steps 250 \\\n",
        "  --save_total_limit 5 \\\n",
        "  --logging_strategy steps \\\n",
        "  --logging_steps 100 \\\n",
        "  --eval_steps 250 \\\n",
        "  --evaluation_strategy steps \\\n",
        "  --metric_for_best_model accuracy \\\n",
        "  --greater_is_better True \\\n",
        "  --load_best_model_at_end True \\\n",
        "  --output_dir out/beer_reviews/google/flan-t5-small"
      ],
      "metadata": {
        "id": "mD58otJKn4CG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa211a86-8686-494b-eda4-acf967c7a1c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-14 14:23:06.874975: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-02-14 14:23:08.901089: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-14 14:23:08.901283: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-14 14:23:08.901312: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "INFO:__main__:Training/evaluation parameters Seq2SeqTrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=250,\n",
            "evaluation_strategy=steps,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "generation_max_length=128,\n",
            "generation_num_beams=None,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=passive,\n",
            "log_on_each_node=True,\n",
            "logging_dir=out/beer_reviews/google/flan-t5-small/runs/Feb14_14-23-12_0af4fb0df49d,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=100,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=2500,\n",
            "metric_for_best_model=accuracy,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=1.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=out/beer_reviews/google/flan-t5-small,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "predict_with_generate=True,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=out/beer_reviews/google/flan-t5-small,\n",
            "save_on_each_node=False,\n",
            "save_steps=250,\n",
            "save_strategy=steps,\n",
            "save_total_limit=5,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "WARNING:datasets.builder:Using custom data configuration default-baeafae2fbdfeb25\n",
            "INFO:datasets.info:Loading Dataset Infos from /usr/local/lib/python3.8/dist-packages/datasets/packaged_modules/json\n",
            "INFO:datasets.builder:Overwrite dataset info from restored data version.\n",
            "INFO:datasets.info:Loading Dataset info from .cache_training/json/default-baeafae2fbdfeb25/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51\n",
            "WARNING:datasets.builder:Found cached dataset json (/content/uczenie_glebokie/.cache_training/json/default-baeafae2fbdfeb25/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n",
            "INFO:datasets.info:Loading Dataset info from /content/uczenie_glebokie/.cache_training/json/default-baeafae2fbdfeb25/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51\n",
            "100% 3/3 [00:00<00:00, 51.62it/s]\n",
            "Downloading (…)lve/main/config.json: 100% 1.40k/1.40k [00:00<00:00, 214kB/s]\n",
            "[INFO|configuration_utils.py:653] 2023-02-14 14:23:13,276 >> loading configuration file config.json from cache at .cache_training/models--google--flan-t5-small/snapshots/9471d3bc4f85c9012776f03c4c00fdfe0d789a95/config.json\n",
            "[INFO|configuration_utils.py:705] 2023-02-14 14:23:13,285 >> Model config T5Config {\n",
            "  \"_name_or_path\": \"google/flan-t5-small\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 1024,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 8,\n",
            "  \"num_heads\": 6,\n",
            "  \"num_layers\": 8,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"transformers_version\": \"4.23.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "Downloading (…)okenizer_config.json: 100% 2.54k/2.54k [00:00<00:00, 411kB/s]\n",
            "Downloading (…)\"spiece.model\";: 100% 792k/792k [00:00<00:00, 14.4MB/s]\n",
            "Downloading (…)/main/tokenizer.json: 100% 2.42M/2.42M [00:00<00:00, 11.9MB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100% 2.20k/2.20k [00:00<00:00, 889kB/s]\n",
            "[INFO|tokenization_utils_base.py:1773] 2023-02-14 14:23:14,739 >> loading file spiece.model from cache at .cache_training/models--google--flan-t5-small/snapshots/9471d3bc4f85c9012776f03c4c00fdfe0d789a95/spiece.model\n",
            "[INFO|tokenization_utils_base.py:1773] 2023-02-14 14:23:14,739 >> loading file tokenizer.json from cache at .cache_training/models--google--flan-t5-small/snapshots/9471d3bc4f85c9012776f03c4c00fdfe0d789a95/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1773] 2023-02-14 14:23:14,739 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1773] 2023-02-14 14:23:14,739 >> loading file special_tokens_map.json from cache at .cache_training/models--google--flan-t5-small/snapshots/9471d3bc4f85c9012776f03c4c00fdfe0d789a95/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:1773] 2023-02-14 14:23:14,739 >> loading file tokenizer_config.json from cache at .cache_training/models--google--flan-t5-small/snapshots/9471d3bc4f85c9012776f03c4c00fdfe0d789a95/tokenizer_config.json\n",
            "Downloading (…)\"pytorch_model.bin\";: 100% 308M/308M [00:03<00:00, 82.6MB/s]\n",
            "[INFO|modeling_utils.py:2156] 2023-02-14 14:23:18,749 >> loading weights file pytorch_model.bin from cache at .cache_training/models--google--flan-t5-small/snapshots/9471d3bc4f85c9012776f03c4c00fdfe0d789a95/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:2606] 2023-02-14 14:23:19,820 >> All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
            "\n",
            "[INFO|modeling_utils.py:2614] 2023-02-14 14:23:19,820 >> All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at google/flan-t5-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
            "INFO:__main__:Using translation prefix: \"beer classification: \"\n",
            "Running tokenizer on train dataset:   0% 0/9 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /content/uczenie_glebokie/.cache_training/json/default-baeafae2fbdfeb25/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-f21015f61be3ef80.arrow\n",
            "Running tokenizer on train dataset: 100% 9/9 [00:04<00:00,  1.96ba/s]\n",
            "INFO:__main__:Set 139 samples for negative-class\n",
            "INFO:__main__:Set 139 samples for neutral-class\n",
            "INFO:__main__:Set 139 samples for positive-class\n",
            "Running tokenizer on validation dataset:   0% 0/1 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /content/uczenie_glebokie/.cache_training/json/default-baeafae2fbdfeb25/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-d8fab47312566c90.arrow\n",
            "Running tokenizer on validation dataset: 100% 1/1 [00:00<00:00,  8.48ba/s]\n",
            "Running tokenizer on prediction dataset:   0% 0/1 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /content/uczenie_glebokie/.cache_training/json/default-baeafae2fbdfeb25/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-2e7224c4ed7048f0.arrow\n",
            "Running tokenizer on prediction dataset: 100% 1/1 [00:00<00:00,  3.06ba/s]\n",
            "[INFO|trainer.py:503] 2023-02-14 14:23:32,532 >> max_steps is given, it will override any value given in num_train_epochs\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "[INFO|trainer.py:1607] 2023-02-14 14:23:32,540 >> ***** Running training *****\n",
            "[INFO|trainer.py:1608] 2023-02-14 14:23:32,540 >>   Num examples = 9000\n",
            "[INFO|trainer.py:1609] 2023-02-14 14:23:32,540 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1610] 2023-02-14 14:23:32,540 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1611] 2023-02-14 14:23:32,540 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:1612] 2023-02-14 14:23:32,541 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1613] 2023-02-14 14:23:32,541 >>   Total optimization steps = 2500\n",
            "  0% 0/2500 [00:00<?, ?it/s][WARNING|logging.py:281] 2023-02-14 14:23:32,557 >> You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "{'loss': 1.235, 'learning_rate': 4.8e-05, 'epoch': 0.09}\n",
            "{'loss': 0.4112, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.18}\n",
            " 10% 250/2500 [00:40<05:30,  6.80it/s][INFO|trainer.py:2907] 2023-02-14 14:24:12,689 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2023-02-14 14:24:12,689 >>   Num examples = 306\n",
            "[INFO|trainer.py:2912] 2023-02-14 14:24:12,689 >>   Batch size = 8\n",
            "\n",
            "  0% 0/39 [00:00<?, ?it/s]\u001b[A\n",
            "  8% 3/39 [00:00<00:02, 15.92it/s]\u001b[A\n",
            " 13% 5/39 [00:00<00:02, 13.14it/s]\u001b[A\n",
            " 18% 7/39 [00:00<00:02, 12.02it/s]\u001b[A\n",
            " 23% 9/39 [00:00<00:02, 11.51it/s]\u001b[A\n",
            " 28% 11/39 [00:00<00:02, 10.94it/s]\u001b[A\n",
            " 33% 13/39 [00:01<00:02, 10.84it/s]\u001b[A\n",
            " 38% 15/39 [00:01<00:02, 10.54it/s]\u001b[A\n",
            " 44% 17/39 [00:01<00:02, 10.40it/s]\u001b[A\n",
            " 49% 19/39 [00:01<00:01, 10.32it/s]\u001b[A\n",
            " 54% 21/39 [00:01<00:01, 10.29it/s]\u001b[A\n",
            " 59% 23/39 [00:02<00:01,  9.86it/s]\u001b[A\n",
            " 64% 25/39 [00:02<00:01,  9.98it/s]\u001b[A\n",
            " 69% 27/39 [00:02<00:01,  9.99it/s]\u001b[A\n",
            " 74% 29/39 [00:02<00:00, 10.01it/s]\u001b[A\n",
            " 79% 31/39 [00:02<00:00,  9.89it/s]\u001b[A\n",
            " 85% 33/39 [00:03<00:00,  9.89it/s]\u001b[A\n",
            " 87% 34/39 [00:03<00:00,  9.78it/s]\u001b[A\n",
            " 90% 35/39 [00:03<00:00,  9.82it/s]\u001b[A\n",
            " 92% 36/39 [00:03<00:00,  9.86it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.3998717963695526, 'eval_bleu': 0.0, 'eval_accuracy': 0.6503, 'eval_gen_len': 2.0033, 'eval_runtime': 3.8957, 'eval_samples_per_second': 78.547, 'eval_steps_per_second': 10.011, 'epoch': 0.22}\n",
            " 10% 250/2500 [00:44<05:30,  6.80it/s]\n",
            "100% 39/39 [00:03<00:00,  9.93it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2656] 2023-02-14 14:24:16,586 >> Saving model checkpoint to out/beer_reviews/google/flan-t5-small/checkpoint-250\n",
            "[INFO|configuration_utils.py:447] 2023-02-14 14:24:16,587 >> Configuration saved in out/beer_reviews/google/flan-t5-small/checkpoint-250/config.json\n",
            "[INFO|modeling_utils.py:1624] 2023-02-14 14:24:17,416 >> Model weights saved in out/beer_reviews/google/flan-t5-small/checkpoint-250/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2023-02-14 14:24:17,417 >> tokenizer config file saved in out/beer_reviews/google/flan-t5-small/checkpoint-250/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2023-02-14 14:24:17,417 >> Special tokens file saved in out/beer_reviews/google/flan-t5-small/checkpoint-250/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:187] 2023-02-14 14:24:17,458 >> Copy vocab file to out/beer_reviews/google/flan-t5-small/checkpoint-250/spiece.model\n",
            "{'loss': 0.3813, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.27}\n",
            "{'loss': 0.3667, 'learning_rate': 4.2e-05, 'epoch': 0.36}\n",
            "{'loss': 0.3495, 'learning_rate': 4e-05, 'epoch': 0.44}\n",
            " 20% 500/2500 [01:26<04:54,  6.79it/s][INFO|trainer.py:2907] 2023-02-14 14:24:59,082 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2023-02-14 14:24:59,082 >>   Num examples = 306\n",
            "[INFO|trainer.py:2912] 2023-02-14 14:24:59,082 >>   Batch size = 8\n",
            "\n",
            "  0% 0/39 [00:00<?, ?it/s]\u001b[A\n",
            "  8% 3/39 [00:00<00:02, 16.29it/s]\u001b[A\n",
            " 13% 5/39 [00:00<00:02, 13.57it/s]\u001b[A\n",
            " 18% 7/39 [00:00<00:02, 12.25it/s]\u001b[A\n",
            " 23% 9/39 [00:00<00:02, 11.58it/s]\u001b[A\n",
            " 28% 11/39 [00:00<00:02, 11.18it/s]\u001b[A\n",
            " 33% 13/39 [00:01<00:02, 11.33it/s]\u001b[A\n",
            " 38% 15/39 [00:01<00:02, 11.17it/s]\u001b[A\n",
            " 44% 17/39 [00:01<00:01, 11.07it/s]\u001b[A\n",
            " 49% 19/39 [00:01<00:01, 10.98it/s]\u001b[A\n",
            " 54% 21/39 [00:01<00:01, 11.03it/s]\u001b[A\n",
            " 59% 23/39 [00:02<00:01, 10.80it/s]\u001b[A\n",
            " 64% 25/39 [00:02<00:01, 10.84it/s]\u001b[A\n",
            " 69% 27/39 [00:02<00:01, 10.82it/s]\u001b[A\n",
            " 74% 29/39 [00:02<00:00, 10.78it/s]\u001b[A\n",
            " 79% 31/39 [00:02<00:00, 10.75it/s]\u001b[A\n",
            " 85% 33/39 [00:02<00:00, 10.71it/s]\u001b[A\n",
            " 90% 35/39 [00:03<00:00, 10.79it/s]\u001b[A\n",
            " 95% 37/39 [00:03<00:00, 11.06it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.3746354281902313, 'eval_bleu': 0.0, 'eval_accuracy': 0.6928, 'eval_gen_len': 2.0033, 'eval_runtime': 3.5771, 'eval_samples_per_second': 85.543, 'eval_steps_per_second': 10.903, 'epoch': 0.44}\n",
            " 20% 500/2500 [01:30<04:54,  6.79it/s]\n",
            "100% 39/39 [00:03<00:00, 11.61it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2656] 2023-02-14 14:25:02,660 >> Saving model checkpoint to out/beer_reviews/google/flan-t5-small/checkpoint-500\n",
            "[INFO|configuration_utils.py:447] 2023-02-14 14:25:02,661 >> Configuration saved in out/beer_reviews/google/flan-t5-small/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:1624] 2023-02-14 14:25:03,321 >> Model weights saved in out/beer_reviews/google/flan-t5-small/checkpoint-500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2023-02-14 14:25:03,322 >> tokenizer config file saved in out/beer_reviews/google/flan-t5-small/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2023-02-14 14:25:03,322 >> Special tokens file saved in out/beer_reviews/google/flan-t5-small/checkpoint-500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:187] 2023-02-14 14:25:03,361 >> Copy vocab file to out/beer_reviews/google/flan-t5-small/checkpoint-500/spiece.model\n",
            "{'loss': 0.3836, 'learning_rate': 3.8e-05, 'epoch': 0.53}\n",
            "{'loss': 0.3733, 'learning_rate': 3.6e-05, 'epoch': 0.62}\n",
            " 30% 750/2500 [02:12<04:19,  6.74it/s][INFO|trainer.py:2907] 2023-02-14 14:25:45,352 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2023-02-14 14:25:45,352 >>   Num examples = 306\n",
            "[INFO|trainer.py:2912] 2023-02-14 14:25:45,352 >>   Batch size = 8\n",
            "\n",
            "  0% 0/39 [00:00<?, ?it/s]\u001b[A\n",
            "  8% 3/39 [00:00<00:02, 15.94it/s]\u001b[A\n",
            " 13% 5/39 [00:00<00:02, 13.40it/s]\u001b[A\n",
            " 18% 7/39 [00:00<00:02, 12.14it/s]\u001b[A\n",
            " 23% 9/39 [00:00<00:02, 11.62it/s]\u001b[A\n",
            " 28% 11/39 [00:00<00:02, 11.20it/s]\u001b[A\n",
            " 33% 13/39 [00:01<00:02, 11.31it/s]\u001b[A\n",
            " 38% 15/39 [00:01<00:02, 11.06it/s]\u001b[A\n",
            " 44% 17/39 [00:01<00:02, 10.93it/s]\u001b[A\n",
            " 49% 19/39 [00:01<00:01, 10.83it/s]\u001b[A\n",
            " 54% 21/39 [00:01<00:01, 10.89it/s]\u001b[A\n",
            " 59% 23/39 [00:02<00:01, 10.83it/s]\u001b[A\n",
            " 64% 25/39 [00:02<00:01, 10.82it/s]\u001b[A\n",
            " 69% 27/39 [00:02<00:01, 10.80it/s]\u001b[A\n",
            " 74% 29/39 [00:02<00:00, 10.79it/s]\u001b[A\n",
            " 79% 31/39 [00:02<00:00, 10.80it/s]\u001b[A\n",
            " 85% 33/39 [00:02<00:00, 10.73it/s]\u001b[A\n",
            " 90% 35/39 [00:03<00:00, 10.58it/s]\u001b[A\n",
            " 95% 37/39 [00:03<00:00, 10.52it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.3509993255138397, 'eval_bleu': 0.0, 'eval_accuracy': 0.6928, 'eval_gen_len': 2.0, 'eval_runtime': 3.6674, 'eval_samples_per_second': 83.437, 'eval_steps_per_second': 10.634, 'epoch': 0.67}\n",
            " 30% 750/2500 [02:16<04:19,  6.74it/s]\n",
            "100% 39/39 [00:03<00:00, 10.79it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2656] 2023-02-14 14:25:49,023 >> Saving model checkpoint to out/beer_reviews/google/flan-t5-small/checkpoint-750\n",
            "[INFO|configuration_utils.py:447] 2023-02-14 14:25:49,024 >> Configuration saved in out/beer_reviews/google/flan-t5-small/checkpoint-750/config.json\n",
            "[INFO|modeling_utils.py:1624] 2023-02-14 14:25:50,025 >> Model weights saved in out/beer_reviews/google/flan-t5-small/checkpoint-750/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2023-02-14 14:25:50,026 >> tokenizer config file saved in out/beer_reviews/google/flan-t5-small/checkpoint-750/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2023-02-14 14:25:50,026 >> Special tokens file saved in out/beer_reviews/google/flan-t5-small/checkpoint-750/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:187] 2023-02-14 14:25:50,084 >> Copy vocab file to out/beer_reviews/google/flan-t5-small/checkpoint-750/spiece.model\n",
            "{'loss': 0.359, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.71}\n",
            "{'loss': 0.3567, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.8}\n",
            "{'loss': 0.3728, 'learning_rate': 3e-05, 'epoch': 0.89}\n",
            " 40% 1000/2500 [02:59<04:10,  6.00it/s][INFO|trainer.py:2907] 2023-02-14 14:26:31,657 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2023-02-14 14:26:31,657 >>   Num examples = 306\n",
            "[INFO|trainer.py:2912] 2023-02-14 14:26:31,657 >>   Batch size = 8\n",
            "\n",
            "  0% 0/39 [00:00<?, ?it/s]\u001b[A\n",
            "  8% 3/39 [00:00<00:02, 15.05it/s]\u001b[A\n",
            " 13% 5/39 [00:00<00:02, 12.35it/s]\u001b[A\n",
            " 18% 7/39 [00:00<00:02, 10.79it/s]\u001b[A\n",
            " 23% 9/39 [00:00<00:02, 10.34it/s]\u001b[A\n",
            " 28% 11/39 [00:01<00:02, 10.17it/s]\u001b[A\n",
            " 33% 13/39 [00:01<00:02,  9.72it/s]\u001b[A\n",
            " 36% 14/39 [00:01<00:02,  9.58it/s]\u001b[A\n",
            " 38% 15/39 [00:01<00:02,  9.54it/s]\u001b[A\n",
            " 41% 16/39 [00:01<00:02,  9.40it/s]\u001b[A\n",
            " 44% 17/39 [00:01<00:02,  9.53it/s]\u001b[A\n",
            " 49% 19/39 [00:01<00:02,  9.94it/s]\u001b[A\n",
            " 54% 21/39 [00:02<00:01, 10.32it/s]\u001b[A\n",
            " 59% 23/39 [00:02<00:01, 10.49it/s]\u001b[A\n",
            " 64% 25/39 [00:02<00:01, 10.58it/s]\u001b[A\n",
            " 69% 27/39 [00:02<00:01, 10.64it/s]\u001b[A\n",
            " 74% 29/39 [00:02<00:00, 10.66it/s]\u001b[A\n",
            " 79% 31/39 [00:02<00:00, 10.58it/s]\u001b[A\n",
            " 85% 33/39 [00:03<00:00, 10.58it/s]\u001b[A\n",
            " 90% 35/39 [00:03<00:00, 10.66it/s]\u001b[A\n",
            " 95% 37/39 [00:03<00:00, 10.95it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.342020720243454, 'eval_bleu': 0.0, 'eval_accuracy': 0.7157, 'eval_gen_len': 2.0, 'eval_runtime': 3.8168, 'eval_samples_per_second': 80.172, 'eval_steps_per_second': 10.218, 'epoch': 0.89}\n",
            " 40% 1000/2500 [03:02<04:10,  6.00it/s]\n",
            "100% 39/39 [00:03<00:00, 11.31it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2656] 2023-02-14 14:26:35,475 >> Saving model checkpoint to out/beer_reviews/google/flan-t5-small/checkpoint-1000\n",
            "[INFO|configuration_utils.py:447] 2023-02-14 14:26:35,476 >> Configuration saved in out/beer_reviews/google/flan-t5-small/checkpoint-1000/config.json\n",
            "[INFO|modeling_utils.py:1624] 2023-02-14 14:26:36,178 >> Model weights saved in out/beer_reviews/google/flan-t5-small/checkpoint-1000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2023-02-14 14:26:36,179 >> tokenizer config file saved in out/beer_reviews/google/flan-t5-small/checkpoint-1000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2023-02-14 14:26:36,179 >> Special tokens file saved in out/beer_reviews/google/flan-t5-small/checkpoint-1000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:187] 2023-02-14 14:26:36,225 >> Copy vocab file to out/beer_reviews/google/flan-t5-small/checkpoint-1000/spiece.model\n",
            "{'loss': 0.3392, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.98}\n",
            "{'loss': 0.3388, 'learning_rate': 2.6000000000000002e-05, 'epoch': 1.07}\n",
            " 50% 1250/2500 [03:45<03:12,  6.50it/s][INFO|trainer.py:2907] 2023-02-14 14:27:18,479 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2023-02-14 14:27:18,479 >>   Num examples = 306\n",
            "[INFO|trainer.py:2912] 2023-02-14 14:27:18,480 >>   Batch size = 8\n",
            "\n",
            "  0% 0/39 [00:00<?, ?it/s]\u001b[A\n",
            "  8% 3/39 [00:00<00:02, 16.30it/s]\u001b[A\n",
            " 13% 5/39 [00:00<00:02, 13.54it/s]\u001b[A\n",
            " 18% 7/39 [00:00<00:02, 12.08it/s]\u001b[A\n",
            " 23% 9/39 [00:00<00:02, 11.55it/s]\u001b[A\n",
            " 28% 11/39 [00:00<00:02, 11.25it/s]\u001b[A\n",
            " 33% 13/39 [00:01<00:02, 11.18it/s]\u001b[A\n",
            " 38% 15/39 [00:01<00:02, 11.03it/s]\u001b[A\n",
            " 44% 17/39 [00:01<00:02, 10.97it/s]\u001b[A\n",
            " 49% 19/39 [00:01<00:01, 10.89it/s]\u001b[A\n",
            " 54% 21/39 [00:01<00:01, 10.97it/s]\u001b[A\n",
            " 59% 23/39 [00:02<00:01, 10.82it/s]\u001b[A\n",
            " 64% 25/39 [00:02<00:01, 10.83it/s]\u001b[A\n",
            " 69% 27/39 [00:02<00:01, 10.79it/s]\u001b[A\n",
            " 74% 29/39 [00:02<00:00, 10.79it/s]\u001b[A\n",
            " 79% 31/39 [00:02<00:00, 10.70it/s]\u001b[A\n",
            " 85% 33/39 [00:02<00:00, 10.72it/s]\u001b[A\n",
            " 90% 35/39 [00:03<00:00, 10.65it/s]\u001b[A\n",
            " 95% 37/39 [00:03<00:00, 10.93it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.3469519019126892, 'eval_bleu': 0.0, 'eval_accuracy': 0.7059, 'eval_gen_len': 2.0, 'eval_runtime': 3.6072, 'eval_samples_per_second': 84.83, 'eval_steps_per_second': 10.812, 'epoch': 1.11}\n",
            " 50% 1250/2500 [03:49<03:12,  6.50it/s]\n",
            "100% 39/39 [00:03<00:00, 11.46it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2656] 2023-02-14 14:27:22,088 >> Saving model checkpoint to out/beer_reviews/google/flan-t5-small/checkpoint-1250\n",
            "[INFO|configuration_utils.py:447] 2023-02-14 14:27:22,089 >> Configuration saved in out/beer_reviews/google/flan-t5-small/checkpoint-1250/config.json\n",
            "[INFO|modeling_utils.py:1624] 2023-02-14 14:27:22,827 >> Model weights saved in out/beer_reviews/google/flan-t5-small/checkpoint-1250/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2023-02-14 14:27:22,829 >> tokenizer config file saved in out/beer_reviews/google/flan-t5-small/checkpoint-1250/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2023-02-14 14:27:22,829 >> Special tokens file saved in out/beer_reviews/google/flan-t5-small/checkpoint-1250/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:187] 2023-02-14 14:27:22,869 >> Copy vocab file to out/beer_reviews/google/flan-t5-small/checkpoint-1250/spiece.model\n",
            "{'loss': 0.3454, 'learning_rate': 2.4e-05, 'epoch': 1.16}\n",
            "{'loss': 0.3329, 'learning_rate': 2.2000000000000003e-05, 'epoch': 1.24}\n",
            "{'loss': 0.3257, 'learning_rate': 2e-05, 'epoch': 1.33}\n",
            " 60% 1500/2500 [04:31<02:26,  6.82it/s][INFO|trainer.py:2907] 2023-02-14 14:28:04,537 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2023-02-14 14:28:04,537 >>   Num examples = 306\n",
            "[INFO|trainer.py:2912] 2023-02-14 14:28:04,537 >>   Batch size = 8\n",
            "\n",
            "  0% 0/39 [00:00<?, ?it/s]\u001b[A\n",
            "  8% 3/39 [00:00<00:02, 15.79it/s]\u001b[A\n",
            " 13% 5/39 [00:00<00:02, 13.35it/s]\u001b[A\n",
            " 18% 7/39 [00:00<00:02, 12.02it/s]\u001b[A\n",
            " 23% 9/39 [00:00<00:02, 11.22it/s]\u001b[A\n",
            " 28% 11/39 [00:00<00:02, 10.77it/s]\u001b[A\n",
            " 33% 13/39 [00:01<00:02, 10.59it/s]\u001b[A\n",
            " 38% 15/39 [00:01<00:02, 10.43it/s]\u001b[A\n",
            " 44% 17/39 [00:01<00:02, 10.34it/s]\u001b[A\n",
            " 49% 19/39 [00:01<00:01, 10.15it/s]\u001b[A\n",
            " 54% 21/39 [00:01<00:01, 10.32it/s]\u001b[A\n",
            " 59% 23/39 [00:02<00:01, 10.25it/s]\u001b[A\n",
            " 64% 25/39 [00:02<00:01, 10.28it/s]\u001b[A\n",
            " 69% 27/39 [00:02<00:01, 10.24it/s]\u001b[A\n",
            " 74% 29/39 [00:02<00:00, 10.13it/s]\u001b[A\n",
            " 79% 31/39 [00:02<00:00,  9.99it/s]\u001b[A\n",
            " 85% 33/39 [00:03<00:00,  9.90it/s]\u001b[A\n",
            " 87% 34/39 [00:03<00:00,  9.63it/s]\u001b[A\n",
            " 90% 35/39 [00:03<00:00,  9.60it/s]\u001b[A\n",
            " 92% 36/39 [00:03<00:00,  9.58it/s]\u001b[A\n",
            " 95% 37/39 [00:03<00:00,  9.38it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.3476087152957916, 'eval_bleu': 0.0, 'eval_accuracy': 0.6928, 'eval_gen_len': 2.0, 'eval_runtime': 3.9139, 'eval_samples_per_second': 78.183, 'eval_steps_per_second': 9.964, 'epoch': 1.33}\n",
            " 60% 1500/2500 [04:35<02:26,  6.82it/s]\n",
            "100% 39/39 [00:03<00:00,  9.47it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2656] 2023-02-14 14:28:08,452 >> Saving model checkpoint to out/beer_reviews/google/flan-t5-small/checkpoint-1500\n",
            "[INFO|configuration_utils.py:447] 2023-02-14 14:28:08,454 >> Configuration saved in out/beer_reviews/google/flan-t5-small/checkpoint-1500/config.json\n",
            "[INFO|modeling_utils.py:1624] 2023-02-14 14:28:09,228 >> Model weights saved in out/beer_reviews/google/flan-t5-small/checkpoint-1500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2023-02-14 14:28:09,229 >> tokenizer config file saved in out/beer_reviews/google/flan-t5-small/checkpoint-1500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2023-02-14 14:28:09,229 >> Special tokens file saved in out/beer_reviews/google/flan-t5-small/checkpoint-1500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:187] 2023-02-14 14:28:09,276 >> Copy vocab file to out/beer_reviews/google/flan-t5-small/checkpoint-1500/spiece.model\n",
            "[INFO|trainer.py:2734] 2023-02-14 14:28:11,713 >> Deleting older checkpoint [out/beer_reviews/google/flan-t5-small/checkpoint-250] due to args.save_total_limit\n",
            "{'loss': 0.3214, 'learning_rate': 1.8e-05, 'epoch': 1.42}\n",
            "{'loss': 0.3343, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.51}\n",
            " 70% 1750/2500 [05:18<01:50,  6.77it/s][INFO|trainer.py:2907] 2023-02-14 14:28:51,231 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2023-02-14 14:28:51,231 >>   Num examples = 306\n",
            "[INFO|trainer.py:2912] 2023-02-14 14:28:51,231 >>   Batch size = 8\n",
            "\n",
            "  0% 0/39 [00:00<?, ?it/s]\u001b[A\n",
            "  8% 3/39 [00:00<00:02, 16.23it/s]\u001b[A\n",
            " 13% 5/39 [00:00<00:02, 13.42it/s]\u001b[A\n",
            " 18% 7/39 [00:00<00:02, 12.15it/s]\u001b[A\n",
            " 23% 9/39 [00:00<00:02, 11.48it/s]\u001b[A\n",
            " 28% 11/39 [00:00<00:02, 11.16it/s]\u001b[A\n",
            " 33% 13/39 [00:01<00:02, 11.25it/s]\u001b[A\n",
            " 38% 15/39 [00:01<00:02, 11.11it/s]\u001b[A\n",
            " 44% 17/39 [00:01<00:01, 11.03it/s]\u001b[A\n",
            " 49% 19/39 [00:01<00:01, 10.90it/s]\u001b[A\n",
            " 54% 21/39 [00:01<00:01, 10.92it/s]\u001b[A\n",
            " 59% 23/39 [00:02<00:01, 10.91it/s]\u001b[A\n",
            " 64% 25/39 [00:02<00:01, 10.90it/s]\u001b[A\n",
            " 69% 27/39 [00:02<00:01, 10.86it/s]\u001b[A\n",
            " 74% 29/39 [00:02<00:00, 10.79it/s]\u001b[A\n",
            " 79% 31/39 [00:02<00:00, 10.73it/s]\u001b[A\n",
            " 85% 33/39 [00:02<00:00, 10.73it/s]\u001b[A\n",
            " 90% 35/39 [00:03<00:00, 10.69it/s]\u001b[A\n",
            " 95% 37/39 [00:03<00:00, 11.02it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.3571603000164032, 'eval_bleu': 0.0, 'eval_accuracy': 0.6993, 'eval_gen_len': 2.0, 'eval_runtime': 3.5883, 'eval_samples_per_second': 85.277, 'eval_steps_per_second': 10.869, 'epoch': 1.56}\n",
            " 70% 1750/2500 [05:22<01:50,  6.77it/s]\n",
            "100% 39/39 [00:03<00:00, 11.62it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2656] 2023-02-14 14:28:54,821 >> Saving model checkpoint to out/beer_reviews/google/flan-t5-small/checkpoint-1750\n",
            "[INFO|configuration_utils.py:447] 2023-02-14 14:28:54,821 >> Configuration saved in out/beer_reviews/google/flan-t5-small/checkpoint-1750/config.json\n",
            "[INFO|modeling_utils.py:1624] 2023-02-14 14:28:55,536 >> Model weights saved in out/beer_reviews/google/flan-t5-small/checkpoint-1750/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2023-02-14 14:28:55,538 >> tokenizer config file saved in out/beer_reviews/google/flan-t5-small/checkpoint-1750/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2023-02-14 14:28:55,538 >> Special tokens file saved in out/beer_reviews/google/flan-t5-small/checkpoint-1750/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:187] 2023-02-14 14:28:55,580 >> Copy vocab file to out/beer_reviews/google/flan-t5-small/checkpoint-1750/spiece.model\n",
            "[INFO|trainer.py:2734] 2023-02-14 14:28:57,958 >> Deleting older checkpoint [out/beer_reviews/google/flan-t5-small/checkpoint-500] due to args.save_total_limit\n",
            "{'loss': 0.3049, 'learning_rate': 1.4000000000000001e-05, 'epoch': 1.6}\n",
            "{'loss': 0.3164, 'learning_rate': 1.2e-05, 'epoch': 1.69}\n",
            "{'loss': 0.3364, 'learning_rate': 1e-05, 'epoch': 1.78}\n",
            " 80% 2000/2500 [06:05<01:16,  6.56it/s][INFO|trainer.py:2907] 2023-02-14 14:29:37,727 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2023-02-14 14:29:37,728 >>   Num examples = 306\n",
            "[INFO|trainer.py:2912] 2023-02-14 14:29:37,728 >>   Batch size = 8\n",
            "\n",
            "  0% 0/39 [00:00<?, ?it/s]\u001b[A\n",
            "  8% 3/39 [00:00<00:02, 16.08it/s]\u001b[A\n",
            " 13% 5/39 [00:00<00:02, 13.43it/s]\u001b[A\n",
            " 18% 7/39 [00:00<00:02, 12.24it/s]\u001b[A\n",
            " 23% 9/39 [00:00<00:02, 11.67it/s]\u001b[A\n",
            " 28% 11/39 [00:00<00:02, 11.30it/s]\u001b[A\n",
            " 33% 13/39 [00:01<00:02, 11.37it/s]\u001b[A\n",
            " 38% 15/39 [00:01<00:02, 11.14it/s]\u001b[A\n",
            " 44% 17/39 [00:01<00:01, 11.03it/s]\u001b[A\n",
            " 49% 19/39 [00:01<00:01, 10.94it/s]\u001b[A\n",
            " 54% 21/39 [00:01<00:01, 11.03it/s]\u001b[A\n",
            " 59% 23/39 [00:02<00:01, 10.93it/s]\u001b[A\n",
            " 64% 25/39 [00:02<00:01, 10.88it/s]\u001b[A\n",
            " 69% 27/39 [00:02<00:01, 10.86it/s]\u001b[A\n",
            " 74% 29/39 [00:02<00:00, 10.56it/s]\u001b[A\n",
            " 79% 31/39 [00:02<00:00, 10.38it/s]\u001b[A\n",
            " 85% 33/39 [00:02<00:00, 10.26it/s]\u001b[A\n",
            " 90% 35/39 [00:03<00:00, 10.22it/s]\u001b[A\n",
            " 95% 37/39 [00:03<00:00, 10.40it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.3335842192173004, 'eval_bleu': 0.0, 'eval_accuracy': 0.732, 'eval_gen_len': 2.0, 'eval_runtime': 3.7023, 'eval_samples_per_second': 82.652, 'eval_steps_per_second': 10.534, 'epoch': 1.78}\n",
            " 80% 2000/2500 [06:08<01:16,  6.56it/s]\n",
            "100% 39/39 [00:03<00:00, 10.40it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2656] 2023-02-14 14:29:41,431 >> Saving model checkpoint to out/beer_reviews/google/flan-t5-small/checkpoint-2000\n",
            "[INFO|configuration_utils.py:447] 2023-02-14 14:29:41,433 >> Configuration saved in out/beer_reviews/google/flan-t5-small/checkpoint-2000/config.json\n",
            "[INFO|modeling_utils.py:1624] 2023-02-14 14:29:42,455 >> Model weights saved in out/beer_reviews/google/flan-t5-small/checkpoint-2000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2023-02-14 14:29:42,456 >> tokenizer config file saved in out/beer_reviews/google/flan-t5-small/checkpoint-2000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2023-02-14 14:29:42,456 >> Special tokens file saved in out/beer_reviews/google/flan-t5-small/checkpoint-2000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:187] 2023-02-14 14:29:42,514 >> Copy vocab file to out/beer_reviews/google/flan-t5-small/checkpoint-2000/spiece.model\n",
            "[INFO|trainer.py:2734] 2023-02-14 14:29:44,637 >> Deleting older checkpoint [out/beer_reviews/google/flan-t5-small/checkpoint-750] due to args.save_total_limit\n",
            "{'loss': 0.3073, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.87}\n",
            "{'loss': 0.3319, 'learning_rate': 6e-06, 'epoch': 1.96}\n",
            " 90% 2250/2500 [06:51<00:45,  5.49it/s][INFO|trainer.py:2907] 2023-02-14 14:30:23,849 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2023-02-14 14:30:23,849 >>   Num examples = 306\n",
            "[INFO|trainer.py:2912] 2023-02-14 14:30:23,849 >>   Batch size = 8\n",
            "\n",
            "  0% 0/39 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 2/39 [00:00<00:01, 19.51it/s]\u001b[A\n",
            " 10% 4/39 [00:00<00:02, 12.06it/s]\u001b[A\n",
            " 15% 6/39 [00:00<00:03, 10.89it/s]\u001b[A\n",
            " 21% 8/39 [00:00<00:03, 10.10it/s]\u001b[A\n",
            " 26% 10/39 [00:00<00:02,  9.91it/s]\u001b[A\n",
            " 31% 12/39 [00:01<00:02,  9.84it/s]\u001b[A\n",
            " 36% 14/39 [00:01<00:02, 10.20it/s]\u001b[A\n",
            " 41% 16/39 [00:01<00:02, 10.38it/s]\u001b[A\n",
            " 46% 18/39 [00:01<00:01, 10.51it/s]\u001b[A\n",
            " 51% 20/39 [00:01<00:01, 10.51it/s]\u001b[A\n",
            " 56% 22/39 [00:02<00:01, 10.72it/s]\u001b[A\n",
            " 62% 24/39 [00:02<00:01, 10.71it/s]\u001b[A\n",
            " 67% 26/39 [00:02<00:01, 10.63it/s]\u001b[A\n",
            " 72% 28/39 [00:02<00:01, 10.69it/s]\u001b[A\n",
            " 77% 30/39 [00:02<00:00, 10.74it/s]\u001b[A\n",
            " 82% 32/39 [00:03<00:00, 10.72it/s]\u001b[A\n",
            " 87% 34/39 [00:03<00:00, 10.68it/s]\u001b[A\n",
            " 92% 36/39 [00:03<00:00, 10.83it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.3510887026786804, 'eval_bleu': 0.0, 'eval_accuracy': 0.7059, 'eval_gen_len': 2.0, 'eval_runtime': 3.7392, 'eval_samples_per_second': 81.837, 'eval_steps_per_second': 10.43, 'epoch': 2.0}\n",
            " 90% 2250/2500 [06:55<00:45,  5.49it/s]\n",
            "100% 39/39 [00:03<00:00, 11.01it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2656] 2023-02-14 14:30:27,589 >> Saving model checkpoint to out/beer_reviews/google/flan-t5-small/checkpoint-2250\n",
            "[INFO|configuration_utils.py:447] 2023-02-14 14:30:27,590 >> Configuration saved in out/beer_reviews/google/flan-t5-small/checkpoint-2250/config.json\n",
            "[INFO|modeling_utils.py:1624] 2023-02-14 14:30:28,290 >> Model weights saved in out/beer_reviews/google/flan-t5-small/checkpoint-2250/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2023-02-14 14:30:28,291 >> tokenizer config file saved in out/beer_reviews/google/flan-t5-small/checkpoint-2250/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2023-02-14 14:30:28,292 >> Special tokens file saved in out/beer_reviews/google/flan-t5-small/checkpoint-2250/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:187] 2023-02-14 14:30:28,338 >> Copy vocab file to out/beer_reviews/google/flan-t5-small/checkpoint-2250/spiece.model\n",
            "[INFO|trainer.py:2734] 2023-02-14 14:30:30,726 >> Deleting older checkpoint [out/beer_reviews/google/flan-t5-small/checkpoint-1000] due to args.save_total_limit\n",
            "{'loss': 0.3129, 'learning_rate': 4.000000000000001e-06, 'epoch': 2.04}\n",
            "{'loss': 0.3309, 'learning_rate': 2.0000000000000003e-06, 'epoch': 2.13}\n",
            "{'loss': 0.2985, 'learning_rate': 0.0, 'epoch': 2.22}\n",
            "100% 2500/2500 [07:37<00:00,  6.56it/s][INFO|trainer.py:2907] 2023-02-14 14:31:10,500 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2023-02-14 14:31:10,500 >>   Num examples = 306\n",
            "[INFO|trainer.py:2912] 2023-02-14 14:31:10,500 >>   Batch size = 8\n",
            "\n",
            "  0% 0/39 [00:00<?, ?it/s]\u001b[A\n",
            "  8% 3/39 [00:00<00:02, 16.21it/s]\u001b[A\n",
            " 13% 5/39 [00:00<00:02, 13.45it/s]\u001b[A\n",
            " 18% 7/39 [00:00<00:02, 12.09it/s]\u001b[A\n",
            " 23% 9/39 [00:00<00:02, 11.57it/s]\u001b[A\n",
            " 28% 11/39 [00:00<00:02, 11.25it/s]\u001b[A\n",
            " 33% 13/39 [00:01<00:02, 11.37it/s]\u001b[A\n",
            " 38% 15/39 [00:01<00:02, 11.14it/s]\u001b[A\n",
            " 44% 17/39 [00:01<00:01, 11.00it/s]\u001b[A\n",
            " 49% 19/39 [00:01<00:01, 10.88it/s]\u001b[A\n",
            " 54% 21/39 [00:01<00:01, 10.96it/s]\u001b[A\n",
            " 59% 23/39 [00:02<00:01, 10.90it/s]\u001b[A\n",
            " 64% 25/39 [00:02<00:01, 10.87it/s]\u001b[A\n",
            " 69% 27/39 [00:02<00:01, 10.79it/s]\u001b[A\n",
            " 74% 29/39 [00:02<00:00, 10.78it/s]\u001b[A\n",
            " 79% 31/39 [00:02<00:00, 10.70it/s]\u001b[A\n",
            " 85% 33/39 [00:02<00:00, 10.71it/s]\u001b[A\n",
            " 90% 35/39 [00:03<00:00, 10.73it/s]\u001b[A\n",
            " 95% 37/39 [00:03<00:00, 11.07it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.34608006477355957, 'eval_bleu': 0.0, 'eval_accuracy': 0.7026, 'eval_gen_len': 2.0, 'eval_runtime': 3.5837, 'eval_samples_per_second': 85.388, 'eval_steps_per_second': 10.883, 'epoch': 2.22}\n",
            "100% 2500/2500 [07:41<00:00,  6.56it/s]\n",
            "100% 39/39 [00:03<00:00, 11.58it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2656] 2023-02-14 14:31:14,085 >> Saving model checkpoint to out/beer_reviews/google/flan-t5-small/checkpoint-2500\n",
            "[INFO|configuration_utils.py:447] 2023-02-14 14:31:14,086 >> Configuration saved in out/beer_reviews/google/flan-t5-small/checkpoint-2500/config.json\n",
            "[INFO|modeling_utils.py:1624] 2023-02-14 14:31:14,798 >> Model weights saved in out/beer_reviews/google/flan-t5-small/checkpoint-2500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2023-02-14 14:31:14,799 >> tokenizer config file saved in out/beer_reviews/google/flan-t5-small/checkpoint-2500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2023-02-14 14:31:14,799 >> Special tokens file saved in out/beer_reviews/google/flan-t5-small/checkpoint-2500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:187] 2023-02-14 14:31:14,847 >> Copy vocab file to out/beer_reviews/google/flan-t5-small/checkpoint-2500/spiece.model\n",
            "[INFO|trainer.py:2734] 2023-02-14 14:31:17,246 >> Deleting older checkpoint [out/beer_reviews/google/flan-t5-small/checkpoint-1250] due to args.save_total_limit\n",
            "[INFO|trainer.py:1852] 2023-02-14 14:31:17,279 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[INFO|trainer.py:1946] 2023-02-14 14:31:17,279 >> Loading best model from out/beer_reviews/google/flan-t5-small/checkpoint-2000 (score: 0.732).\n",
            "{'train_runtime': 465.1893, 'train_samples_per_second': 42.993, 'train_steps_per_second': 5.374, 'train_loss': 0.3786498863220215, 'epoch': 2.22}\n",
            "100% 2500/2500 [07:45<00:00,  5.37it/s]\n",
            "[INFO|trainer.py:2656] 2023-02-14 14:31:17,732 >> Saving model checkpoint to out/beer_reviews/google/flan-t5-small\n",
            "[INFO|configuration_utils.py:447] 2023-02-14 14:31:17,733 >> Configuration saved in out/beer_reviews/google/flan-t5-small/config.json\n",
            "[INFO|modeling_utils.py:1624] 2023-02-14 14:31:18,659 >> Model weights saved in out/beer_reviews/google/flan-t5-small/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2023-02-14 14:31:18,660 >> tokenizer config file saved in out/beer_reviews/google/flan-t5-small/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2023-02-14 14:31:18,661 >> Special tokens file saved in out/beer_reviews/google/flan-t5-small/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:187] 2023-02-14 14:31:18,723 >> Copy vocab file to out/beer_reviews/google/flan-t5-small/spiece.model\n",
            "***** train metrics *****\n",
            "  epoch                    =       2.22\n",
            "  train_loss               =     0.3786\n",
            "  train_runtime            = 0:07:45.18\n",
            "  train_samples            =       9000\n",
            "  train_samples_per_second =     42.993\n",
            "  train_steps_per_second   =      5.374\n",
            "INFO:__main__:*** Evaluate ***\n",
            "[INFO|trainer.py:2907] 2023-02-14 14:31:18,741 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2023-02-14 14:31:18,741 >>   Num examples = 306\n",
            "[INFO|trainer.py:2912] 2023-02-14 14:31:18,741 >>   Batch size = 8\n",
            "100% 39/39 [00:03<00:00, 10.58it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       2.22\n",
            "  eval_accuracy           =      0.732\n",
            "  eval_bleu               =        0.0\n",
            "  eval_gen_len            =        2.0\n",
            "  eval_loss               =     0.3336\n",
            "  eval_runtime            = 0:00:03.82\n",
            "  eval_samples            =        306\n",
            "  eval_samples_per_second =     79.922\n",
            "  eval_steps_per_second   =     10.186\n",
            "INFO:__main__:*** Predict ***\n",
            "[INFO|trainer.py:2907] 2023-02-14 14:31:22,573 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:2909] 2023-02-14 14:31:22,573 >>   Num examples = 841\n",
            "[INFO|trainer.py:2912] 2023-02-14 14:31:22,573 >>   Batch size = 8\n",
            "100% 106/106 [00:09<00:00, 10.81it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.7396\n",
            "  predict_bleu               =        0.0\n",
            "  predict_gen_len            =        2.0\n",
            "  predict_loss               =     0.3075\n",
            "  predict_runtime            = 0:00:09.90\n",
            "  predict_samples            =        841\n",
            "  predict_samples_per_second =      84.93\n",
            "  predict_steps_per_second   =     10.705\n",
            "[INFO|modelcard.py:444] 2023-02-14 14:31:32,647 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Translation', 'type': 'translation'}, 'metrics': [{'name': 'Bleu', 'type': 'bleu', 'value': 0.0}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.732}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Score : 0.732**"
      ],
      "metadata": {
        "id": "EOCepjNqltl1"
      }
    }
  ]
}